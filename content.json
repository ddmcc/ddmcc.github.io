{"posts":[{"title":"Servlet单实例多线程问题","text":"问题：Servlet是线程安全的吗？ Servlet类本质上也是一个普通的类，并且Servlet容器默认只允许单个实例存在。当请求到达服务器时，Servlet实例如果已经存在的话则直接加载该实例，如果该Servlet类还未实例化则会先初始化这个Servlet。当请求到达Web服务器时，Web服务器中有一个线程池，它会从线程池中取一个工作线程，通过该线程调用请求的Servlet。因此，对Servlet来说，可以同时被多个请求调用，请求结束后，线程放回线程池。 1,Servlet容器如何同时处理多个请求?Java的内存模型JMM(Java Memory Model) JMM主要是为了规定线程和内存之间的一些关系。根据JMM的设计，系统存在一个主内存(Main Memory)，Java中所有实例变量都存储在主存中，对于所有线程都是共享的。每条线程都有自己的工作内存(Working Memory)，工作内存由缓存和堆栈两部分组成，缓存中保存的是主存变量的拷贝，缓存可能并不总和主存同步，也就是缓存中变量的修改可能没有立刻写到主存中，堆栈中保存的线程的局部变量，线程之间无法相互直接访问堆栈中的变量。 Servlet采用多线程来处理多个请求同时访问。 servlet依赖于一个线程池来服务请求。线程池实际上是一系列的工作者线程集合。Servlet使用一个调度线程来管理工作者线程。 当容器收到一个Servlet请求，调度线程从线程池中选出一个工作者线程,将请求传递给该工作者线程，然后由该线程来执行Servlet的service方法。当这个线程正在执行的时候,容器收到另外一个请求,调度线程同样从线程池中选出另一个工作者线程来服务新的请求，容器并不关心这个请求是否访问的是同一个Servlet.当容器同时收到对同一个Servlet的多个请求的时候，那么这个Servlet的service()方法将在多线程中并发执行。 Servlet容器默认采用单实例多线程的方式来处理请求， 这样减少产生Servlet实例的开销，提升了对请求的响应时间，对于Tomcat可以在server.xml中通过元素设置线程池中线程的数目。 2,如何开发线程安全的Servlet?看下面一个例子 123456789101112131415161718192021222324public class MyServlet extends HttpServlet { private String name; // 定义一个全部变量 public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { name = request.getParameter(&quot;name&quot;); // 获取参数 response.setContentType(&quot;text/html&quot;); PrintWriter out = response.getWriter(); try { Thread.sleep(5000); // 休眠5s } catch (InterruptedException e) { e.printStackTrace(); } out.print(name); out.flush(); out.close(); }} 模拟两个线程分别在不同的浏览器中输入地址 可以看到，两个显示的都是222，因为两个线程都调用同一个实例，A线程将成员变量name设置为111后，B线程又将改为了222，所以读取了相同的结果 解决方法主要有以下三种1,实现 SingleThreadModel 接口该接口指定了系统如何处理对同一个Servlet的调用。如果一个Servlet被这个接口指定,那么在这个Servlet中的service方法将不会有两个线程被同时执行，当然也就不存在线程安全的问题。 1public class MyServlet extends HttpServlet implements SingleThreadModel 不过该接口已被废弃，不建议使用 2,同步对共享数据的操作使用synchronized 关键字能保证一次只有一个线程可以访问被保护的区段 将上述代码中的 doGet() 方法加上synchronized即可 12public synchronized void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException 3,避免使用实例变量，使用局部变量本实例中的线程安全问题是由实例变量造成的，只要在Servlet里面的任何方法里面都不使用实例变量，那么该Servlet就是线程安全的。 123456789101112131415161718192021222324public class MyServlet extends HttpServlet { // private String name; // 定义一个全部变量 public void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { String name = request.getParameter(&quot;name&quot;); // 使用局部变量 response.setContentType(&quot;text/html&quot;); PrintWriter out = response.getWriter(); try { Thread.sleep(5000); // 休眠5s } catch (InterruptedException e) { e.printStackTrace(); } out.print(name); out.flush(); out.close(); }} 参考资料 servlet单实例多线程模式 java中的servlet采用的是单实例多线程方式","link":"/2018/04/06/2018-04-05-Servlet-Singleton%20pattern-multithreading/"},{"title":"maven添加oracle jar包","text":"问题在 Maven 中央仓库上的几个ojdbc包依赖配置,都没法通过Maven自动下载到本地目录,tomcat服务器启动错误 解决方案1,选择导入jar在空白处点击右键 -&gt; Import -&gt; Maven -&gt; Install or deploy an artifact to a Maven repository，然后点击”Next”按钮 2,进入导入界面 Artifact file：本地的jar路径 Group Id,Artifact Id,Version: pom.xml导入的名字与版本 3,导入后即可在本地仓库看到ojdbc目录配置pom.xml 12345&lt;dependency&gt; &lt;groupId&gt;ojdbc&lt;/groupId&gt; &lt;artifactId&gt;ojdbc6&lt;/artifactId&gt; &lt;version&gt;11.2.0.4&lt;/version&gt;&lt;/dependency&gt;","link":"/2018/04/05/2018-04-05-maven-add-ojdbc/"},{"title":"maven项目导入jstl包,tomcat启动找不到jar文件,页面无法使用jstl标签","text":"问题在pom.xml配置文件中，已经添加jstl包，服务器启动报异常，jstl jar包不能初始化，成功添加jar包后页面jstl不解析，导致无法取数据 1org.apache.jasper.JasperException: The absolute uri: [http://java.sun.com/jsp/jstl/core] cannot be resolved in either web.xml or the jar files deployed with this application 解决网上查发现是项目中并没有导入jar包，一脸懵逼？？？pom.xml不是导入了吗 没导入就没导入吧，那我手动导入行不行？？ 把jar包手动拷贝到 /WEB-INF 目录下 有报错，虽然不知道什么原因，但是对于一个强迫症患者来说这怎么行？？ 于是乎直接把jar包扔到了 tomcat 下的 lib 目录下了，哈哈哈这下总可以了吧 启动 完美！看见登录页面了！ 下面登录就可以登录了，走你 ε=(´ο｀*)))唉？页面好像不太对，数据呢？ 我的jstl表达式怎么出来了啦？Σ(⊙▽⊙”a表达式好像没有解析 ${user.nickname },${user.gender eq 0? '男':'女'} ${user.province },${user.city } 这样还不行？继续查之~~~~ 终于在页面添加 &lt;%@ page isELIgnored=&quot;false&quot;%&gt; 问题解决！！ 总结 (1)，pom.xml配置jstl jar包 123456&lt;!-- https://mvnrepository.com/artifact/javax.servlet/jstl --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; (2)，tomcat的lib目中添加jstl jar包 (3)，jsp页面添加 &lt;%@ page isELIgnored=&quot;false&quot;%&gt;","link":"/2018/04/07/2018-04-07-maven%20object-import-jstl%20jar/"},{"title":"如何用自己的域名来访问博客","text":"准备首先需要一个域名,可以在各个服务商购买 腾讯 阿里 GoDaddy 我是从GoDaddy 买的,不要 998 一年只要7块钱，不是打广告！ 开始1. 向你的 Github Pages 仓库添加一个CNAME文件 在首行填写域名,其中只能包含一个顶级域名 注意：只要填写如xxxx.com，注意前面没有http://，也没有www 2. 在域名提供商中添加域名解析 (1)先添加一个CNAME，主机记录写@，后面记录值写上你的xxxx.github.io (2)先添加一个CNAME，主机记录写www，后面记录值写上你的xxxx.github.io 这样别人用www和不用www都能访问你的网站 我是在goDaddy买的，在 腾讯云 解析的，所以先要在goDaddy上添加DNS服务器 在腾讯控制台找到DNS服务器 doGoDaddy处添加DNS服务器 然后在添加解析 访问等几分钟后就可以访问了，注意 要等！要等！！","link":"/2018/04/06/2018-04-06-how-to-use-domain/"},{"title":"maven项目中使用JUnit进行单元测试","text":"发现通过Spring进行对象管理之后，做测试变得复杂了。因为所有的Bean都需要在 applicationContext.xml中加载好，之后再通过@Resource去取得。如果每次都要整个业务流做的差不多了再去测试，这样效率很 低，也很麻烦。这时候就需要Spring-text框架整合JUnit进行测试. 步骤配置pom.xml添加spring-text jar包 12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;4.1.6.RELEASE&lt;/version&gt;&lt;/dependency&gt; 添加JUnit包 123456&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 编写测试类在src/test/java 中编写一个测试类 123456789101112131415@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;) public class EmpTest { @Resource private EmpService empService; @Test public void testFindAll(){ List&lt;Emp&gt; list = empService.findAll(new Emp()); for(Emp emp : list){ System.out.println(emp.getEname()+&quot;,&quot;+emp.getEtags()); } }} 在方法中添加 @Test 注解，在类名上添加 @RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;) 调用声明业务层接口，由Spring注入 12@Resourceprivate EmpService empService; 总结结构 代码1234567891011121314151617181920212223242526package com.test;import java.util.List;import javax.annotation.Resource;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.test.context.ContextConfiguration;import org.springframework.test.context.junit4.SpringJUnit4ClassRunner;import com.soft.bean.Emp;import com.soft.service.EmpService;@RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(locations = &quot;classpath:applicationContext.xml&quot;) public class EmpTest { @Resource private EmpService empService; @Test public void testFindAll(){ List&lt;Emp&gt; list = empService.findAll(new Emp()); for(Emp emp : list){ System.out.println(emp.getEname()+&quot;,&quot;+emp.getEtags()); } }}","link":"/2018/04/11/2018-04-11-maven%20object-use-junit/"},{"title":"SpringMVC后台执行正常,但是前端显示404","text":"问题今天在做springMVC查询数据时,前端通过ajax发送请求，后台数据查询正常,且无异常提醒，但前端却提示404，数据也没有传过来 解决方案检查发现原来是后端 controller 层处理方法没有返回状态码，在方法中添加 @ResponseBody 注解后，问题解决 12345@RequestMapping(value = &quot;/find.action&quot; , method = RequestMethod.POST ,produces = &quot;application/json;charset=utf-8&quot; )public @ResponseBody Object findStore() { List&lt;Store&gt; store = storeService.find(); return JSONObject.toJSON(store);}","link":"/2018/04/15/2018-04-15-springMVC-successful-status-404/"},{"title":"远程连接MySql","text":"远程访数据库1234567- 1、mysql -u root -p- 2、use mysql;- 3、update user set host='%' where user='用户名'; - 4、flush privileges; 这样就可以远程访问数据库了 更改密码验证在mysql 5.6对密码的强度进行了加强，推出了validate_password 插件。 查看密码规则参数123456789101112131415161718192021mysql&gt; select @@validate_password_policy; +----------------------------+ | @@validate_password_policy | +----------------------------+ | MEDIUM | +----------------------------+ 1 row in set (0.00 sec)mysql&gt; SHOW VARIABLES LIKE 'validate_password%'; +--------------------------------------+--------+ | Variable_name | Value | +--------------------------------------+--------+ | validate_password_dictionary_file | | | validate_password_length | 8 | | validate_password_mixed_case_count | 1 | | validate_password_number_count | 1 | | validate_password_policy | MEDIUM | | validate_password_special_char_count | 1 | +--------------------------------------+--------+ 6 rows in set (0.08 sec) 参数解释12345678910111213141516171819202122validate_password_dictionary_file插件用于验证密码强度的字典文件路径。validate_password_length密码最小长度，参数默认为8，它有最小值的限制，最小值为：validate_password_number_count + validate_password_special_char_count + (2 * validate_password_mixed_case_count)validate_password_mixed_case_count密码至少要包含的小写字母个数和大写字母个数。validate_password_number_count密码至少要包含的数字个数。validate_password_policy密码强度检查等级，0/LOW、1/MEDIUM、2/STRONG。有以下取值：Policy Tests Performed 0 or LOW Length 1 or MEDIUM Length; numeric, lowercase/uppercase, and special characters 2 or STRONG Length; numeric, lowercase/uppercase, and special characters; dictionary file 默认是1，即MEDIUM，所以刚开始设置的密码必须符合长度，且必须含有数字，小写或大写字母，特殊字符。validate_password_special_char_count密码至少要包含的特殊字符数。 修改参数123456789101112131415161718192021222324252627mysql&gt; set global validate_password_policy=0; Query OK, 0 rows affected (0.05 sec) mysql&gt; set global validate_password_mixed_case_count=0; Query OK, 0 rows affected (0.00 sec) mysql&gt; set global validate_password_number_count=3; Query OK, 0 rows affected (0.00 sec) mysql&gt; set global validate_password_special_char_count=0; Query OK, 0 rows affected (0.00 sec) mysql&gt; set global validate_password_length=3; Query OK, 0 rows affected (0.00 sec) mysql&gt; SHOW VARIABLES LIKE 'validate_password%'; +--------------------------------------+-------+ | Variable_name | Value | +--------------------------------------+-------+ | validate_password_dictionary_file | | | validate_password_length | 3 | | validate_password_mixed_case_count | 0 | | validate_password_number_count | 3 | | validate_password_policy | LOW | | validate_password_special_char_count | 0 | +--------------------------------------+-------+ 6 rows in set (0.00 sec) 这样就可以设置简单的密码了","link":"/2018/05/05/2018-05-05-how-to-connect-MySQL-remotely/"},{"title":"Hibernate save插入数据返回主键为0","text":"Hibernate 利用模板的save方法插入实体，返回的Serializable id主键为0在使用HibernateTemplate的save()方法后得不到持久化对象的id值，得到的持久化对象的id值一直为0。 因为表映射实体id 没有加自增，在实体类中添加自增注解 @GeneratedValue(strategy=GenerationType.AUTO) 即可 123456@Id@Column(name = &quot;id&quot;, nullable = false)@GeneratedValue(strategy=GenerationType.AUTO)public long getId() { return id;}","link":"/2018/06/25/2018-06-25-hibernate-serializable/"},{"title":"设计模式之策略模式","text":"什么是策略模式定义算法（行为）族，将它们分别封装起来，让它们之间可以替换，让算法或行为与此算法的使用者或行为的拥有者解耦。 如何设计改造前的代码有一个需求，根据不同客户计算价格的程序，输入价格与用户类型会输入计算后的价格。客户类型有VIP,一般的客户，一级新客户分别打6折，9折和8折。 代码如下： 1234567891011121314151617181920212223public class Customer { public static final int VIP = 1; public static final int GENERAL = 2; public static final int NEW = 3; public void getPrice(Double price, int cusType) { switch (cusType) { case VIP: System.out.println(&quot;VIP客户打6折：&quot; + price * 0.6); break; case GENERAL: System.out.println(&quot;普通客户打9折：&quot; + price * 0.9); break; case NEW: System.out.println(&quot;新客户打8折：&quot; + price * 0.8); break; default: System.out.println(&quot;无打折&quot;); break; } }} 运行一下 1234567public class Main { public static void main(String[] args) { Customer customer = new Customer(); customer.getPrice(120.5, Customer.NEW); }} 运行结果 以现在需求看上去感觉还行。但是如果现在需求变更，增加一个常客，打7折呢？就得在原来方法上加一个分支。下面是更改后的代码 123456789101112131415161718192021222324252627public class Customer { public static final int VIP = 1; public static final int GENERAL = 2; public static final int NEW = 3; public static final int REGULAR = 4; public void getPrice(Double price, int cusType) { switch (cusType) { case VIP: System.out.println(&quot;VIP客户打6折：&quot; + price * 0.6); break; case GENERAL: System.out.println(&quot;普通客户打9折：&quot; + price * 0.9); break; case NEW: System.out.println(&quot;新客户打8折：&quot; + price * 0.8); break; case REGULAR: System.out.println(&quot;常客打7折：&quot; + price * 0.7); default: System.out.println(&quot;无打折&quot;); break; } }} 这样设计我们发现每次需求有变更都需要去更改原来的代码逻辑，所以现在我们需要把会变的这一部分单独的提出来，把它们封装起来。下次再有新的需求我们只需要更改提出来的部分。 改造后的代码 先建一个处理价格的接口Handle，里面有方法handle(double price); 让实现类自己去实现。 12345public interface Handle { void handle(double price);} 在创建具体的客户处理类取实现Handle接口，在handle方法中实现具体的计算规则。 VIP客户 12345678public class VipHandle implements Handle { @Override public void handle(double price) { System.out.println(&quot;VIP客户打6折：&quot; + price * 0.6); }} 新客户 12345678public class NewHandle implements Handle { @Override public void handle(double price) { System.out.println(&quot;新客户打8折：&quot; + price * 0.8); }} 普通客户 12345678public class GenralHandle implements Handle { @Override public void handle(double price) { System.out.println(&quot;普通客户打9折：&quot; + price * 0.9); }} 常客 12345678public class RegularHanndle implements Handle { @Override public void handle(double price) { System.out.println(&quot;常客打7折：&quot; + price * 0.7); }} 我们把不同的计算分为不同的类去实现，当下一次需求变更需要增加不同种类的客户或者改变计算规则，我们就可以直接 增加新的计算类，或者更改对应的计算类，而不会影响到原来的代码。 在客户类中拥有一个Handle接口的实例变量，当我们根据不同的客户去把不同的计算类对象引用给Handle变量，利用多态绑定运行实现类的方法。 123456789101112public class Customer { private Handle handle; public void getPrice(double price) { handle.handle(price); } public void setHandle(Handle handle) { this.handle = handle; }} 调用 123456789101112public class Main { public static void main(String[] args) { Customer customer = new Customer(); // 根据不同的客户设置不同的计算类 customer.setHandle(new VipHandle()); customer.getPrice(150.5); customer.setHandle(new GenralHandle()); customer.getPrice(120.0); }} 运行结果 模式使用在 easypoi 中可以自定义设置excel数据校验接口，自定义校验类必须实现IExcelVerifyHandler&lt;T&gt;。它是一个接口，里面只有一个ExcelVerifyHandlerResult verifyHandler(T var1) 方法。所以我们可以实现这个接口，然后在verifyHandler方法对记录进行校验，每条记录会作为参数传进来。 12345678910public class TestHandle implements IExcelVerifyHandler&lt;StudentFields&gt; { @Override public ExcelVerifyHandlerResult verifyHandler(StudentFields studentFields) { // 这里实现自定义数据校验，每条记录将作为参数传进来 return null; } } 在我们需要校验的地方，只需要打开校验，并动态的设置校验接口，就可以实现很灵活的数据校验。因为在ImportParams导入参数类中有private IExcelVerifyHandler verifyHandler属性。当导入时打开了校验就会调用verifyHandler.verifyHandler(StudentFields record);对每条记录进行校验。 12345678910111213141516171819202122232425262728293031@Override public ResponseInfo importStudent(File file, HttpServletRequest request) { ImportParams params = new ImportParams(); // 开启数据验证 params.setNeedVerfiy(true); // 从第二行开始读取数据 params.setTitleRows(1); // 设置验证数据接口 params.setVerifyHandler(importHandle); try { ExcelImportResult&lt;StudentFields&gt; result = ExcelImportUtil.importExcelMore(file, StudentFields.class, params); // 成功记录 List&lt;StudentFields&gt; successList = result.getList(); for (StudentFields studentFields : successList) { } // 有失败的记录 if (result.isVerfiyFail()) { // 失败列表 List&lt;StudentFields&gt; failList = result.getFailList(); } return this.getResult(SUCCESS, DO_SUCCESS); } catch (ExcelImportException e) { return this.getResult(FAIL, e.getMessage()); } finally { if (file.exists()) { file.delete(); } } } 这样利用策略模式把数据校验独立出来并进行封装，在后期需要导入不同的excel，进行不同的数据校验只需要新建一个校验实现类继承IExcelVerifyHandler&lt;T&gt;，实现verifyHandler方法，在导入时会调用verifyHandler.verifyHandler(T var)，从而调用到动态链接到我们实现的方法中。。当校验规则改变时，也不需要去修改原来的代码，只需要去独立出来的部分或者新建一个校验类。 设计原则 把可能会变化的部分独立并封装起来，以便以后可以轻易的改动，而部影响不需要变化的部分。 针对接口编程而不是针对实现编程。针对接口的真正意思是针对”超类型”，把变量声明为超类型。即利用多态动态绑定，去执行具体的实现类的方法。而不会绑死在父类方法中。 多用组合，少用继承。 “有一个”可能比”是一个”好。","link":"/2018/12/18/2018-12-18-strategy-pattern/"},{"title":"设计模式之观察者模式","text":"什么是观察者模式定义对象间的一对多依赖，当对象的状态发生改变，它的依赖者都会收到通知。 如何设计自己实现观察者模式观察者模式可以看成是报社与报纸订阅者的关系，但有新的报纸时，此报纸的订阅者都会收到新的报纸。报社(subject主题)，订阅者(observer观察者) 代码如下： 1234567891011121314/** * * 主题接口 * */public interface Subject { void addObserver(Observer var); void removeObserver(Observer var); void notifyObserver();} 观察者接口 12345678910/** * * 观察者接口 * */public interface Observer { void update(Object data); } 定义报社实现主题接口 12345678910111213141516171819202122232425262728293031323334353637public class Newspaper implements Subject { private List&lt;Observer&gt; observers; private Object data; public Newspaper () { observers = new ArrayList&lt;&gt;(); } @Override public void addObserver(Observer var) { if (!observers.contains(var)) { observers.add(var); } } @Override public void removeObserver(Observer var) { if (observers.indexOf(var) &gt; -1) { observers.remove(var); } } @Override public void notifyObserver() { for (Observer observer : observers) { observer.update(data); } } // 测试设置值的改变并通知观察者 public void setData(Object data) { this.data = data; notifyObserver(); }} 报社订阅者实现观察者代码 1234567891011121314public class Subscriber implements Observer{ private String name; public Subscriber(Subject subject, String name) { this.name = name; subject.addObserver(this); } @Override public void update(Object data) { System.out.println(String.format(&quot;%s数据更新 -&gt; %s&quot;, this.name, data)); }} 运行一下 12345678910111213public class Main { public static void main(String[] args) { Subject subject = new Newspaper(); new Subscriber(subject, &quot;张三&quot;); Subscriber subscriber = new Subscriber(subject, &quot;李四&quot;); new Subscriber(subject, &quot;王五&quot;); ((Newspaper) subject).setData(&quot;123456&quot;); subject.removeObserver(subscriber); ((Newspaper) subject).setData(&quot;654321&quot;); }} 运行结果 使用Java内置观察者模式 内置的观察者模式提供了推/拉数据两种模式，调用notifyObservers()时需要观察者自己去拉数据，调用notifyObservers(Object data),则会推数据。 用了内置观察者模式后只剩下了两个类，报社和订阅者。 在订阅者数据更新方法中，还会把被观察者通过参数传进来。 在报社通知观察者之前需要调用setChanged();，确定状态已经改变了，才会去通知观察者。在源码中，通知之前有做状态标记的判断。这么做有个好处就是，比如报社已经开始印刷报纸了，但是还没印完，这时是不应该通知订阅者的。需要全部印刷完，再设置状态已改变的标记，才能去通知。 1234567891011121314151617181920212223242526272829public void notifyObservers(Object arg) { /* * a temporary array buffer, used as a snapshot of the state of * current Observers. */ Object[] arrLocal; synchronized (this) { /* We don't want the Observer doing callbacks into * arbitrary code while holding its own Monitor. * The code where we extract each Observable from * the Vector and store the state of the Observer * needs synchronization, but notifying observers * does not (should not). The worst result of any * potential race-condition here is that: * 1) a newly-added Observer will miss a * notification in progress * 2) a recently unregistered Observer will be * wrongly notified when it doesn't care */ if (!changed) return; arrLocal = obs.toArray(); clearChanged(); } for (int i = arrLocal.length-1; i&gt;=0; i--) ((Observer)arrLocal[i]).update(this, arg); } 订阅者代码 12345678910111213141516171819202122232425public class Subscriber implements Observer { private String name; public Subscriber(Observable observable, String name) { this.name = name; observable.addObserver(this); } /** * This method is called whenever the observed object is changed. An * application calls an &lt;tt&gt;Observable&lt;/tt&gt; object's * &lt;code&gt;notifyObservers&lt;/code&gt; method to have all the object's * observers notified of the change. * * @param o the observable object. * @param arg an argument passed to the &lt;code&gt;notifyObservers&lt;/code&gt; */ @Override public void update(Observable o, Object arg) { if (o instanceof Newspaper) { System.out.println(String.format(&quot;%s数据更新 -&gt; %s&quot;, this.name, arg)); } }} 主题类代码 1234567891011public class Newspaper extends Observable { public void setData(Object data) { // 设置状态已改变 setChanged(); // 推数据方式 notifyObservers(data); // 拉数据方式 //notifyObservers(); }} Java内置观察者模式的问题 内置的观察者模式和自己实现的其实是差不多的，只不过内置的主题是个类，而不是接口。所以这也限制了它。因为Java单继承多实现，所以继承了Observable就不能拥有其他类的功能了。并且它把关键方法setChanged设为protected，这意味着你必须继承Observable才能创建实例组合到自己的对象中。 内置观察者模式违反了OO设计原则，针对接口编程而不是针对实现编程。 违反了’多用组合，少用继承’，意思是因为setChanged方法，你必须继承Observable，才能把实例组合到我们的对象中。 模式使用","link":"/2019/04/16/2019-04-16-observer-pattern/"},{"title":"生成git ssh key","text":"检查是否已经存在ssh key1$ ls -al ~/.ssh 通常名称为以下这几种： id_dsa.pub id_ecdsa.pub id_ed25519.pub id_rsa.pub 如不存在则生成新的key，已存在key则直接复制到github账号上 生成新的ssh key12345678910111213141516171819202122232425262728293031323334353637383940root@ddmcc:~# ssh-keygen -t rsa -b 4096 -C &quot;308119975@qq.com&quot;Generating public/private rsa key pair.Enter file in which to save the key (/root/.ssh/id_rsa): Created directory '/root/.ssh'.Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa.Your public key has been saved in /root/.ssh/id_rsa.pub.The key fingerprint is:SHA256:o8juUP0rMeU0hq1S9+EVegAgwblVtRJezVB0xNSGiSs 308119975@qq.comThe key's randomart image is:+---[RSA 4096]----+| .oo.o+o+*o=++ || o... o..=.+ o|| o oo .o o . || ..o B.E + || ...BS+ = || o..+o..o || . o..o. || o . . || .o .. |+----[SHA256]-----+root@ddmcc:~# lstotal 32drwx------ 4 root root 4096 May 9 20:36 ./drwxr-xr-x 23 root root 4096 Apr 8 21:28 ../-rw------- 1 root root 0 Apr 14 15:43 .bash_history-rw-r--r-- 1 root root 3106 Oct 23 2015 .bashrcdrwx------ 2 root root 4096 Apr 14 15:42 .cache/-rw-r--r-- 1 root root 148 Aug 17 2015 .profiledrwx------ 2 root root 4096 May 9 20:37 .ssh/-rw------- 1 root root 679 Apr 18 23:15 .viminfo-rw------- 1 root root 51 May 9 20:17 .Xauthorityroot@ddmcc:~# cd .sshroot@ddmcc:~/.ssh# lsid_rsa id_rsa.pubroot@ddmcc:~/.ssh# cat id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDA6EeAUcQXeV/G+prk9yTgS8VD1C9XY7GgOH7/X6AXaNc6VfFUg5oKrA2bwJZ36WyJSEwuYfrRkBDrAOWsVG+fZkY11Ia2ReEDQISkLdaklDX9SRNJZUUpjPeBs3VLOB+IFO4f+RWB4M9ojsHC2HNty5XpKBEnU8u5I4nmK7qLrEeT2RoypCLQEQ86fM9ofJi49/qmt4jOka+x8+PKFItx7SdpnhQDG18oEov8qdDOgoijLTe066ELDS/ZTc658P42Op4FN+6rJptYYrNPHcn7RhgFdXCVT9sUS7Wtgmsa65yiyrRtLiw0H7aGV8NLis4AWHfLwXEHF3GXequmraH6p620e/uUB92dS9dvwkPJyWOFf7YHJpxNrB5GBy6b4iPv4UiKFDNfeBdahIejtCIMqm8u6UX0EmQxR4orgPCY6oCfozwdFF2q+ZLs/sIEgT8F1/GrU5C1w+gxuNs+oIVQqJ/wj3QhM+w33Hsha+EZuIFMHcYZTOo2vAi43gNREMX6SxOh8ZLx5QVD4vaW66mVNhxYtTXH3fr/YXFa8/Or/l666vyB7V3j3cqjLoWvueYpbLhqumvqi/OIqDAJST94M/dz1Lip8G97AFxbG41Z7bPZIPq52PvzLS5gOiafUmTTRg4eHseOn5lgLoQSxKmgpgSzDz1kqk+kP4M2shjzWQ== 308119975@qq.comroot@ddmcc:~/.ssh# root@ddmcc:~# 复制key到github上在个人账号-Settings SSH and GPG keys New SSH key Paste your key into the “Key” field Add SSH key","link":"/2019/05/09/2019-05-09-create-ssh-keys/"},{"title":"ubuntu中安装Java","text":"准备工作 下载jdk包 jdk-8u152-linux-x64.tar.gz 将下载包放到ubantu上 （本文放在/opt/java中） 解压下载包 tar -zxvf jdk-8u152-linux-x64.tar.gz 配置环境变量1root@ddmcc:/opt/java# vim /etc/profile 在文件后面追加： 123#set java environmentexport JAVA_HOME=/opt/java/jdk1.8.0_152export PATH=${JAVA_HOME}/bin:${PATH} 追加后： 12345678910111213141516171819202122232425262728293031# /etc/profile: system-wide .profile file for the Bourne shell (sh(1))# and Bourne compatible shells (bash(1), ksh(1), ash(1), ...).if [ &quot;$PS1&quot; ]; then if [ &quot;$BASH&quot; ] &amp;&amp; [ &quot;$BASH&quot; != &quot;/bin/sh&quot; ]; then # The file bash.bashrc already sets the default PS1. # PS1='\\h:\\w\\$ ' if [ -f /etc/bash.bashrc ]; then . /etc/bash.bashrc fi else if [ &quot;`id -u`&quot; -eq 0 ]; then PS1='# ' else PS1='$ ' fi fifiif [ -d /etc/profile.d ]; then for i in /etc/profile.d/*.sh; do if [ -r $i ]; then . $i fi done unset ifi#set java environmentexport JAVA_HOME=/opt/java/jdk1.8.0_152export PATH=${JAVA_HOME}/bin:${PATH} 使Java环境生效执行命令 source /etc/profile ，后输入 java -version查看 如果出现下面的版本，说明安装成功 123java version &quot;1.8.0_152&quot;Java(TM) SE Runtime Environment (build 1.8.0_152-b16)Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode)","link":"/2019/05/09/2019-05-09-installing-java-in-ubuntu/"},{"title":"ubuntu中安装Maven","text":"准备工作 下载maven包 apache-maven-3.5.3-bin.tar.gz 将下载包放到ubantu上 （本文放在/opt/maven中） 解压下载包 tar -zxvf apache-maven-3.5.3-bin.tar.gz 配置环境变量12345678910111213root@ddmcc:/opt/maven# lltotal 8608drwxrwxrwx 3 root root 4096 May 9 21:17 ./drwxr-xr-x 5 root root 4096 May 9 21:16 ../drwxr-xr-x 6 root root 4096 May 9 21:17 apache-maven-3.5.3/-rw-r--r-- 1 root root 8799579 Apr 8 20:51 apache-maven-3.5.3-bin.tar.gzroot@ddmcc:/opt/maven# rm -fr apache-maven-3.5.3-bin.tar.gz root@ddmcc:/opt/maven# lltotal 12drwxrwxrwx 3 root root 4096 May 9 21:18 ./drwxr-xr-x 5 root root 4096 May 9 21:16 ../drwxr-xr-x 6 root root 4096 May 9 21:17 apache-maven-3.5.3/root@ddmcc:/opt/maven# vim /etc/profile 在文件后面追加： 123#set maven environmentexport MAVEN_HOME=/opt/maven/apache-maven-3.5.3export PATH=${MAVEN_HOME}/bin:$PATH 追加后： 123456789101112131415161718192021222324252627282930313233343536# /etc/profile: system-wide .profile file for the Bourne shell (sh(1))# and Bourne compatible shells (bash(1), ksh(1), ash(1), ...).if [ &quot;$PS1&quot; ]; then if [ &quot;$BASH&quot; ] &amp;&amp; [ &quot;$BASH&quot; != &quot;/bin/sh&quot; ]; then # The file bash.bashrc already sets the default PS1. # PS1='\\h:\\w\\$ ' if [ -f /etc/bash.bashrc ]; then . /etc/bash.bashrc fi else if [ &quot;`id -u`&quot; -eq 0 ]; then PS1='# ' else PS1='$ ' fi fifiif [ -d /etc/profile.d ]; then for i in /etc/profile.d/*.sh; do if [ -r $i ]; then . $i fi done unset ifi#set java environmentexport JAVA_HOME=/opt/java/jdk1.8.0_152export PATH=${JAVA_HOME}/bin:${PATH}#set maven environmentexport MAVEN_HOME=/opt/maven/apache-maven-3.5.3export PATH=${MAVEN_HOME}/bin:$PATH 使环境生效执行命令 source /etc/profile ，后输入 mvn -v查看 如果出现下面的版本，说明安装成功 123456Apache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-25T03:49:05+08:00)Maven home: /opt/maven/apache-maven-3.5.3Java version: 1.8.0_152, vendor: Oracle CorporationJava home: /opt/java/jdk1.8.0_152/jreDefault locale: en_US, platform encoding: UTF-8OS name: &quot;linux&quot;, version: &quot;4.4.0-21-generic&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;","link":"/2019/05/09/2019-05-09-installing-maven-in-ubuntu/"},{"title":"github+阿里云容器镜像服务+jenkins自动化持续部署","text":"工作流程 提交代码到github触发webhooks通知镜像容器服务 镜像容器服务拉取代码根据写好的Dockerfile构建镜像 镜像构建完成触发通知jenkins,jenkins插件Generic Webhook Trigger收到通知 拉取镜像部署 准备工作github仓库 新建代码仓库 安装jenkins 安装jenkins 系统管理-&gt;插件管理-&gt;安装 Generic Webhook Trigger 插件 新建任务,直接到构建触发器如果安装了 Generic Webhook Trigger 插件就可以看到选项,勾选它。填写token,此处填写的token为镜像服务配置链接中 invoke?itoken={} 括号的值。 新增header token参数 构建中添加构建步骤,选执行shell。拉取镜像时,如果新建的镜像时puclic则不需要登录,private则需要先登录.写好后应用,保存。 生成API token,点击用户名-&gt;设置-&gt;API token-&gt;填写生成.此处填写的token为镜像服务配置链接中 http://账户名:{}@jenkins地址 括号的值 容器镜像服务 容器镜像服务上新建镜像仓库 绑定github账号,代码源设置github仓库,勾选代码变更自动构建镜像 新建构建规则,选定版本变更构建或分支代码构建,设置Dockerfile路径,版本 新建仓库 新建构建规则 新建触发器 格式为: http://账户名:加密API TOKEN@jenkins地址/generic-webhook-trigger/invoke?token=jenkins任务配置的token 工作git提交代码或直接github上修改代码触发。","link":"/2019/05/15/2019-05-14-automatic-continuous-ntegration/"},{"title":"github+jenkins自动化持续部署","text":"工作流程 提交代码到github触发webhooks通知jenkins jenkins拉取代码构建，发布 准备工作github仓库 新建代码仓库 添加webhook 创建github access tokens用户 -&gt; Settings -&gt; Developer settings -&gt; Personal access tokens -&gt; Generate new token 新建后复制token,离开页面后就看不到了 jenkins上添加github token管理jenkins -&gt; 系统管理 -&gt; 找到 Github 服务器 没有的话先安装 Github Plugin 插件 Secret 就是刚刚生成的token 添加后可以点击连接测试，成功会显示github的用户名。 新建任务构建一个Maven项目新建任务 -&gt; 构建一个Maven项目 填入项目地址github项目 -&gt; 填入项目地址 Git -&gt; 填入项目地址Source Code Management -&gt; Git -&gt; 填入项目地址 勾选触发器Build Triggers -&gt; 勾选 GitHub hook trigger for GITScm polling 编写脚本Post Steps -&gt; Execute shell -&gt; 填入脚本 start.sh: export ZIPKIN=itoken-zipkin-1.0.0-SNAPSHOT.jar export PORT=9411 case &quot;$1&quot; in start) ## 启动ZIPKIN echo &quot;--------ZIPKIN 开始启动--------------&quot; BUILD_ID=DONTKILLME nohup java -jar $ZIPKIN --server.port=$PORT --spring.profiles.active=prod &gt;/dev/null 2&gt;log &amp; ZIPKIN_PID=`lsof -i:$PORT|grep &quot;LISTEN&quot;|awk '{print $2}'` until [ -n &quot;$ZIPKIN_PID&quot; ] do ZIPKIN_PID=`lsof -i:$PORT|grep &quot;LISTEN&quot;|awk '{print $2}'` done echo &quot;ZIPKIN_PID is $ZIPKIN_PID&quot; echo &quot;--------ZIPKIN 启动成功--------------&quot; ;; stop) P_ID=`ps -ef | grep -w $ZIPKIN | grep -v &quot;grep&quot; | awk '{print $2}'` if [ &quot;$P_ID&quot; == &quot;&quot; ]; then echo &quot;===ZIPKIN process not exists or stop success&quot; else kill $P_ID echo &quot;ZIPKIN killed success&quot; fi echo &quot;===stop success===&quot; ;; restart) $0 stop sleep 5 $0 start echo &quot;===restart success===&quot; ;; esac exit 0 工作git提交代码或直接github上修改代码触发。","link":"/2019/06/07/2019-05-14-automatic-continuous-ntegration-with-centos/"},{"title":"ubuntu中安装jenkins","text":"安装原来把jenkins安装docker容器里，太不方便了，还有权限的问题。 wget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add - sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list' apt-get update apt-get install jenkins 1234567891011121314151617181920root@ddmcc:~# wget -q -O - https://pkg.jenkins.io/debian/jenkins-ci.org.key | sudo apt-key add -OKroot@ddmcc:~# sh -c 'echo deb http://pkg.jenkins.io/debian-stable binary/ &gt; /etc/apt/sources.list.d/jenkins.list'root@ddmcc:~# apt-get updateHit:1 https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial InReleaseHit:2 http://security.ubuntu.com/ubuntu xenial-security InReleaseIgn:3 http://pkg.jenkins.io/debian-stable binary/ InReleaseGet:4 http://pkg.jenkins.io/debian-stable binary/ Release [2,042 B]Get:5 http://pkg.jenkins.io/debian-stable binary/ Release.gpg [181 B]Get:6 http://pkg.jenkins.io/debian-stable binary/ Packages [14.9 kB]Hit:7 http://us.archive.ubuntu.com/ubuntu xenial InReleaseGet:8 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]Get:9 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]Get:10 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [956 kB]Get:11 http://us.archive.ubuntu.com/ubuntu xenial-updates/main i386 Packages [823 kB]Get:12 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [748 kB]Get:13 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe i386 Packages [684 kB]Fetched 3,444 kB in 42s (80.7 kB/s)Reading package lists... Doneroot@ddmcc:~# apt-get install jenkins 然后就等待下载完成后自动安装后，打开 http://ip:8080/ 进入jenkins。密码在/var/lib/jenkins/secrets/initialAdminPassword ，登陆后安装插件。 遇到的问题安装后启动，显示启动失败了。 1234567891011121314151617181920212223242526272829303132333435Reading package lists... DoneBuilding dependency tree Reading state information... DoneThe following additional packages will be installed: daemonThe following NEW packages will be installed: daemon jenkins0 upgraded, 2 newly installed, 0 to remove and 137 not upgraded.Need to get 76.8 MB of archives.After this operation, 77.7 MB of additional disk space will be used.Do you want to continue? [Y/n] yGet:1 http://pkg.jenkins.io/debian-stable binary/ jenkins 2.164.3 [76.7 MB] Get:2 http://us.archive.ubuntu.com/ubuntu xenial/universe amd64 daemon amd64 0.6.4-1 [98.2 kB]Fetched 76.8 MB in 1min 54s (672 kB/s)Selecting previously unselected package daemon.(Reading database ... 60101 files and directories currently installed.)Preparing to unpack .../daemon_0.6.4-1_amd64.deb ...Unpacking daemon (0.6.4-1) ...Selecting previously unselected package jenkins.Preparing to unpack .../jenkins_2.164.3_all.deb ...Unpacking jenkins (2.164.3) ...Processing triggers for man-db (2.7.5-1) ...Processing triggers for systemd (229-4ubuntu21.21) ...Processing triggers for ureadahead (0.100.0-19) ...Setting up daemon (0.6.4-1) ...Setting up jenkins (2.164.3) ...Job for jenkins.service failed because the control process exited with error code. See &quot;systemctl status jenkins.service&quot; and &quot;journalctl -xe&quot; for details.invoke-rc.d: initscript jenkins, action &quot;start&quot; failed.dpkg: error processing package jenkins (--configure): subprocess installed post-installation script returned error exit status 1Processing triggers for systemd (229-4ubuntu21.21) ...Processing triggers for ureadahead (0.100.0-19) ...Errors were encountered while processing: jenkinsE: Sub-process /usr/bin/dpkg returned an error code (1) 发现启动失败了，根据提示输入 journalctl -xe 123456789101112131415161718192021222324252627282930313233343536root@ddmcc:~# journalctl -xe-- Unit acpid.service has finished starting up.-- -- The start-up result is done.May 15 19:40:23 ddmcc jenkins[26604]: ERROR: No Java executable found in current PATH: /bin:/usr/bin:/sbin:/usr/sbinMay 15 19:40:23 ddmcc jenkins[26604]: If you actually have java installed on the system make sure the executable is in the aforementioned path and that 'type -p java' returns the java executable pathMay 15 19:40:23 ddmcc systemd[1]: Starting LSB: Start Jenkins at boot time...-- Subject: Unit jenkins.service has begun start-up-- Defined-By: systemd-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel-- -- Unit jenkins.service has begun starting up.May 15 19:40:23 ddmcc systemd[1]: jenkins.service: Control process exited, code=exited status=1May 15 19:40:23 ddmcc systemd[1]: Failed to start LSB: Start Jenkins at boot time.-- Subject: Unit jenkins.service has failed-- Defined-By: systemd-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel-- -- Unit jenkins.service has failed.-- -- The result is failed.May 15 19:40:23 ddmcc systemd[1]: jenkins.service: Unit entered failed state.May 15 19:40:23 ddmcc systemd[1]: jenkins.service: Failed with result 'exit-code'.May 15 19:40:23 ddmcc systemd[1]: Reloading.May 15 19:40:23 ddmcc systemd[1]: Started ACPI event daemon.-- Subject: Unit acpid.service has finished start-up-- Defined-By: systemd-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel-- -- Unit acpid.service has finished starting up.-- -- The start-up result is done.May 15 19:44:20 ddmcc dhclient[1075]: DHCPREQUEST of 192.168.171.135 on ens33 to 192.168.171.254 port 67 (xid=0x1fd4248f)May 15 19:44:20 ddmcc dhclient[1075]: DHCPACK of 192.168.171.135 from 192.168.171.254May 15 19:44:20 ddmcc dhclient[1075]: bound to 192.168.171.135 -- renewal in 830 seconds.lines 1962-1995/1995 (END) 发现是Java环境的问题，提示没找到,输入 echo $PATH 查看环境变量。建立软连接 ln -s /opt/Java/jdk1.8.0_152/bin/java /usr/bin/java ,重启jenkins,然后输入 systemctl status jenkins.service jenkins已正常启动。 123456789101112131415161718192021root@ddmcc:~# echo $PATH/opt/Maven/apache-maven-3.5.3/bin:/opt/Java/jdk1.8.0_152/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/binroot@ddmcc:~# ln -s /opt/Java/jdk1.8.0_152/bin/java /usr/bin/javaroot@ddmcc:~# service jenkins restartroot@ddmcc:~# systemctl status jenkins.service● jenkins.service - LSB: Start Jenkins at boot time Loaded: loaded (/etc/init.d/jenkins; bad; vendor preset: enabled) Active: active (exited) since Wed 2019-05-15 19:50:09 CST; 10s ago Docs: man:systemd-sysv-generator(8) Process: 26691 ExecStart=/etc/init.d/jenkins start (code=exited, status=0/SUCCESS)May 15 19:50:07 ddmcc systemd[1]: Stopped LSB: Start Jenkins at boot time.May 15 19:50:07 ddmcc systemd[1]: Starting LSB: Start Jenkins at boot time...May 15 19:50:07 ddmcc jenkins[26691]: Correct java version foundMay 15 19:50:07 ddmcc jenkins[26691]: * Starting Jenkins Automation Server jenkinsMay 15 19:50:08 ddmcc su[26726]: Successful su for jenkins by rootMay 15 19:50:08 ddmcc su[26726]: + ??? root:jenkinsMay 15 19:50:08 ddmcc su[26726]: pam_unix(su:session): session opened for user jenkins by (uid=0)May 15 19:50:09 ddmcc jenkins[26691]: ...done.May 15 19:50:09 ddmcc systemd[1]: Started LSB: Start Jenkins at boot time.root@ddmcc:~#","link":"/2019/05/15/2019-05-15-installing-jenkins-in-ubuntu/"},{"title":"TCP三次握手与四次挥手","text":"TCP是什么？TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 三次握手 TCP使用三次握手建立一个连接：第一次握手：客户端发送SYN=x包至服务器，并进入SYN_SENT状态，等待服务器确认 第二次握手：服务器收到客户端的SYN包，发送一个ACK=x+1，同时发送自己的SYN=y，此时服务器进入SYN_RCVD状态 第三次握手：客户端接收到服务器发送的SYN(y)+ACK(x+1)后，进入ESTABLISHED状态，并发送服务器SYN包的确认ACK(y+1)，服务器接收到客户端ACK(y+1)后，进入ESTABLISHED状态 当客户端和服务器都进入ESTABLISHED状态后，客户端和服务器之间就可以开始双向传递数据了 为什么要三次握手在谢希仁著《计算机网络》书中同时举了一个例子，如下： “已失效的连接请求报文段”的产生在这样一种情况下：client发出的第一个连接请求报文段并没有丢失，而是在某个网络结点长时间的滞留了，以致延误到连接释放以后的某个时间才到达server。本来这是一个早已失效的报文段。但server收到此失效的连接请求报文段后，就误认为是client再次发出的一个新的连接请求。于是就向client发出确认报文段，同意建立连接。假设不采用“三次握手”，那么只要server发出确认，新的连接就建立了。由于现在client并没有发出建立连接的请求，因此不会理睬server的确认，也不会向server发送数据。但server却以为新的运输连接已经建立，并一直等待client发来数据。这样，server的很多资源就白白浪费掉了。采用“三次握手”的办法可以防止上述现象发生。例如刚才那种情况，client不会向server的确认发出确认。server由于收不到确认，就知道client并没有要求建立连接。” 四次挥手 TCP使用四次挥手关闭一个连接：第一次挥手：主动关闭方发送一个FIN并进入FIN_WAIT1状态 第二次挥手：被动关闭方接收到主动关闭方发送的FIN并发送ACK，此时被动关闭方进入CLOSE_WAIT状态；主动关闭方收到被动关闭方的ACK后，进入FIN_WAIT2状态 第三次挥手：被动关闭方发送一个FIN并进入LAST_ACK状态 第四次挥手：主动关闭方收到被动关闭方发送的FIN并发送ACK，此时主动关闭方进入TIME_WAIT状态，经过2MSL时间后关闭连接；被动关闭方收到主动关闭方的ACK后，关闭连接 为什么要四次分手那四次分手又是为何呢？TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议。TCP是全双工模式，这就意味着，当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了；但是，这个时候主机1还是可以接受来自主机2的数据；当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的；当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。如果要正确的理解四次分手的原理，就需要了解四次分手过程中的状态变化。 FIN_WAIT_1: 这个状态要好好解释一下，其实FIN_WAIT_1和FIN_WAIT_2状态的真正含义都是表示等待对方的FIN报文。而这两种状态的区别是：FIN_WAIT_1状态实际上是当SOCKET在ESTABLISHED状态时，它想主动关闭连接，向对方发送了FIN报文，此时该SOCKET即进入到FIN_WAIT_1状态。而当对方回应ACK报文后，则进入到FIN_WAIT_2状态，当然在实际的正常情况下，无论对方何种情况下，都应该马上回应ACK报文，所以FIN_WAIT_1状态一般是比较难见到的，而FIN_WAIT_2状态还有时常常可以用netstat看到。（主动方） FIN_WAIT_2: 上面已经详细解释了这种状态，实际上FIN_WAIT_2状态下的SOCKET，表示半连接，也即有一方要求close连接，但另外还告诉对方，我暂时还有点数据需要传送给你(ACK信息)，稍后再关闭连接。（主动方） CLOSE_WAIT 这种状态的含义其实是表示在等待关闭。怎么理解呢？当对方close一个SOCKET后发送FIN报文给自己，你系统毫无疑问地会回应一个ACK报文给对方，此时则进入到CLOSE_WAIT状态。接下来呢，实际上你真正需要考虑的事情是察看你是否还有数据发送给对方，如果没有的话，那么你也就可以 close这个SOCKET，发送FIN报文给对方，也即关闭连接。所以你在CLOSE_WAIT状态下，需要完成的事情是等待你去关闭连接。（被动方） LAST_ACK: 这个状态还是比较容易好理解的，它是被动关闭一方在发送FIN报文后，最后等待对方的ACK报文。当收到ACK报文后，也即可以进入到CLOSED可用状态了。（被动方） TIME_WAIT: 表示收到了对方的FIN报文，并发送出了ACK报文，就等2MSL后即可回到CLOSED可用状态了。如果FINWAIT1状态下，收到了对方同时带FIN标志和ACK标志的报文时，可以直接进入到TIME_WAIT状态，而无须经过FIN_WAIT_2状态。（主动方） CLOSED: 表示连接中断。 HTTP连接HTTP协议即超文本传送协议(Hypertext Transfer Protocol )，是Web联网的基础，也是手机联网常用的协议之一，HTTP协议是建立在TCP协议之上的一种应用。HTTP连接最显著的特点是客户端发送的每次请求都需要服务器回送响应，在请求结束后，会主动释放连接。从建立连接到关闭连接的过程称为“一次连接”。1）在HTTP 1.0中，客户端的每次请求都要求建立一次单独的连接，在处理完本次请求后，就自动释放连接。 2）在HTTP 1.1中则可以在一次连接中处理多个请求，并且多个请求可以重叠进行，不需要等待一个请求结束后再发送下一个请求。 由于HTTP在每次请求结束后都会主动释放连接，因此HTTP连接是一种“短连接”，要保持客户端程序的在线状态，需要不断地向服务器发起连接请求。通常 的做法是即时不需要获得任何数据，客户端也保持每隔一段固定的时间向服务器发送一次“保持连接”的请求，服务器在收到该请求后对客户端进行回复，表明知道 客户端“在线”。若服务器长时间无法收到客户端的请求，则认为客户端“下线”，若客户端长时间无法收到服务器的回复，则认为网络已经断开。 SOCKET原理套接字（socket）概念套接字（socket）是通信的基石，是支持TCP/IP协议的网络通信的基本操作单元。它是网络通信过程中端点的抽象表示，包含进行网络通信必须的五种信息：连接使用的协议，本地主机的IP地址，本地进程的协议端口，远地主机的IP地址，远地进程的协议端口。应用层通过传输层进行数据通信时，TCP会遇到同时为多个应用程序进程提供并发服务的问题。多个TCP连接或多个应用程序进程可能需要通过同一个 TCP协议端口传输数据。为了区别不同的应用程序进程和连接，许多计算机操作系统为应用程序与TCP／IP协议交互提供了套接字(Socket)接口。传输层屏蔽了低层的实现过程，只抽象成给上层应用层使用的接口，这接口就是Socket。 建立socket连接建立Socket连接至少需要一对套接字，其中一个运行于客户端，称为ClientSocket ，另一个运行于服务器端，称为ServerSocket 。套接字之间的连接过程分为三个步骤：服务器监听，客户端请求，连接确认。服务器监听：服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态，等待客户端的连接请求。客户端请求：指客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。连接确认：当服务器端套接字监听到或者说接收到客户端套接字的连接请求时，就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发 给客户端，一旦客户端确认了此描述，双方就正式建立连接。而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求。 SOCKET连接与TCP连接创建Socket连接时，可以指定使用的传输层协议，Socket可以支持不同的传输层协议（TCP或UDP），当使用TCP协议进行连接时，该Socket连接就是一个TCP连接。 Socket连接与HTTP连接由于通常情况下Socket连接就是TCP连接，因此Socket连接一旦建立，通信双方即可开始相互发送数据内容，直到双方连接断开。但在实际网 络应用中，客户端到服务器之间的通信往往需要穿越多个中间节点，例如路由器、网关、防火墙等，大部分防火墙默认会关闭长时间处于非活跃状态的连接而导致 Socket 连接断连，因此需要通过轮询告诉网络，该连接处于活跃状态。而HTTP连接使用的是“请求—响应”的方式，不仅在请求时需要先建立连接，而且需要客户端向服务器发出请求后，服务器端才能回复数据。很多情况下，需要服务器端主动向客户端推送数据，保持客户端与服务器数据的实时与同步。此时若双方建立的是Socket连接，服务器就可以直接将数 据传送给客户端；若双方建立的是HTTP连接，则服务器需要等到客户端发送一次请求后才能将数据传回给客户端，因此，客户端定时向服务器端发送连接请求， 不仅可以保持在线，同时也是在“询问”服务器是否有新的数据，如果有就将数据传给客户端。 转自 TCP三次握手/四次挥手 通俗大白话来理解TCP协议的三次握手和四次分手","link":"/2019/05/19/2019-05-19-tcp-three-shook-hands/"},{"title":"设计模式之工厂方法模式","text":"什么是工厂方法模式定义一个创建对象的接口,但由子类决定要实例化的类是哪一个。工厂方法让类把实例化推迟到子类。 Creator是一个类(接口),有一个工厂方法 factoryMethod() ,由子类去实现。 ConcreteCreator实现了 factoryMethod() 以实际制造出一种或多种产品。如ConcreteProduct。所有的产品实现公同的接口Product,这样生产出的产品就可以引用这个接口,而不是真正的实现类。 如何设计假设有一个音乐播放器,可以根据用户的喜好去推送音乐。可以有中文和英文歌曲,下面又有分类:电子(Electronic),华语(Chinese),流行(Popular)等。 代码如下： 定义工厂接口123456789101112131415161718192021package creator;import music.Music;/** * * 音乐生产接口 * */public interface MusicCreator { int POPULAR = 1; int ROCK = 2; int ELECTRONIC = 3; Music createMusic(int style);} 具体的生产类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package creator;import music.Music;import music.ChineseElectronicMusic;import music.ChinesePopularMusic;import music.ChineseRockMusic;/** * * 中文歌曲工厂具体实现类 * */public class ChineseMusicCreator implements MusicCreator { @Override public Music createMusic(int style) { Music music; switch (style) { case POPULAR: music = new ChinesePopularMusic(); break; case ROCK: music = new ChineseRockMusic(); break; case ELECTRONIC: music = new ChineseElectronicMusic(); break; default: music = new ChinesePopularMusic(); } return music; }}package creator;import music.EnglishElectronicMusic;import music.EnglishPopularMusic;import music.EnglishRockMusic;import music.Music;/** * * 英文歌曲工厂具体实现类 * */public class EnglishMusicCreator implements MusicCreator { @Override public Music createMusic(int style) { Music music; switch (style) { case POPULAR: music = new EnglishPopularMusic(); break; case ROCK: music = new EnglishRockMusic(); break; case ELECTRONIC: music = new EnglishElectronicMusic(); break; default: music = new EnglishPopularMusic(); } return music; }} 产品接口12345678910111213package music;/** * * 歌曲接口 * */public interface Music { // 播放音乐 void playMusic();} 具体的产品123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package music;public class ChineseElectronicMusic implements Music { @Override public void playMusic() { System.out.println(&quot;中文电子音乐~~~~&quot;); }}package music;public class ChinesePopularMusic implements Music { @Override public void playMusic() { System.out.println(&quot;中文流行音乐!!!&quot;); }}package music;public class ChineseRockMusic implements Music { @Override public void playMusic() { System.out.println(&quot;中文摇滚音乐~~~~~&quot;); }}package music;public class EnglishElectronicMusic implements Music { @Override public void playMusic() { System.out.println(&quot;英文电子音乐~~~~&quot;); }}package music;public class EnglishPopularMusic implements Music { @Override public void playMusic() { System.out.println(&quot;英文流行音乐!!!&quot;); }}package music;public class EnglishRockMusic implements Music { @Override public void playMusic() { System.out.println(&quot;英文摇滚音乐~~~~~&quot;); }} 客户端代码12345678910111213141516171819import creator.ChineseMusicCreator;import creator.EnglishMusicCreator;import creator.MusicCreator;import music.Music;public class Main { public static void main(String[] args) { MusicCreator englishMusicCreator = new EnglishMusicCreator(); Music englishMusic = englishMusicCreator.createMusic(MusicCreator.POPULAR); englishMusic.playMusic(); MusicCreator chineseMusicCreator = new ChineseMusicCreator(); Music chineseMusic = chineseMusicCreator.createMusic(MusicCreator.POPULAR); chineseMusic.playMusic(); }} 在简单工厂模式中，我们会把所有的产品在一个类中去判断，然后生产出要的产品。这就导致如果有新的种类产品的时候我们就要去修改原来的工厂方法，这就违反了开-闭原则。 工厂方法模式是对简单工厂模式进一步的解耦，因为在工厂方法模式中是一个或多个子类对应一个工厂类，而这些工厂类都实现于一个抽象接口。这相当于是把原本会因为业务代码而庞大的简单工厂类，拆分成了一个个的工厂类，这样代码就不会都耦合在同一个类里了，就像把一个大蛋糕切成了多个小蛋糕。 设计原则 依赖倒置原则,即不能让高级组件依赖低级组件,而且不管高级组件或低级组件,都应该依赖抽象。 在上面的例子中MusicCreator是高级组件,具体的音乐实现类是低级组件,它们都依赖了Music这个抽象。所以遵循了依赖倒置原则。","link":"/2019/05/21/2019-05-21-factory-method-pattern/"},{"title":"ConcurrentHashMap","text":"JDK1.8之前ConcurrentHashMap concurrentHashMap在JDK1.5出现的，为了解决HashMap线程不安全问题和Hashtable使用synchronized导致并发性能低问题。 在1.7中使用分段锁来提升map的并发性能。在 ConcurrentHashMap 有一个 Segment 数组，(Segment&lt;K,V&gt; 是ConcurrentHashMap内部类)，将HashMap分成多个段(默认分成16个Segment，通过hash来定位Segment的位置),将锁的颗粒度降低至一个段(即一个Segment数组)。 Segment 继承了 ReentrantLock 表示Segment是一个可重入锁,ConcurrentHashMap通过可重入锁来实现分段锁机制。 在Segment类中有一个volatile HashEntry&lt;K,V&gt;[] table桶数组(HashEntry也是ConcurrentHashMap的一个内部类,用作储存键值数据的节点代表一个桶),而每个桶又是一个单向链表。 结构如图: 源码如下: public class ConcurrentHashMap&lt;K, V&gt; extends AbstractMap&lt;K, V&gt; implements ConcurrentMap&lt;K, V&gt;, Serializable { final Segment&lt;K,V&gt;[] segments; static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable { transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; } // 桶 static final class HashEntry&lt;K,V&gt; { final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; } } JDK1.8的ConcurrentHashMap1.8中主要有两点不同 抛弃了分段锁,利用数组+链表+红黑树来实现,对数组的每个元素来加锁,将锁的颗粒度降至每个节点(即每个桶)。ConcurrentHashMap中有一个volatile Node&lt;K,V&gt;[] table数组,Node&lt;K,V&gt;是ConcurrentHashMap的一个内部类,继承Map.Entry&lt;K,V&gt;。 增加了红黑树来保存数据。尽管好的hash算法能降低冲突,但在大的扩容因子和大数据量下,也会提高冲突的几率(扩容因子小会影响性能,因为扩容很消耗性能),当每个桶的Node节点大于等于8时,将单向链表转为红黑树来提高查询效率(如果数组长度&gt;64才会转),单向链表查询的复杂度为O(n),红黑树的查询复杂度为O(logn),所以能提高查询的效率。 当删除红黑树时,如果数量&lt;=6,则会转回单向链表。 结构: 链表转为红黑树private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) { Node&lt;K,V&gt; b; int n, sc; if (tab != null) { // 如果table &lt; MIN_TREEIFY_CAPACITY,则不转,直接扩容,MIN_TREEIFY_CAPACITY默认为64 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) tryPresize(n &lt;&lt; 1); // 通过CAS得到指定位置Node节点,加锁转换 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) { synchronized (b) { if (tabAt(tab, index) == b) { TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) { TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; } setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); } } } } } 红黑树转为链表static &lt;K,V&gt; Node&lt;K,V&gt; untreeify(Node&lt;K,V&gt; b) { Node&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; q = b; q != null; q = q.next) { Node&lt;K,V&gt; p = new Node&lt;K,V&gt;(q.hash, q.key, q.val, null); if (tl == null) hd = p; else tl.next = p; tl = p; } return hd; } 常用方法get方法public V get(Object key) { Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) { if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; } else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) { if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; } } return null; } 获取key的hash值,再通过 speed() 方法,对高位也进行hash,然后在查询对应的桶。 static final int spread(int h) { return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS; } 然后通过tabAt()放法获取链表或红黑树的第一个节点,然后遍历通过key的hash值查询相应的value。tabAt()方式是通过Unsafe类的getObjectVolatile方法来获取值,volatile可以保证值的可见性,从而保证读到的值是最新的。 put方法final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) { Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { V oldVal = null; synchronized (f) { if (tabAt(tab, i) == f) { if (fh &gt;= 0) { binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) { K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; } } } else if (f instanceof TreeBin) { Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } addCount(1L, binCount); return null; } 1,数组为空,是否有数据,为空则先初始化table数组,然后再次进入循环。 2,判断目标桶上首节点是否为空,为空则直接不加锁新增节点。 3,判断目标桶是否在扩容操作(MOVED为正在转移的节点的hash值，值为-1),返回扩容后的table数组,重新进入循环。 4,对桶进行加锁(即对单向链表或红黑树),判断链表或红黑树进行新增Node节点,当onlyIfAbsent为false时会替换原来的值。 5,新增后判断binCount是否需要转换结构,oldVal不为空直接返回oldVal,oldVal不为空一种可能是本来就是红黑树结构了,还有就是链表结构,但是没有新增节点。 6,调用addCount增加table中Node的数量,有可能会触发扩容操作。 size方法有size()和mappingCount()两个方法能获取元素的个数。在添加和删除元素时，会通过CAS操作更新ConcurrentHashMap的baseCount属性值来统计元素个数。但是CAS操作可能会失败，因此，ConcurrentHashMap又定义了一个CounterCell数组来记录CAS操作失败时的元素个数 final long sumCount() { CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) { for (int i = 0; i &lt; as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum; } // 只返回int个数 public int size() { long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } public long mappingCount() { long n = sumCount(); return (n &lt; 0L) ? 0L : n; // ignore transient negative values }","link":"/2019/05/27/2019-05-27-concurrentHashMap/"},{"title":"阿里云服务器中安装Java","text":"准备工作 下载jdk包 jdk-8u152-linux-x64.tar.gz 将下载包放到服务器上 （本文放在/usr/java中） 解压下载包 tar -zxvf jdk-8u152-linux-x64.tar.gz 配置环境变量1[root@iZuf6fc5fgstkxrkw804kxZ java]# vim /etc/profile 在文件后面追加： 123#set java environmentexport JAVA_HOME=/usr/java/jdk1.8.0_152export PATH=${JAVA_HOME}/bin:${PATH} 使Java环境生效执行命令 source /etc/profile ，后输入 java -version查看 123456[root@iZuf6fc5fgstkxrkw804kxZ java]# source /etc/profile[root@iZuf6fc5fgstkxrkw804kxZ java]# java -versionjava version &quot;1.8.0_152&quot;Java(TM) SE Runtime Environment (build 1.8.0_152-b16)Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode)[root@iZuf6fc5fgstkxrkw804kxZ java]# 如果出现下面的版本，说明安装成功 123java version &quot;1.8.0_152&quot;Java(TM) SE Runtime Environment (build 1.8.0_152-b16)Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode)","link":"/2019/05/29/2019-05-29-installing-java-in-linux/"},{"title":"阿里云服务器中安装Maven","text":"准备工作 下载maven包 apache-maven-3.5.3-bin.tar.gz 将下载包放到ubantu上 （本文放在/usr/maven中） 解压下载包 tar -zxvf apache-maven-3.5.3-bin.tar.gz 配置环境变量12345[root@iZuf6fc5fgstkxrkw804kxZ maven]# lltotal 8600drwxr-xr-x 6 root root 4096 May 29 23:26 apache-maven-3.5.3-rw-r--r-- 1 root root 8799579 Apr 8 20:51 apache-maven-3.5.3-bin.tar.gz[root@iZuf6fc5fgstkxrkw804kxZ maven]# vim /etc/profile 在文件后面追加： 123#set maven environmentexport MAVEN_HOME=/opt/maven/apache-maven-3.5.3export PATH=${MAVEN_HOME}/bin:$PATH 追加后： 123456789101112....#set java environmentexport JAVA_HOME=/usr/java/jdk1.8.0_152export PATH=${JAVA_HOME}/bin:${PATH}#set maven environmentexport MAVEN_HOME=/usr/maven/apache-maven-3.5.3export PATH=${MAVEN_HOME}/bin:$PATH 使环境生效执行命令 source /etc/profile ，后输入 mvn -v查看 如果出现下面的版本，说明安装成功 123456789[root@iZuf6fc5fgstkxrkw804kxZ maven]# source /etc/profile[root@iZuf6fc5fgstkxrkw804kxZ maven]# mvn -vApache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-25T03:49:05+08:00)Maven home: /usr/maven/apache-maven-3.5.3Java version: 1.8.0_152, vendor: Oracle CorporationJava home: /usr/java/jdk1.8.0_152/jreDefault locale: en_US, platform encoding: UTF-8OS name: &quot;linux&quot;, version: &quot;3.10.0-514.26.2.el7.x86_64&quot;, arch: &quot;amd64&quot;, family: &quot;unix&quot;[root@iZuf6fc5fgstkxrkw804kxZ maven]#","link":"/2019/05/29/2019-05-29-installing-maven-in-linux/"},{"title":"ThreadLocal的使用与源码","text":"使用当有一个单例类中有实例变量，而业务逻辑又要对变量进行处理，当有多个线程同时操作时，如果没有给处理代码加上锁，就有可能出现线程安全问题。如：我们最常见的获取JDBC连接的连接，还有我们交给Spring容器管理的类等 1234567891011121314151617181920212223242526272829303132333435363738394041424344public static void main(String[] args) throws InterruptedException { // 假设此时有两个线程 main代表线程a thread代表线程b，当线程a执行到设置完name的值，这时线程b拿到了cpu的时间片，执行setName // 这是线程b的name也变成了线程b的name，因为Test对象是单例，两个线程操作的是同一个对象。 Test test = Test.getInstance(); System.out.println(Thread.currentThread().getName()); test.setName(Thread.currentThread().getName()); Thread thread = new Thread(() -&gt; { Test test1 = Test.getInstance(); test1.setName(Thread.currentThread().getName()); System.out.println(test1.getName()); }); thread.start(); thread.join(); System.out.println(test.getName());}// 一个单例类public class Test { private String name; private Test() { } private static Test test = null; public static Test getInstance() { if (null == test) { test = new Test(); } return test; } public String getName() { return name; } public void setName(String name) { this.name = name; } } 输出结果为： 在上面的Test单例的，就如我们交给Spring管理的Bean， 当有两个线程同时操作时，其中一个先操作修改了，刚好另一个线程拿到cpu时间片，就会引发线程安全问题。 所以我们可能需要在修改数据的地方加上同步锁，但这样性能又不好。这时就可以使用ThreadLocal了。 12345678910111213141516171819202122232425public class Test { private ThreadLocal&lt;String&gt; name = new ThreadLocal&lt;&gt;(); private Test() { } private static Test test = null; public static Test getInstance() { if (null == test) { test = new Test(); } return test; } public String getName() { return name.get(); } public void setName(String name) { this.name.set(name); }} 再运行： 发现两个线程已经互不影响了，即使线程a设置了自己线程名，b线程输出的还是b线程名。 源码主要是三个方法： public T get(); public void set(T value); public void remove(); getpublic T get() { // 获取当前线程 Thread t = Thread.currentThread(); // 获取当前线程的ThreadLocalMap ThreadLocalMap map = getMap(t); if (map != null) { // 通过ThreadLocal key获取Entry节点，并返回值 ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(&quot;unchecked&quot;) T result = (T)e.value; return result; } } // 初始化值 return setInitialValue(); } set方法public void set(T value) { // 获取当前线程 Thread t = Thread.currentThread(); // 获取当前线程的ThreadLocalMap ThreadLocalMap map = getMap(t); if (map != null) // 插入Entry key为ThreadLocal，value为value map.set(this, value); else // 新建一个ThreadLocalMap 并把第一个Entry插入 createMap(t, value); } removepublic void remove() { // 获取当前线程ThreadLocalMap ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this); } 详解 结构如上图所示 ThreadLocalMap是ThreadLocal的一个静态内部类，内部又有一个Entry的静态内部类，和有一个Entry数组用于存储&lt;key, value&gt;,key就是ThreadLocal，value就是要存储的值。 static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; { Entry(ThreadLocal&lt;?&gt; k, Object v) { super(k); value = v; } } // 长度必须是2的幂次方 /** * The table, resized as necessary. * table.length MUST always be a power of two. */ private Entry[] table; Entry继承自WeakReference并调用WeakReference构造函数，所以Entry的key是一个弱引用，即ThreadLocal是弱引用。所以当外部没有引用到ThreadLocal时，那么系统GC时，经过可达性分析，GC Roots与ThreadLocal之间引用不可达，ThreadLocal就将被回收。这样就出现了Entry中null key的情况，则无法访问到这些null key的值。如果这时线程结束，或者段开值的强引用链(Thread ref -&gt; Current Thread -&gt; ThreadLocalMap -&gt; Entry -&gt; value)，将Entry.value = null，则Entry能顺利被回收。否则就会出现内存泄漏的情况。 但是，在我们现实开发中通常会用线程池来维护线程，即线程工作完后会放回到线程池中，所以就有可能出现内存泄漏的情况。在源码中，作者也对这个问题进行了进理。见下 set操作private void set(ThreadLocal&lt;?&gt; key, Object value) { // We don't use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); // 采用开放地址法，hash冲突的时候使用线性探测 for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal&lt;?&gt; k = e.get(); // key相等直接替换值 if (k == key) { e.value = value; return; } // key为空 则调用replaceStaleEntry进行替换成新的值 if (k == null) { replaceStaleEntry(key, value, i); return; } } tab[i] = new Entry(key, value); int sz = ++size; // 再次循环扫描清除null key的值，或者大小大于阈值扩容 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash(); } ThreadLocalMap并不是像HashMap那样当hash冲突时，用分离链表法来解决。它是用开放定址法，继续获取下一位置判断，直到该位置没有Entry节点。 在检测插入的时候，如果key相同，即相同ThreadLocal，则替换原来的值。如果key为空即脏Entry(Stale Entry)则调用 replaceStaleEntry 去处理。tab[i] == null 则此位置为空，插入新的Entry，插入后会调用cleanSomeSlots去清除Stale Entry并判断扩容。 cleanSomeSlotsprivate boolean cleanSomeSlots(int i, int n) { boolean removed = false; Entry[] tab = table; int len = tab.length; do { i = nextIndex(i, len); Entry e = tab[i]; if (e != null &amp;&amp; e.get() == null) { n = len; removed = true; // 将i的值设为null 并将tab[i] = null 然后就继续向后检查，直至tab[i] == null 并返回i i = expungeStaleEntry(i); } } while ( (n &gt;&gt;&gt;= 1) != 0); return removed; } 参数：i 即刚刚插入新节点的位置，所以tab[i]不可能为null，直接从下一索引开始判断。 参数：n 是用来控制循环次数的，入参的时候它是map的实际大小（存放多少个ThreadLocal），如果没有遇到脏entry就整个扫描过程持续log2(n)次，log2(n)的得来是因为n &gt;&gt;&gt;= 1，每次n右移一位相当于n除以2。 如果遇到脏entry，就返回tab[i] == null 的i，让下一次查找的起点为i，并且 n=len 即整个hash表的长度，扩大范围在进行扫描log2(n)趟。n的实际作用是扩大搜索范围。 注：在replaceStaleEntry方法中n的参数直接就是hash表的长度。 expungeStaleEntry private int expungeStaleEntry(int staleSlot) { Entry[] tab = table; int len = tab.length; // 将当前位置Entry清除 // expunge entry at staleSlot tab[staleSlot].value = null; tab[staleSlot] = null; size--; // 继续向后查找，如果key = null 也清除掉，直至tab[i] == null // Rehash until we encounter null Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal&lt;?&gt; k = e.get(); if (k == null) { e.value = null; tab[i] = null; size--; } else { int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) { tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; } } } return i; } cleanSomeSlot总结 当前n等于hash表的size即n=10，i=1，在第一趟搜索过程中通过nextIndex，i指向了索引为2的位置，此时table[2]为null，说明第一趟未发现脏entry,则第一趟结束进行第二趟的搜索。 第二趟首先通过nextIndex方法，索引由2的位置变成了i=3，当前table[3] != null，但是该key为null，说明找到了一个脏Entry，先将n置为哈希表的长度len,然后继续调用expungeStaleEntry方法，该方法会将当前索引为3的脏entry给清除掉（令value为null，并且table[3]也为null），然后它会继续往后环形搜索，往后会发现索引为4，5的位置的entry同样为脏entry，索引为6的位置的entry不是脏entry保持不变，直至i = 7的时候此处table[7]位null，该方法就以i = 7返回 由于在第二趟搜索中发现脏entry，n增大为数组的长度len，因此扩大搜索范围（增大循环次数）继续向后环形搜索；直到在整个搜索范围里都未发现脏entry，cleanSomeSlot方法执行结束退出 replaceStaleEntry private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) { Entry[] tab = table; int len = tab.length; Entry e; // Back up to check for prior stale entry in current run. // We clean out whole runs at a time to avoid continual // incremental rehashing due to garbage collector freeing // up refs in bunches (i.e., whenever the collector runs). // staleSlot为null key索引 // 向前搜索 起点为staleSlot，如果发现有脏entry，则更新slotToExpunge为null key的索引 int slotToExpunge = staleSlot; for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i; // Find either the key or trailing null slot of run, whichever // occurs first // 向后搜索 起点为当前staleSlot for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal&lt;?&gt; k = e.get(); // If we find key, then we need to swap it // with the stale entry to maintain hash table order. // The newly stale slot, or any other stale slot // encountered above it, can then be sent to expungeStaleEntry // to remove or rehash all of the other entries in run. // 如果key相同，则替换值并把位置交换到staleSlot的位置 if (k == key) { e.value = value; tab[i] = tab[staleSlot]; tab[staleSlot] = e; // 相等说明向前没找到null key的entry，从起点i开始搜索，清除，这时i的被交换过去的脏entry // Start expunge at preceding stale entry if it exists if (slotToExpunge == staleSlot) slotToExpunge = i; cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; } // If we didn't find stale entry on backward scan, the // first stale entry seen while scanning for key is the // first still present in the run. if (k == null &amp;&amp; slotToExpunge == staleSlot) slotToExpunge = i; } // If key not found, put new entry in stale slot tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); // If there are any other stale entries in run, expunge them if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); } 针对前后有无null key的entry分为四种情况 向前向后都有 slotToExpunge初始状态和staleSlot相同，当前向环形搜索遇到脏entry时，在第1行代码中slotToExpunge会更新为当前脏entry的索引i，直到遇到哈希桶（table[i]）为null的时候，前向搜索过程结束。在接下来的for循环中进行后向环形查找，若查找到key相等的entry，先覆盖当前位置的entry，然后再与staleSlot位置上的脏entry进行交换。交换之后脏entry就更换到了i处，最后使用cleanSomeSlots方法从i（即slotToExpunge 向前搜索到脏entry的索引）为起点开始进行清理脏entry的过程 前有后没有则把staleSlot上的entry清除，并把新的entry存入到tab[staleSlot]。然后调用cleanSomeSlots从slotToExpunge作为起点开始清除，即向前搜索null key的索引。 前没有后有先覆盖当前位置的entry，然后再与staleSlot位置上的脏entry进行交换。交换之后脏entry就更换到了i处，最后使用cleanSomeSlots方法从i为起点开始进行清理脏entry的过程 前后都没有则把staleSlot上的entry清除，并把新的entry存入到tab[staleSlot]。然后也就不需要清除了 get操作private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value; } // 子类可以重写该方法，返回一个默认值 protected T initialValue() { return null; } 所以get方法就是 获取当前的线程 -&gt; 获取当前线程ThreadLocalMap -&gt; 获取储存这个ThreadLocal的Entry节点 -&gt; 返回value值 如果value值为空或者ThreadLocalMap为空，则调用initialValue获取初始值，initialValue可以重写来返回一个默认的值，再把ThreadLocal和null值存到ThreadLocalMap中。 remove操作/** * Remove the entry for key. */ private void remove(ThreadLocal&lt;?&gt; key) { Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { if (e.get() == key) { e.clear(); expungeStaleEntry(i); return; } } } 删除则是搜索相同key的entry并调用expungeStaleEntry清除。 所以源码中通过expungeStaleEntry，cleanSomeSlots,replaceStaleEntry这几个方法来清理null key的enrty 弱引用导致内存泄漏问题待续。。。","link":"/2019/06/15/2019-06-15-use-of-ThreadLocal/"},{"title":"如何理解线程安全","text":"线程安全问题都是由全局变量及静态变量引起的。 JVM运行时数据区包括了程序计数器，本地方法栈，jvm栈，堆。在这四个区中，前三个都是线程间隔离的。只有堆内存是线程间共享的。而全局变量放在堆内存中，各线程内jvm栈只保存了对象引用，所以各线程更改的还是一个内存地址的数据。 在JDK1.8中元数据区取代了永久代，元数据区并不在虚拟机中，而在本地内存中。静态变量是保存在元数据区中的。所以对于线程来说，操作的还是同一内存地址上的数据。 若每个线程中对全局变量、静态变量只有读操作，而无写操作，一般来说，这个全局变量是线程安全的；若有多个线程同时执行写操作，一般都需要考虑线程同步，否则的话就可能影响线程安全。 可见性问题jvm有一个主内存，每个线程有自己的高速缓存，当运行时每个线程都会在自己的高速缓存中建立一个变量副本，操作完后再把值写入到主内存。在多线程的情况下，有可能当线程A操作完，但值还未写入，这时线程B获取时间片在执行变量值还是未改变的。 线程A改变了值，线程B没有立即看到线程A修改的值这就是可见性问题。 原子性问题即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。比如：size += 1 相当于 size = size + 1，这边就有两个操作了，先size+1，再把值赋给size。如果这两个操作不具备原子性，就会造成线程不安全。在多线程情况下，有可能线程A刚做完size+1的操作，这时被线程B抢走了时间片，而B读到的就是错误的数据。 有序性问题即程序执行的顺序按照代码的先后顺序执行 ，但是有时候并不是如此。 指令重排序：源代码中的顺序与得到的字节码顺序不一样或者字节码顺序与实际的执行顺序不一样在编译的时候静态编译器会.java编译成.class，动态编译器jit会将.class编译成jvm执行的文件，在编译的时候还会对性能进行优化，这时候就有可能改变了代码的顺序，即源代码和执行代码的顺序是不一致的。 可以在下面的代码中看到一个简单的示例： 1234567891011121314public class Reordering { int x = 0, y = 0; public void writer() { x = 1; y = 2; } public void reader() { int r1 = y; // y的读取 int r2 = x; }} 假设此代码在两个线程中同时执行，并且y的读取看到值2。由于此写入是在写入x之后发生的，因此程序员可能会假设x的读取必须看到值1。但是，写入可能已重新排序。如果发生这种情况，则可能发生对y的写入，随后是两个变量的读取，然后可能发生对x的写入。结果将是r1的值为2，而r2的值为0。 如何保证线程安全 上面说了线程安全是因为全局变量及静态变量引起的。所以把全局变量和静态变量改成局部变量就不会出现线程安全的问题。因为局部变量的值保存在jvm栈内存里。 用同步锁（synchronized，Lock）可以解决原子性问题，因为线程间是互斥的，所以能保证原子性。 用同步锁还可以保证可见性问题，在synchronized关键字中，获得锁后会先清除工作内存的变量副本，然后从主内存拷贝变量的最新副本到线程工作内存，执行代码，将更改后的最新的共享变量的值刷新到主内存，释放互斥锁。而Lock，在Java并发编程实战中,有句话是”线程A拿到lock对象对数据进行修改, 线程B拿到lock对象后,对于线程A的修改线程B是可见的, 线程B可以看到线程A的所有操作结果”，也可以保证可见性。volatile变量也可以保证该变量对所有线程可见，但不能保证原子性。 对于有序性volatile可以保证，在volatile变量前的代码不会再其后面执行，再其后面的代码不会再其前面执行，但不能其中的代码重排序。 Atomic***即可以保证原子性，又可以保证可见性，底层是通过CAS和volatile实现的 总结线程安全是因为全局变量及静态变量引起的，如果有全局变量或静态变量，并且存在多线程对变量的写操作，则要考虑解决可见性，有序性，有序性问题。 附： lock：作用于主内存，把变量标识为线程独占状态。 unlock：作用于主内存，解除独占状态。 read：作用主内存，把一个变量的值从主内存传输到线程的工作内存。 load：作用于工作内存，把read操作传过来的变量值放入工作内存的变量副本中。 use：作用工作内存，把工作内存当中的一个变量值传给执行引擎。 assign：作用工作内存，把一个从执行引擎接收到的值赋值给工作内存的变量。 store：作用于工作内存的变量，把工作内存的一个变量的值传送到主内存中。 write：作用于主内存的变量，把store操作传来的变量的值放入主内存的变量中 参考http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html","link":"/2019/06/24/2019-06-24-how-to-understand-thread-safety/"},{"title":"死锁产生的条件及解决","text":"死锁产生的条件 互斥线程对所分配的资源进行排他性控制，即在一段时间内某资源仅为一个线程所占有。此时若有其他线程请求该资源，则请求线程只能阻塞等待。 不可剥夺线程所获得的资源在未使用完毕之前，不能被其他线程强行夺走，即只能由获得该资源的线程自己来释放。 请求与保持线程在至少拥有一个资源的情况下，线程继续请求其它资源，而该资源被其他线程所持有，此时线程被堵塞，但保持对现有资源的持有。 循环等待存在线程资源的循环等待链，链中每一个线程已获得的资源同时被下一个线程所请求。即A等待B，B等待C，C等待A。 只有同时满足以上四个条件才有可能发生死锁，只要有一个不满足就不会发生。 预防死锁破坏互斥不好意思，破坏不了。。 互斥条件不能被破坏，否则会造成结果的不可再现性 即 原子性问题 破坏不可剥夺 如果占有某些资源的一个线程进行进一步资源请求被拒绝，则该线程必须释放它最初占有的资源。 如果一个线程请求当前被另一个线程占有的一个资源，则可以抢占另一个线程，要求它释放资源。 剥夺资源法 破坏请求与保持 1，创建线程时，要求它申请所需的全部资源，系统要么满足其所有要求，要么什么也不给它。即 一次性分配策略 2，要求每个线程提出新的资源申请前，释放它所占有的资源。 破坏循环等待资源有序分配法，即所申请的资源必须按照顺序申请。 避免死锁 预防死锁和避免死锁的区别：预防死锁是设法至少破坏产生死锁的四个必要条件之一,严格的防止死锁的出现,而避免死锁则不那么严格的限制产生死锁的必要条件的存在,因为即使死锁的必要条件存在,也不一定发生死锁。避免死锁是在系统运行过程中注意避免死锁的最终发生。 资源有序分配法 对它所必须的资源，必须一次申请完。 所申请的资源必须按照顺序申请。 即线程A申请A资源，B资源，C资源，B线程也得A资源，B资源，C资源。避免可能有生成环路的条件。 银行家算法银行家算法 加锁顺序当多个线程需要相同的一些锁，但是按照不同的顺序加锁，死锁就很容易发生。如果能确保所有的线程都是按照相同的顺序获得锁（类似资源有序分配法），那么死锁就不会发生。 加锁时限线程尝试获取锁的时候加上一定的时限，超过时限则放弃对该锁的请求，并释放自己占有的锁。然后等待一段随机的时间再重试 死锁检测死锁定理： 如果资源分配图中没有环路，则系统中没有死锁，如果图中存在环路则系统中可能存在死锁。如果每个资源类中只包含一个资源实例，则环路是死锁存在的充分必要条件。 找一个非孤立、只有分配边的线程结点，去掉分配边，将其变为孤立结点；再把相应的资源分配给一个等待该资源的线程，即将某线程的申请边变为分配边；重复以上步骤，若所有线程都可成为孤立结点，称该图是可完全简化的，否则称该图是不可完全简化的。死锁状态的充分条件是：资源分配图是不可完全简化的。 死锁： 无死锁： 解除死锁 资源剥夺法。挂起某些死锁线程，并抢占它的资源，将这些资源分配给其他的死锁线程。但应防止被挂起的线程长时间得不到资源，而处于资源匮乏的状态。 撤销线程法。强制撤销部分、甚至全部死锁线程并剥夺这些线程的资源。撤销的原则可以按线程优先级和撤销线程代价的高低进行。 线程回退法。让一（或多）个线程回退足以回避死锁的地步，线程回退时自愿释放资源而不是被剥夺。要求系统保持线程的历史信息，设置还原点。","link":"/2019/06/26/2019-06-26-deadlock-conditions/"},{"title":"线程池拒绝策略","text":"线程池四种拒绝策略在线程池配置文件中配置RejectedExecutionHandler属性，可以为线程池指定拒绝策略。 在ThreadPoolExecutor类中，有四个内部类为JDK自带的四种拒绝策略。CallerRunsPolicy，AbortPolicy，DiscardPolicy，DiscardOldestPolicy。也可以自定义拒绝策略，实现RejectedExecutionHandler的rejectedExecution方法。 CallerRunsPolicypublic static class CallerRunsPolicy implements RejectedExecutionHandler { /** * Creates a {@code CallerRunsPolicy}. */ public CallerRunsPolicy() { } /** * Executes task r in the caller's thread, unless the executor * has been shut down, in which case the task is discarded. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { if (!e.isShutdown()) { r.run(); } } } 从源码上可以看出直接在 execute 方法中同步调用被拒绝的任务。 AbortPolicypublic static class AbortPolicy implements RejectedExecutionHandler { /** * Creates an {@code AbortPolicy}. */ public AbortPolicy() { } /** * Always throws RejectedExecutionException. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task * @throws RejectedExecutionException always */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { throw new RejectedExecutionException(&quot;Task &quot; + r.toString() + &quot; rejected from &quot; + e.toString()); } } 直接抛出异常RejectedExecutionException，丢弃任务。（jdk默认策略，队列满并线程满时直接拒绝添加新任务，并抛出异常） DiscardPolicypublic static class DiscardPolicy implements RejectedExecutionHandler { /** * Creates a {@code DiscardPolicy}. */ public DiscardPolicy() { } /** * Does nothing, which has the effect of discarding task r. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { } } 可以看出这种策略什么都没做，直接丢弃任务。 DiscardOldestPolicypublic static class DiscardOldestPolicy implements RejectedExecutionHandler { /** * Creates a {@code DiscardOldestPolicy} for the given executor. */ public DiscardOldestPolicy() { } /** * Obtains and ignores the next task that the executor * would otherwise execute, if one is immediately available, * and then retries execution of task r, unless the executor * is shut down, in which case task r is instead discarded. * * @param r the runnable task requested to be executed * @param e the executor attempting to execute this task */ public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { if (!e.isShutdown()) { e.getQueue().poll(); e.execute(r); } } } 获取当前线程池的队列并调用poll()方法删除队列头的任务（即下一个执行的任务），然后添加该任务。","link":"/2019/07/05/2019-07-05-thread-policy/"},{"title":"设计模式之单例模式","text":"什么是单例模式确保一个类只有一个实例，并提供一个全局访问点。 如何设计饿汉模式在类加载的时候就先创建类实例，任务线程访问的时候直接返回实例。 public class SingletonPatternTest1 { private static SingletonPatternTest1 instance = new SingletonPatternTest1(); private SingletonPatternTest1() { } public static SingletonPatternTest1 getInstance() { return instance; } } 这么做的好处是它是 线程安全的 ，并且确保了只有一个实例存在。缺点是如果没有用到这个实例，这个实例也会被创建，浪费资源。 懒汉模式延迟实例化。先不创建实例，在访问获取实例时，在判断是否已经创建。 public class SingletonPatternTest1 { private static SingletonPatternTest1 instance; private SingletonPatternTest1() { } public static SingletonPatternTest1 getInstance() { if (instance == null) { instance = new SingletonPatternTest1(); } return instance; } } 它的优点是在第一次访问的时候才会被创建，避免了创建了没有使用而浪费资源的问题。但是每次访问都需要判断是否创建，也会影响性能。而且在 多线程的情况下，它 并不是线程安全的。有可能会产生不同的对象。 懒汉同步式通过增加synchronized关键字，使得getInstance方法成同步方法，同时只能一个线程能访问。 public class SingletonPatternTest1 { private static SingletonPatternTest1 instance; private SingletonPatternTest1() { } public static synchronized SingletonPatternTest1 getInstance() { if (instance == null) { instance = new SingletonPatternTest1(); } return instance; } } 确保了每次只有一个线程进入方法，解决了线程安全的问题。但因为是同步方法，所以会很影响性能。而且我们只需要确保第一次访问的时候不被重复创建实例，在第一次创建之后，同步方法就成了累赘了。 双重检查加锁先判断判断实例是否已经创建，否则在进行加锁。保证了只有第一次才会进行同步。 public class SingletonPatternTest1 { private static volatile SingletonPatternTest1 instance; private SingletonPatternTest1() { } public static SingletonPatternTest1 getInstance() { if (instance == null) { synchronized (SingletonPatternTest1.class) { if (instance == null) { instance = new SingletonPatternTest1(); } } } return instance; } } 这里有个疑问就是：在设计模式书上说，没有volatile会使’双重检查加锁’失效。（原话是JDK1.4或更早的volatile实现会失效）我们知道volatile是保证了可见性与禁止重排序，synchronized关键字不是已经保证了可见性和禁止重排序了吗？ 答案：synchronized并不能禁止指令重排序，编译器指令可能会重排序。假设new Instance()分为开辟内存地址，初始化对象，对引用变量进行赋值。指令重排序后可能顺序变成了1,3,2。即另一线程拿到的可能是一个还没有完全初始化完成的实例。 注册表式Spring IOC就是用这种方式来管理单例bean的，虽然源码要复杂的多，但是设计思想还是差不多的！ public class SingletonPattenTest { private SingletonPattenTest(){} private volatile static Map&lt;Class&lt;?&gt;, Object&gt; map = new HashMap&lt;&gt;(); public static Object getInstance(Class&lt;?&gt; clazz) throws IllegalAccessException, InstantiationException { Object obj = map.get(clazz); if (obj == null) { synchronized (SingletonPattenTest.class) { if (obj == null) { obj = clazz.newInstance(); map.put(clazz, obj); } } } return obj; } } 枚举式public enum SingletonPattenTest { INSTANCE; } 在通过 SingletonPattenTest.INSTANCE 就可以获得对象了 好处： 避免反射攻击的问题。 在普通的单例模式中，私有化构造函数并不能阻止创建唯一的实例，可以通过反射 .setAccessible 来创建实例。而枚举类并不允许通过反射来创建实例。 避免序列化问题问题。 任何一个readObject方法，不管是显式的还是默认的，它都会返回一个新建的实例，这个新建的实例不同于该类初始化时创建的实例。要解决的话可以重写方法readResolve， 它会在readObject之后被调用，并替换readObject返回的实例。但需要注意的是，如果单例实例中存在非transient对象引用，就会有被攻击的危险。可以在readResolve之前，对非transient对象引用域进行操作。而枚举序列化的时候仅仅是将枚举对象的name属性输出到结果中，反序列化的时候则是通过java.lang.Enum的valueOf方法来根据名字查找枚举对象。同时，编译器是不允许任何对这种序列化机制的定制的，因此禁用了writeObject、readObject、readObjectNoData、writeReplace和readResolve等方法 避免线程安全问题。枚举类所有属性都会被声明称static类型，它是在类加载的时候初始化的，而类的加载和初始化过程都是线程安全的。所以，创建一个enum类型是线程安全的。 静态内部类public class SingletonPattenTest { private SingletonPattenTest(){} private static class SingletonHolder { private static SingletonPattenTest instance = new SingletonPattenTest(); } public static SingletonPattenTest getInstance(){ return SingletonHolder.instance; } } 优点：外部类加载时并不需要立即加载内部类，内部类不被加载则不去初始化instance，故而不占内存。即当SingletonPattenTest第一次被加载时，并不需要去加载SingleTonHoler，只有当getInstance()方法第一次被调用时，才会加载SingleTonHoler类并初始化instance，这种方法不仅能确保线程安全，也能保证单例的唯一性，同时也延迟了单例的实例化。 缺点： 需要两个类去做到这一点，虽然不会创建静态内部类的对象，但是其 Class 对象还是会被创建，而且是属于永久带的对象。 创建的单例，一旦在后期被销毁，不能重新创建。","link":"/2019/07/17/2019-07-17-singleton-pattern/"},{"title":"微信js-sdk定位服务签名生成","text":"公众号要用到定位服务，需要后台提供签名接口，记录一下。 签名是由jsapi_ticket+随机字符串+时间戳+页面请求url拼接后通过SHA-1加密生成的。参数必须全部小写，url是调用定位服务的页面的url。 获取tokenjsapi_ticket是由参数access_token获取的，而access_token又是由appid和secret获取的。所以先要去获取access_token。 /** * * 获取accessToken * * @return access_token */ public String getAccessToken() { Object appId = DictionaryCache.systemConfig.get(&quot;h5.app.id&quot;);//应用ID Object appSecret = DictionaryCache.systemConfig.get(&quot;h5.app.secret&quot;);//(应用密钥) String url =&quot;https://api.weixin.qq.com/cgi-bin/token?grant_type=client_credential&amp;appid=&quot;+appId+&quot;&amp;secret=&quot;+appSecret+&quot;&quot;; JSONObject backData = doGet(url); if (backData == null || backData.getInteger(&quot;errcode&quot;) != null || (accessToken = backData.getString(&quot;access_token&quot;)) == null) { logger.info(String.format(&quot;获取access_token失败 response -&gt; %s &quot;, backData)); return null; } return accessToken; } 因为access_token是有有效期的，现在默认是7200s，所以需要全局缓存，以便后面的请求使用。而且每天这个接口有调用次数限制。好像是2000次？这边由于项目用的Spring框架，bean是单例的所以直接保存在本类中了 // 微信公众号access_token 默认保存7200秒 private volatile String accessToken = &quot;&quot;; // 微信公众号TICKET 默认保存7200秒 private volatile String ticket = &quot;&quot;; 获取jsapi_ticket获取jsapi_ticket，jsapi_ticket和token一样也是7200秒，就是两小时 /** * * 获取jsApiTicket * * @return ticket */ public String getJSApiTicket() { String accessToken; if (StringUtil.isEmpty(this.accessToken)) { synchronized (this) { if (StringUtil.isEmpty(this.accessToken)) { accessToken = getAccessToken(); } else { accessToken = this.accessToken; } } } else { accessToken = this.accessToken; } if (StringUtil.isEmpty(accessToken)) { return null; } String url = &quot;https://api.weixin.qq.com/cgi-bin/ticket/getticket?access_token=&quot;+accessToken+&quot;&amp;type=jsapi&quot;; JSONObject backData = doGet(url); if (backData == null || backData.getInteger(&quot;errcode&quot;) != 0 || (ticket = backData.getString(&quot;ticket&quot;)) == null) { logger.info(String.format(&quot;获取jsApiTicket失败 response -&gt; %s &quot;, backData)); return null; } return ticket; } 生成签名// 获取签名需要传入url，动态的获取url以便其他地方也能用。 public Map&lt;String, String&gt; getParams(String url) { try { String ticket; if (StringUtil.isEmpty(this.ticket)) { synchronized (this) { if (StringUtil.isEmpty(this.ticket)) { ticket = getJSApiTicket(); } else { ticket = this.ticket; } } } else { ticket = this.ticket; } if (StringUtil.isEmpty(ticket)) { Map&lt;String, String&gt; result = Maps.newHashMap(); result.put(&quot;flag&quot;, &quot;-1&quot;); return result; } return sign(ticket, url); } catch (Exception e) { e.printStackTrace(); Map&lt;String, String&gt; result = Maps.newHashMap(); result.put(&quot;flag&quot;, &quot;-1&quot;); return result; } } // 签名的方法 public Map&lt;String, String&gt; sign(String ticket, String url) { Map&lt;String, String&gt; ret = Maps.newHashMap(); String nonceStr = createNonceStr(); String timestamp = createTimestamp(); try { String string1 = &quot;jsapi_ticket=&quot; + ticket + &quot;&amp;noncestr=&quot; + nonceStr + &quot;&amp;timestamp=&quot; + timestamp + &quot;&amp;url=&quot; + url; MessageDigest crypt = MessageDigest.getInstance(&quot;SHA-1&quot;); crypt.reset(); crypt.update(string1.getBytes(&quot;UTF-8&quot;)); ret.put(&quot;signature&quot;, byteToHex(crypt.digest())); } catch (NoSuchAlgorithmException | UnsupportedEncodingException e) { e.printStackTrace(); ret.put(&quot;flag&quot;, &quot;-1&quot;); return ret; } ret.put(&quot;flag&quot;, &quot;200&quot;); ret.put(&quot;url&quot;, url); ret.put(&quot;ticket&quot;, ticket); ret.put(&quot;nonceStr&quot;, nonceStr); ret.put(&quot;timestamp&quot;, timestamp); ret.put(&quot;appId&quot;, DictionaryCache.systemConfig.get(&quot;h5.app.id&quot;).toString()); return ret; } private String byteToHex(final byte[] hash) { Formatter formatter = new Formatter(); for (byte b : hash) { formatter.format(&quot;%02x&quot;, b); } String result = formatter.toString(); formatter.close(); return result; } private String createNonceStr() { return UUID.randomUUID().toString(); } private String createTimestamp() { return Long.toString(System.currentTimeMillis() / 1000); } public JSONObject doGet(String url) { logger.info(String.format(&quot;HttpGet -&gt; %s&quot;, url)); JSONObject jsonObject = null; HttpClient client = HttpClients.createDefault(); HttpGet httpget = new HttpGet(url); HttpResponse response; try { response = client.execute(httpget); HttpEntity resEntity = response.getEntity(); String jsonString = EntityUtils.toString(resEntity, &quot;UTF-8&quot;); jsonObject = JSON.parseObject(jsonString); } catch (ClientProtocolException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } return jsonObject; } 定时刷新4，因为有效期是两小时，所以还要设置个定时器去刷新token和ticket。设置为每小时刷新一次。 @Scheduled(cron = &quot;0 0 0/1 * * ?&quot;) public void updateAccessToken() { service.getAccessToken(); service.getJSApiTicket(); } 遇到的问题 在获取token时，接口返回无效的ip。 需要在微信公众号上添加白名单。 调用微信wx.config 提示无效的签名。 这是由于前端传到后台的url有参数，且不止一个。（微信规定url有参数的话也要带上），然后后台收到的url参数是不全的。差不多是http://ip:port/wx/config?index=0&amp;id=1 而在后端只能收到http://ip:port/wx/config?index=0 这一段，所以提示签名无效。 根据微信的提示 2.invalid signature签名错误。建议按如下顺序检查： 1.确认签名算法正确，可用http://mp.weixin.qq.com/debug/cgi-bin/sandbox?t=jsapisign 页面工具进行校验。 2.确认config中nonceStr（js中驼峰标准大写S）, timestamp与用以签名中的对应noncestr, timestamp一致。 3.确认url是页面完整的url(请在当前页面alert(location.href.split(‘#’)[0])确认)，包括’http(s)://‘部分，以及’？’后面的GET参数部分,但不包括’#’hash后面的部分。 4.确认 config 中的 appid 与用来获取 jsapi_ticket 的 appid 一致。 5.确保一定缓存access_token和jsapi_ticket。 6.确保你获取用来签名的url是动态获取的，动态页面可参见实例代码中php的实现方式。如果是html的静态页面在前端通过ajax将url传到后台签名，前端需要用js获取当前页面除去’#’hash部分的链接（可用location.href.split(‘#’)[0]获取,而且需要encodeURIComponent），因为页面一旦分享，微信客户端会在你的链接末尾加入其它参数，如果不是动态获取当前链接，将导致分享后的页面签名失败。 前端没有对url进行转码，加上encodeURIComponent后就可以了。","link":"/2019/07/18/2019-07-18-wx-js-sdk-signature/"},{"title":"高速缓存，写缓冲器和无效队列","text":"高速缓存由于处理器的运行速度远远大于内存的读写速度，为了提高性能，所以硬件设计者引入了高速缓存的概念。高速缓存是一种速度比内存快，但大小远远比内存小的存储部件。有了它之后，处理器不再与内存发生直接的操作，而是读写高速缓存里的数据，高速缓存在与内存发生读写操作。 缓存的结构缓存的结构类似HashMap，是链表散列结构的。有若干个通，每个桶里面都是一个链表。每个链表节点就是一个缓存条目（Cache Line），缓存条目由三部分组成：Tag，Data Block，Flag Tag：包含缓存行中数据的位置信息 Data Block（缓存行）：存储从主内存中读取的数据以及准备写入主内存的数据，一个缓存行可存储多个变量 Flag：缓存行的状态信息 CPU访问内存时，会通过内存地址解码的三个数据：index（桶编号）、tag（缓存条目的相对编号）、offset（变量在缓存条目中的位置偏移）来获取高速缓存中对应的数据。 缓存一致性协议（MESI）由于每个处理器都有自己的缓存，当多线程并发访问同一共享变量时，就会出现这些线程所在的处理器都会有一份该变量的副本。当一个线程对副本进行修改后，副本间的数据如何同步呢？这时就要靠缓存一致性协议 MESI定义 Modified 该缓存条目的数据是被修改过的，且与主存的数据不一致（还未写回主存）。在任一时刻，多个处理器的高速缓存，相同tag的缓存条目只有一个能处于该状态。 如果其它处理器要读取同一tag的缓存条目数据之前，要先等数据写回主存。在数据写回主存之后，该缓存条目的状态变为独享的。 Exclusive 已独占的方法保存着副本数据，其它高速缓存的变量副本都是无效的。该缓存条目的数据与主存数据一致。 当有其它处理器读取该数据时，该缓存条目的状态变为共享的。如果有处理器修改了该缓存条目的数据，状态则变为修改的。 Shared 该缓存条目被多个高速缓存同时缓存。且它们的数据都与主存中的一致。 当有缓存对该缓存行的数据进行修改时，改缓存行的状态变为无效的。 Invalid 该缓存行是无效的，不包含任何内存地址对应的有效副本数据。该状态是缓存条目的初始状态。 MESI消息 请求消息 描述 返回消息 描述 Read 通知其他处理器，主内存正准备读取某个数据。该消息包含待读取数据的内存地址 Read Response 返回请求读取的数据，该消息可能是主内存返回的，也可能是其他高速缓存返回。 Invalidate 通知其他处理器将高速缓存中指定内存地址对应的缓存条目状态置为I（无效），即通知这些处理器删除指定内存地址的副本数据 Invalidate Acknowledge 接收Invalidate消息必须回复该消息，表示已经删除其高速缓存上面的数据 Read Invalidate 由Read和Invalidate组合而成的复合消息。作用在于通知其他处理器，当前处理器准备更新（Read-Modify-Write）一个数据，请求其他处理器删除自己高速缓存中的副本数据。 Read Response与Invalidate Acknowledge 接收到请求的处理器必须返回这两个消息 Writeback 该消息包含需要写入主内存的数据及其对应的内存地址 处理器在执行内存读写操作时，在有必要的情况下会往总线发送特定的请求消息，同时每个处理器还会嗅探（也称拦截）总线中由其他处理器发出的请求消息并在一定条件下往总线回复相应的响应消息。 MESI总结从上面可以看出，MESI协议对内存数据的访问控制类似于读写锁。多个高速缓存同时对同一变量副本进行读的操作是并发的，而写的操作是独占的。 并发读当处理器Processor 0要读取缓存中的数据A时，如果发现A所在的缓存条目状态为M、E或S，那么处理器可直接读取数据。 如果A所在的缓存条目状态状态为 I，说明Processor 0的缓存中不包含A的有效数据。这时，Processor 0会往总线发送一条Read消息来读取A的有效数据，而缓存状态不为 I 的其他处理器（如Process 1）或主内存（其他处理器缓存条目状都为 I 时从主内存读）收到消息后需要回复Read Response，来将有效的A数据返回给发送者。 需要注意的是，返回有效数据的其他处理器（如Process 1），如果状态为M，则会先将数据写入主内存，此时状态为E，然后在返回Read Response后，再将状态更新为S。这样，Processor 0读取的永远是最新的数据，即使其他处理器对这个数据做了更改，也会获取到其他处理器最新的修改信息。 互斥写处理器Processor 0要对缓存中的数据A时进行写操作时，如果发现A所在的缓存条目状态为M、E，那么处理器可直接对数据进行写的操作。完成后再把缓存条目的状态改为M 如果缓存条目状态为 S 则会先向总线发送Invalidate消息，来通知其他高速缓存该缓存条目无效，获得该缓存条目的独占权。当收到拥有该数据副本的处理器返回的Invalidate Acknowledge，才确认已获得该缓存条目的独占权，再将数据A写入，并把缓存条目的状态改成M。 如果缓存条目状态为 I 则会先发送Read Invalidate消息，先获取最新的数据，再通过 Invalidate来获取独占权。 需要注意的是，如果接收到Invalidate消息的其他其他处理器，缓存条目状态为M，则该处理器会先将数据写入主内存（以方便发送Read Invalidate指令的处理器读到最新值），然后再将状态改为I 写缓冲器和无效化队列 根据缓存一致性协议，实现了对数据的共享读和互斥写，按理说已经解决了数据的一致性问题，那么又为什么还会出现”可见性”这样的问题呢？ 原因在于写缓冲器和无效化队列的引入。MESI协议虽然解决了缓存一致性问题，但其本身有一个性能缺陷：处理器每次写数据时，都得等待其他所有处理器将其高速缓存中对应的数据删除，并接收到它们返回的Read Response与Invalidate Acknowledge消息后才执行写操作。这个过程无疑是很消耗时间的。 写缓冲器写缓冲器是处理器内部一个容量比高速缓存还小的高速存储部件，每个处理器都有自身的写缓冲器，且一个处理器无法读取另一个处理器上的写缓冲器内容。写缓冲器的引入主要是为了解决上面提到的MESI的写性能问题。 写操作 如果相应的缓存条目状态为 E、M，则直接写入 如果相应的缓存条目状态为 S， 处理器会将写操作相关信息存入写缓冲器，并发送Invalidate消息。（不再等待响应消息） 如果相应的缓存条目状态为 I，将写操作相关信息存入写缓冲器，并发送Read Invalidate消息。（不再等待响应消息） 当处理器将写操作写入写缓冲器后，则认为写操作已经完成。而实际上，当处理器收到其他所有处理器回应的Read Response、Invalidate Acknowledge消息后，处理器才会将写缓冲器中对应的写操作写入相应的缓存行，这个时候，写操作才算真正完成。 写缓冲器让处理器在执行写操作时不需要再额外的等待，减少了写操作的等待，提高了处理器的指令执行效率。 读操作 引入写缓存器后，处理器读取数据时，由于该数据的更新结果可能仍然停留在写缓冲器中，所以处理器会先从写缓冲器中找寻数据，没有找到时，才从高速缓存中找。 这种处理器直接从写缓冲器中读取数据的技术被称为：存储转发 无效化队列处理器在接收到Invalidate消息后，并不马上删除消息中指定地址对应的副本数据，而是将消息存入无效化队列之后就回复Invalidate Acknowledge消息，从而减少了执行写操作的处理器的等待时间。 需要注意的是，有些处理器（如X86）可能并没有使用无效化队列 新问题写缓冲器和无效化队列的引入带来了性能的提高，同时又带来了新的两个问题：内存重排序与可见性 内存重排序 如下表，变量data初始值为0，变量ready初始值为false。两个处理器Processor 1和Processor 2在各自的线程上执行上述代码。执行的绝对时间顺序为 S1——&gt;S2——&gt;L3——&gt;L4 Processor 1 Processor 2 data = 1; // S1 ready = true; // S2 while(!ready) continue; //L3 system.out.println(data); //L4 以StoreStore（写又写）操作为例，看写缓冲造成的重排序。如果S1步data值的写操作被写入写缓冲器、还没真正的写到高速缓存中，而S2步的ready值的写操作已经写入到了高速缓存。那在L3步读取ready值时，根据MESI协议，会读到正确的ready值：true；但在L4步读取data时，会读到data的初始值0，而不是在另外一个处理器写缓冲器中的值：修改值1。在处理器Processor 2看来，S1和S2的执行顺序就好像反了一样，即发生了重排序。 以LoadLoad（读又读）为例，看无效化队列造成的重排序。同上面的步骤，S2已被同步到高速缓存，S1写入写缓冲器，并发送了Invalidate消息。当执行L3时，读取到正确的值：true，当执行到L4时，由于无效化队列，Processor 2虽然发送了Invalidate Acknowledge消息，但并没有删除自己高速缓存中的data数据，所以会读取到其高速缓存中的data：0 可见性 因为写缓冲器中的内容是无法被其他处理器读取的，这个也就造成了一个处理器更新一个共享变量后，对其他处理器而言，看不到这个更新的值，即可见性。 看似内存重排序和可见性是一样的问题，都是因为数据被写到了写缓冲器，没有及时写到高速缓存中，从而产生了这么个“时间差”","link":"/2019/07/22/2019-07-22-cache-write-buffer-and-invalidate-queue/"},{"title":"nginx负载均衡的简单使用","text":"启动两个Tomcat1,编写docker-compose.yml version: '3' services: tomcat1: image: tomcat container_name: tomcat1 ports: - 9090:8080 tomcat2: image: tomcat container_name: tomcat2 ports: - 9091:8080 2,docker-compose up -d 启动tomcat root@ddmcc:/usr/local/docker/tomcat# docker-compose up -d Creating network &quot;tomcat_default&quot; with the default driver Creating tomcat1 ... Creating tomcat2 ... Creating tomcat1 Creating tomcat1 ... done 3,输入docker ps查看,tomcat已正常启动 4,在两个tomcat的index页面追加文字以便待会区分。输入docker exec -it tomcat1 /bin/bash已交互的方式进入容器 进入webapps/ROOT目录在追加端口到末尾 root@ddmcc:/usr/local/docker/tomcat# docker exec -it tomcat1 /bin/bash root@b91b07cde612:/usr/local/tomcat# root@b91b07cde612:/usr/local/tomcat# root@b91b07cde612:/usr/local/tomcat# cd webapps/ROOT/ root@b91b07cde612:/usr/local/tomcat/webapps/ROOT# echo &quot;9090&quot; &gt;&gt; index.jsp 输入192.168.132.129:9090 查看端口是否已追加 tomcat2重复tomcat1的操作,追加端口9091 配置nginx1,编写docker-compose.yml version: '3.1' services: nginx: restart: always image: nginx container_name: nginx ports: - 81:80 volumes: - ./conf/nginx.conf:/etc/nginx/nginx.conf - ./wwwroot:/usr/share/nginx/wwwroot 2,创建文件夹/conf,并在其下面创建配置文件nginx.conf user nginx; worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; upstream app { server 192.168.132.129:9090 weight=10; server 192.168.132.129:9091 weight=10; } server { listen 80; server_name 192.168.132.129; location / { proxy_pass http://app; index index.html index.htm; } } } 3,docker-compose up -d 启动nginx 4,输入192.168.132.129:81可以看到反向代理到了9090和9091,并且实现了负载均衡","link":"/2019/07/23/2019-07-23-use-nginx-load-alance/"},{"title":"nginx搭建伪CDN服务器","text":"1,编写docker-compose.yml version: '3.1' services: nginx: restart: always image: nginx container_name: nginx ports: - 81:80 volumes: - ./conf/nginx.conf:/etc/nginx/nginx.conf - ./wwwroot:/usr/share/nginx/wwwroot 2,编写nginx配置,nginx.conf user nginx; worker_processes 1; events { worker_connections 1024; } http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server { listen 80; server_name 192.168.132.129; location / { root /usr/share/nginx/wwwroot/cdn; index index.html index.htm; } } } 3,在nginx目录下创建/wwwroot/cdn目录,再在目录下创建/2019/07/23目录,将文件 2019-07-23-use-nginx-mimic-cdn.md上传到该目录 4,输入192.168.132.129:81/2019/07/23/2019-07-23-use-nginx-mimic-cdn.md可以下载访问该文件了","link":"/2019/07/23/2019-07-23-use-nginx-mimic-cdn/"},{"title":"设计模式之责任链模式","text":"定义让多个对象都有可能接收请求，将这些对象连接成一条链，并且沿着这条链传递请求，直到有对象处理它为止。 例子假设有一个请假的流程。员工发起请假请求，如果小于等于1天需要组长审批，小于等于三天需要经理审批，大于三天需要总监审批。如果不用责任链模式，代码可能会如下： 首先定义请假信息类，包含字段：name(请假人),days(请假天数) 1234567891011121314151617181920212223242526272829public class LeaveInfo { private double days; private String name; public LeaveInfo() {} public LeaveInfo(String name, double days) { this.days = days; this.name = name; } public double getDays() { return days; } public void setDays(double days) { this.days = days; } public String getName() { return name; } public void setName(String name) { this.name = name; }} 再定义请假类型 123456789101112131415161718public enum LeaveType { // 组长 TEAM_LEADER(1), // 经理 MANAGER(3); double days; LeaveType(double days) { this.days = days; } public double getDays() { return days; }} 编写请假处理类 123456789101112131415public class LeaveService { public void leave(LeaveInfo leaveInfo) { if (leaveInfo.getDays() &lt;= 0) { } else if (leaveInfo.getDays() &lt;= LeaveType.TEAM_LEADER.getDays()) { System.out.println(&quot;组长审批 -&gt; &quot; + leaveInfo.getDays() +&quot; 天&quot;); } else if (leaveInfo.getDays() &lt;= LeaveType.MANAGER.getDays()) { System.out.println(&quot;经理审批 -&gt; &quot; + leaveInfo.getDays() +&quot; 天&quot;); } else { System.out.println(&quot;总监审批 -&gt; &quot; + leaveInfo.getDays() +&quot; 天&quot;); } }} 还有可能是一级一级审批上去的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public void transferLeave(LeaveInfo leaveInfo) { if (leaveInfo.getDays() &lt;= 0) { } else if (leaveInfo.getDays() &lt;= LeaveType.TEAM_LEADER.getDays()) { System.out.println(&quot;组长审批 -&gt; &quot; + leaveInfo.getDays() +&quot; 天&quot;); } else if (leaveInfo.getDays() &lt;= LeaveType.MANAGER.getDays()) { System.out.println(&quot;组长审批 -&gt; &quot; + leaveInfo.getDays() +&quot; 天&quot;); System.out.println(&quot;经理审批 -&gt; &quot; + leaveInfo.getDays() +&quot; 天&quot;); } else { System.out.println(&quot;组长审批 -&gt; &quot; + leaveInfo.getDays() +&quot; 天&quot;); System.out.println(&quot;经理审批 -&gt; &quot; + leaveInfo.getDays() +&quot; 天&quot;); System.out.println(&quot;总监审批 -&gt; &quot; + leaveInfo.getDays() +&quot; 天&quot;); }}````在上面的代码中虽然实现了需求，但是代码并不美观。- 假设增加超过五天还需要总经理审批，就不得不重新来修改`if···else···`代码。- 如果审批顺序发生变化，也要对leave进行修改。如果流程足够的复杂，这样的代码对后期维护是很麻烦的，假设这些代码不是自己写的，或者过了很久回过头来看这些代码。则必须先去理解每个if的条件是什么。重要的是这样的写法违背了`开闭原则`，后期代码维护拓展就不得不打开leave方法来修改，并且可能会影响流程中结果。## 引入责任链模式在每个处理对象中都有下一个处理对象的引用，形成一个责任链。![markdown](https://ddmcc-1255635056.file.myqcloud.com/3942ea02-09de-457d-933f-e745377440a4.png)1，抽象出一个处理类```javapublic abstract class AbstractLeaveHandle { protected AbstractLeaveHandle nextLeaveHandle; public AbstractLeaveHandle(AbstractLeaveHandle leaveHandle) { this.nextLeaveHandle = leaveHandle; } protected void leave(LeaveInfo leaveInfo) { prepare(leaveInfo); if (nextLeaveHandle != null) { nextLeaveHandle.leave(leaveInfo); } } protected void transferLeave(LeaveInfo leaveInfo) { transferPrepare(leaveInfo); if (nextLeaveHandle != null) { nextLeaveHandle.transferLeave(leaveInfo); } } /** * 由具体处理着自己去实现 * * @param leaveInfo 请假信息 */ protected abstract void prepare(LeaveInfo leaveInfo); protected abstract void transferPrepare(LeaveInfo leaveInfo);} 在每个处理着中都有下一个处理者的引用，当有下一个处理者则传递下去。 2，编写三个具体处理者继承AbstractLeaveHandle 组长处理者： 1234567891011121314151617181920public class TeamLeaderHandle extends AbstractLeaveHandle { public TeamLeaderHandle(AbstractLeaveHandle leaveHandle) { super(leaveHandle); } @Override protected void prepare(LeaveInfo leaveInfo) { if (leaveInfo.getDays() &lt;= LeaveType.TEAM_LEADER.getDays()) { System.out.println(String.format(&quot;组长审批 -&gt; %s 请假 %s 天&quot;, leaveInfo.getName(), leaveInfo.getDays())); } } @Override protected void transferPrepare(LeaveInfo leaveInfo) { if (leaveInfo.getDays() &gt; 0) { System.out.println(String.format(&quot;组长审批 -&gt; %s 请假 %s 天&quot;, leaveInfo.getName(), leaveInfo.getDays())); } }} 经理处理者： 123456789101112131415161718192021public class ManagerHandle extends AbstractLeaveHandle { public ManagerHandle(AbstractLeaveHandle leaveHandle) { super(leaveHandle); } @Override protected void prepare(LeaveInfo leaveInfo) { if (leaveInfo.getDays() &lt;= LeaveType.MANAGER.getDays()) { System.out.println(String.format(&quot;经理审批 -&gt; %s 请假 %s 天&quot;, leaveInfo.getName(), leaveInfo.getDays())); } } @Override protected void transferPrepare(LeaveInfo leaveInfo) { if (leaveInfo.getDays() &gt;= LeaveType.MANAGER.getDays()) { System.out.println(String.format(&quot;经理审批 -&gt; %s 请假 %s 天&quot;, leaveInfo.getName(), leaveInfo.getDays())); } }} 总监处理者： 1234567891011121314151617181920public class DirectorHandle extends AbstractLeaveHandle { public DirectorHandle(AbstractLeaveHandle leaveHandle) { super(leaveHandle); } @Override protected void prepare(LeaveInfo leaveInfo) { if (leaveInfo.getDays() &gt; LeaveType.MANAGER.getDays()) { System.out.println(String.format(&quot;总监审批 -&gt; %s 请假 %s 天&quot;, leaveInfo.getName(), leaveInfo.getDays())); } } @Override protected void transferPrepare(LeaveInfo leaveInfo) { if (leaveInfo.getDays() &gt; LeaveType.MANAGER.getDays()) { System.out.println(String.format(&quot;总监审批 -&gt; %s 请假 %s 天&quot;, leaveInfo.getName(), leaveInfo.getDays())); } }} 3，调用的方法 1234567891011public class Main { public static void main(String[] args) { AbstractLeaveHandle directorHandle = new DirectorHandle(null); AbstractLeaveHandle managerHandle = new ManagerHandle(directorHandle); AbstractLeaveHandle leaderHandle = new TeamLeaderHandle( managerHandle); leaderHandle.leave(new LeaveInfo(&quot;大锤&quot;, 6)); leaderHandle.transferLeave(new LeaveInfo(&quot;大锤&quot;, 3)); }} 在引入责任链模式之后解决的违背开闭原则的问题，在新增或减少审批流程不需要去修改具体的审批实现，修改某一个环节的审批也不会对其他的代码产生影响。并且遵守了 单一职责原则，各个审批者只做自己的事。 但是这样你会发现每个对象引用下一个处理对象，如果多的话它们之间的关系会感觉很乱。 升级的责任链模式主要多了一个离职责任链类 123456public interface Chain { void doFilter(LeaveInfo leaveInfo); Chain addHandle(AbstractLeaveHandle addHandle);} 12345678910111213141516171819202122232425public class LeaveChain implements Chain { private int currentPosition = 0; private List&lt;AbstractLeaveHandle&gt; leaveHandles; @Override public void doFilter(LeaveInfo leaveInfo) { if (currentPosition &lt; leaveHandles.size()) { ++currentPosition; AbstractLeaveHandle nextHandle = leaveHandles.get(currentPosition - 1); nextHandle.prepare(leaveInfo); this.doFilter(leaveInfo); } } @Override public Chain addHandle(AbstractLeaveHandle addHandle) { if (leaveHandles == null) { leaveHandles = new ArrayList&lt;&gt;(); } leaveHandles.add(addHandle); return this; }} 在客户端调用代码 1234567public class Main { public static void main(String[] args) { Chain chain = new LeaveChain(); chain.addHandle(new TeamLeaderHandle()).addHandle(new ManagerHandle()).doFilter(new LeaveInfo(&quot;大锤&quot;, 3)); }} 这样每个对象拥有下个处理者的引用而造成的关系混乱也就不存在了。而且这样处理会更加的灵活，我们可以通过*.xml上配置列表的处理对象。 123456789&lt;bean id=&quot;leaveChain&quot; class=&quot;xxx.xxx.xxx.LeaveChain&quot;&gt; &lt;property name=&quot;leaveHandles&quot;&gt; &lt;list&gt; &lt;ref bean=&quot;teamLeaderHandle&quot; /&gt; &lt;ref bean=&quot;managerHandle&quot; /&gt; ··· ··· &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 在Servlet中的Filter就是这么处理的。在web.xml中配置Filter，如果需要增删改配置文件就好了。 使用场景会发现，在jdk源码使用责任链的模式中，基本都不是我们上述的业务流程，或者说都不怎么相似。像过滤器这样都是经过所有的过滤器后，最后在处理我们的业务。 比如做吃饭这件事，在吃饭之前要买菜，洗菜，煮菜… 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public interface EatPrepareFilter { void doFilter(Eat eat, EatChain chain);}public class EatChain implements EatPrepareFilter { private int currentPosition = 0; private List&lt;EatPrepareFilter&gt; prepareFilterList; @Override public void doFilter(Eat eat, EatChain chain) { if (this.currentPosition == this.prepareFilterList.size()) { eat.eat(); } else { prepareFilterList.get(currentPosition++).doFilter(eat, this); } } public EatChain addFilter(EatPrepareFilter filter) { if (prepareFilterList == null) { prepareFilterList = new ArrayList&lt;&gt;(); } prepareFilterList.add(filter); return this; }}public class BuyFilter implements EatPrepareFilter { @Override public void doFilter(Eat eat, EatChain chain) { System.out.println(&quot;买菜&quot;); chain.doFilter(eat, chain); }}public class WashFilter implements EatPrepareFilter { @Override public void doFilter(Eat eat, EatChain chain) { System.out.println(&quot;洗菜&quot;); chain.doFilter(eat, chain); }}public class BoiledFilter implements EatPrepareFilter { @Override public void doFilter(Eat eat, EatChain chain) { System.out.println(&quot;煮菜&quot;); chain.doFilter(eat, chain); }}public class Eat { public void eat() { System.out.println(&quot;吃饭中&quot;); }}public class Main { public static void main(String[] args) { EatChain chain = new EatChain(); chain.addFilter(new BuyFilter()).addFilter(new WashFilter()).addFilter(new BoiledFilter()).doFilter(new Eat(), chain); }} 优缺点优点 将请求者与处理者解耦 将具体的处理业务抽出，由各自处理类处理(单一职责) 不需要知道链的具体结构与实现 通过改变链里的处理者与它们的顺序能够动态的增加删除 缺点 出错不易排查","link":"/2019/07/31/2019-07-31-chain-of-responsibility-pattern/"},{"title":"设计模式之装饰者模式","text":"定义动态的扩展对象的功能，是继承的一种替代方案。 为什么要用装饰者模式我们如果需要扩展一个类的功能，通常我们会有两种做法。 直接在类里面增加 创建一个新的类来继承这个类，扩展的功能由新的类来实现 对于第一种方式，我是拒绝的。首先它违背了 开闭原则，一个类对扩展是开放的，对修改是关闭的（注：对修改是关闭的不是说不让修改，而是在不会在影响现有功能代码的情况下修改），所以它可能会对原有的代码产生影响，并且会让类里的代码变得更加臃肿。 对于用继承的方法，它解决了开闭原则问题。但是继承它是在编译的时候就已经决定好的，也就是这个类的行为与属性不能动态的修改。但是对于装饰者模式，它利用组合的方式来扩展。在我们编写代码时可以动态的更改我们想要组合的对象，即动态更改我们相要扩展的功能。多用组合，少用继承 也是我们程序设计的原则。 UML类图 疑问看你这类图不也用了继承了么！ 注意：这里继承不是要继承父类的行为，而是为了它们有相同的超类型。装饰者与被装饰者都依赖于组件接口。这就是 依赖倒置原则 等等..为什么装饰者要继承超类？ 这是因为如果装饰者不继承超类，装饰者与被装饰者没有公共的超类类型，尽管装饰者实例中有被装饰者的实例，但这样一来每个被装饰者只能够被装饰一次了！！ 这要怎么理解呢？？ ↓ ↓ ↓ ↓ ↓ 接着往下看 使用场景比如有个家具代理公司，卖各种家具。支持客户自由的挑选家具组合。现在需要设计一个收银系统，能够计算不同家具组合下的总价。 假设有房间家具要购买。有床，衣柜，书桌。 UML类图 创建代码1，共同的组件接口Furniture，里面有属性name用来记录家具的名字，还有cost()方法，这是计算价格的方法，由子类自己去实现。 1234567891011121314public abstract class Furniture { protected String name; protected abstract double cost(); public String getName() { return name; } public void setName(String name) { this.name = name; }} 2，创建普通的床，衣柜，书桌，注意这三个类就是被装饰者，即使它们不被装饰，也是可用的。 在cost方法中返回自己的价格。 12345678910111213141516171819202122232425262728293031323334353637public class Bed extends Furniture { public Bed() { name = &quot;床&quot;; } @Override protected double cost() { return 500; }}public class Desk extends Furniture { public Desk() { name = &quot;书桌&quot;; } @Override protected double cost() { return 200; }}public class Wardrobe extends Furniture { public Wardrobe() { name = &quot;衣柜&quot;; } @Override protected double cost() { return 300; }} 3，接口和被装饰者已经创建好了，该创建装饰者了。 原料装饰与品牌装饰： Decorator | |--- MaterialDecorator // 原料装饰 | |--- SolidWoodFurniture // 实木家具 | |--- MahoganyFurniture // 红木家具 | |--- TrademarkDecorator // 品牌装饰 |--- IKEAFurniture // 宜家 |--- QYFurniture // 全友 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102public abstract class MaterialDecorator extends Furniture { protected Furniture wrappedObj; @Override protected abstract double cost(); public MaterialDecorator(Furniture wrappedObj) { this.wrappedObj = wrappedObj; }}public class SolidWoodFurniture extends MaterialDecorator { public SolidWoodFurniture(Furniture component) { super(component); } @Override protected double cost() { // 实木家具+500 return this.wrappedObj.cost() + 500; } @Override public String getName() { return this.wrappedObj.getName() + &quot;,实木&quot;; }}public class MahoganyFurniture extends MaterialDecorator { public MahoganyFurniture(Furniture component) { super(component); } @Override protected double cost() { // 红木家具 + 2000 return this.wrappedObj.cost() + 2000; } @Override public String getName() { return this.wrappedObj.getName() + &quot;,红木&quot;; }}public abstract class TrademarkDecorator extends Furniture { protected Furniture wrappedObj; @Override protected abstract double cost(); public TrademarkDecorator(Furniture wrappedObj) { this.wrappedObj = wrappedObj; }}public class QYFurniture extends TrademarkDecorator { public QYFurniture(Furniture component) { super(component); } @Override protected double cost() { // 全友家居品牌+ 300 return this.wrappedObj.cost() + 300; } @Override public String getName() { return this.wrappedObj.getName() + &quot;,全友&quot;; }}public class IKEAFurniture extends TrademarkDecorator { public IKEAFurniture(Furniture component) { super(component); } @Override protected double cost() { // 打上宜家招牌 价格+1000 return this.wrappedObj.cost() + 1000; } @Override public String getName() { return this.wrappedObj.getName() + &quot;,宜家&quot;; }} 4，测试运行，我想要买一个宜家的实木床 1234567891011public class Main { public static void main(String[] args) { Furniture furniture = new SolidWoodFurniture(new IKEAFurniture(new Bed())); System.out.println(furniture.getName()); System.out.println(furniture.cost()); // 输出 床,宜家,实木 2000.0 }} 解决疑问上面我们说到装饰者也继承了家具超类的问题。我们来看一下，如果我要一个宜家的桌子。那么代码可能如下： Furniture IKEADesk = new IKEAFurniture(new Desk()); 上面是装饰者继承了Furniture的情况下的代码，用超类类型变量来接受桌子。如果没有继承的情况下，那么代码就是： IKEAFurniture IKEADesk = new IKEAFurniture(new Desk()); 看了看这个普通的宜家桌子不太喜欢，我想要这个桌子是红木的！！很简单，我们只需要在包装一层，包装层红木的：Furniture desk = new MahoganyFurniture(IKEADesk); 。嗯，的确很简单。但是如果没有继承，IKEAFurniture实例并不是一个可以被装饰的对象。也就无法组合成宜家的红木桌子了。 所以可以说 装饰者继承组件就是为了能够被装饰多次，也可以说是能让装饰者也能成为被修饰者。 组合替换继承装饰者模式中利用了组合的方式，在不更改原有类的基础上动态的去修改，增加类的功能。如果要用继承实现，上面的例子。那么需要更多的类。 上面其实我们只创建了4个具体的装饰者，就能实现他们之间的随意组合。而如果用继承的方式则需要4*4也就是创建16个类。如果品类多的话，无疑是个类爆炸。并且可能出来后期修改一个功能，会要在很多类中修改。 JDK中的栗子这是IO流的部分类图，inputStream组件提供了基本的字节读写功能。FilterInputStream 是抽象的装饰者，它下面的子类是有具体实现的装饰者。 如LineNumberInputStream提供了行数的统计功能，BufferedInputStream利用缓冲来提高性能和提供readLine()一次读取一行来增强接口。 编写一个自己的I/O装饰者： 继承FilterInputStream，并拓展功能，把所有输入的大写字母转为小写。 123456789101112public class LowerCaseInputStream extends FilterInputStream { protected LowerCaseInputStream(InputStream in) { super(in); } @Override public int read() throws IOException { int c = super.read(); return (c == -1 ? c : Character.toLowerCase((char)c)); }} 运行： 123456789101112public static void main(String[] args) { try { int c; InputStream in = new LowerCaseInputStream(new BufferedInputStream(new FileInputStream(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\text.txt&quot;))); while ((c = in.read()) &gt;= 0) { System.out.print((char)c); } in.close(); } catch (IOException e) { e.printStackTrace(); }} 优缺点优点： 提供比继承更加灵活的扩展方式。利用组合和委托能动态的加上新的行为。继承是编译期就已确定的，静态的。 可以在被装饰者的行为前后加上自己的行为，也可以完全取代。 能够利用不同的装饰器和调整它们的排序，设计出不同的组合。 缺点： 利用装饰者模式可以比利用继承来创建更少的类，但在使用时会有更多的小对象。特别是在复杂的时候，一层装饰一层，不易排查错误。 如果设计了很多的装饰者，产生大量的小类，会对使用者产生困扰。如Java I/O流","link":"/2019/08/03/2019-08-03-decorator-pattern/"},{"title":"使用UML表示类关系","text":"在上一篇责任链设计模式中，尝试画UML关系图，发现画错了。所以这里记录下，加深印象。 Generalization(继承)UML中用带空心箭头的实线表示Generalization关系，箭头指向父类。 Interface Realzation(实现)UML中用空心箭头的虚线表示Interface Realzation关系，箭头指向父接口 Dependency(依赖)UML中用带箭头的虚线表示Dependency关系，箭头指向被依赖元素。 依赖在代码中主要体现为 Class2的某个函数的返回值、形参、局部变量或对Class1静态方法的调用，则表示Class2引用了Class1 Association(双向关联)UML中用实线表示Association关系。 依赖与关联的区别 依赖是指其中的一个类作为另一个类的方法的参数或者返回值，或者是某个方法的变量而已。而发生关联关系的两个类，是指类A成为类B的全局属性 从关系的生命周期来看，依赖关系是仅当类的方法被调用时而产生，伴随着方法的结束而结束。关联关系当类实例化的时候产生，当类对象销毁的时候关系结束。相比依赖，关联关系的生存期更长 Directed Association(单向关联)UML中用带箭头实线表示Directed Association关系。箭头指向被关联类，即Class2拥有Class1的全局变量。 Aggregation(聚合)UML中用带空心菱形头的实线表示Aggregation关系，菱形头指向整体。聚合关系在代码上与关联关系表现一致 聚合用来表示集体与个体之间的关联关系，即集体和个体是一种has-a的关系，例如班级与学生之间存在聚合关系。 Composition(组合)UML中用带实心菱形头的实线表示Composition关系，菱形头指向整体。 组合用来表示整体与个体之间的关联关系，即整体和个体是一种contains-a的关系，例如人和头关系。 聚合与组合的区别 聚合关系没有组合紧密。学生不会因为班级的解散而无法存在，聚合关系的类具有不同的生命周期，互不相干；而学生如果没有头将无法存活，组合关系的类具有相同的生命周期。 在聚合关系中，可以同时了解Classes类和Student类，因为他们是独立的。在组合关系中，只认识Student类，根本不知道Head类的存在，因为头被严密地封装在学生类中。 总来说聚合的成员可独立，组合的成员必须依赖于整体才有意义。","link":"/2019/08/06/2019-08-06-use-uml/"},{"title":"设计模式之命令模式","text":"定义将一个请求封装为一个对象，从而使我们可用不同的请求对客户进行参数化；可以宏命令操作，对请求进行排队或者记录请求日志，以及支持可撤销的操作 使用场景 对请求发送者和请求接收者解耦，使得调用者和接收者不直接交互，而是通过请求对象 在某些场合，需要对请求进行记录，撤销/重做等处理，或者需要将多个请求组合在一起，即宏命令 对请求进行排队执行 模式结构在客户端生成命令对象，调用调用者对象Invoker中的**setCommand()**方法，设置命令对象（命令对象可以是一个或多个，命令也可以不执行或记录下来执行多次），在某个时间点再调用命令对象中的execute()方法执行命令，这将导致调用接受者receiver中的具体操作。 具体也就是把Invoker和Receiver解耦，他们之间没有直接的引用关系，将请求发送者与请求处理中责任分开。 时序图（网上扣得） 使用栗子假设有一个餐厅，里面有中餐（宫保鸡丁）和西餐（汉堡），中餐由中餐厨师负责，西餐由西餐厨师负责，点单由前台小妹负责。 在以往的餐厅流程可能是这样的： 客人点菜 -&gt; 菜单给前台小妹 -&gt; 前台小妹根据菜去通知不同厨师 -&gt; 厨师煮 -&gt; 出菜 引入了命令模式后把菜单封装成命令对象，前台小妹不需要自己去通知厨师，只需开单即可！不用关注是谁处理了这个订单，因为这个定单已经有了负责的厨师。 客人点菜 -&gt; 菜单（命令对象）给前台小妹 -&gt; 前台小妹开单（调用订单orderUp()方法） -&gt; 厨师煮 -&gt; 出菜 类图 代码厨师接口与具体的厨师，即命令的接受者 12345678910111213141516171819202122232425262728293031323334public interface Chef { void doCook(String name);}public class ChineseChef implements Chef { private String name; public ChineseChef(String name) { this.name = name; } @Override public void doCook(String foodName) { System.out.println(String.format(&quot;中餐厨师：%s 正在烹饪 %s&quot;, name, foodName)); }}public class WesternChef implements Chef { private String name; public WesternChef(String name) { this.name = name; } @Override public void doCook(String foodName) { System.out.println(String.format(&quot;西餐厨师：%s 正在烹饪 %s&quot;, name, foodName)); }} 订单接口与具体的菜品，即封装的命令对象 1234567891011121314151617181920212223242526272829303132333435public interface Order { void execute();}public class Hamburger implements Order { private Chef chef; public Hamburger(WesternChef westernChef) { this.chef = westernChef; } @Override public void execute() { chef.doCook(&quot;汉堡&quot;); }}public class KungPaoChicken implements Order { private Chef chef; public KungPaoChicken(ChineseChef chineseChef) { this.chef = chineseChef; } @Override public void execute() { chef.doCook(&quot;宫保鸡丁&quot;); }} 前台小妹Waiter，她是订单的发送者，但是她并不知道订单的内容是什么，到底由哪个厨师来处理。只知道调用订单里的execute就可以了。 通过封装订单，来减少前台小妹和厨师的接触机会，因为她并不喜欢油腻大叔。 12345678910111213141516171819public class Waiter { private String name; public Waiter(String name) { this.name = name; } public void takeOrder(Order newOrder) { System.out.println(name +&quot; 接到新的订单&quot;); newOrder.execute(); } public void orderUp(Order newOrder) { System.out.println(name +&quot; 来新订单了！！&quot;); newOrder.execute(); }} 调用 12345678910111213public static void main(String[] args) { Waiter waiter = new Waiter(&quot;菜花&quot;); Order hamburger = new Hamburger(new WesternChef(&quot;John&quot;)); Order chicken = new KungPaoChicken(new ChineseChef(&quot;大锤&quot;)); waiter.takeOrder(hamburger); waiter.takeOrder(chicken); // 输出 菜花 接到新的订单 西餐厨师：John 正在烹饪 汉堡 菜花 接到新的订单 中餐厨师：大锤 正在烹饪 宫保鸡丁} 在实际开发中，命令不一样要在处理者中处理（厨师），可以直接在封装的命令对象中自己处理。 上面一个订单只能点一个菜，如何客人需要点多个呢？ 对！没错！！封装一个支持多个命令的宏命令对象！ 123456789101112131415public class MacroOrder implements Order { private Order[] orders; public MacroOrder(Order[] orders) { this.orders = orders; } @Override public void execute() { for (Order order : orders) { order.execute(); } }} 在宏命令对象中有一个对象数组用于装载所有的订单，然后循环执行execute。 调用 123456789101112131415public class Main { public static void main(String[] args) { Waiter waiter = new Waiter(&quot;菜花&quot;); Order hamburger = new Hamburger(new WesternChef(&quot;John&quot;)); Order chicken = new KungPaoChicken(new ChineseChef(&quot;大锤&quot;)); Order macroOrder = new MacroOrder(new Order[]{hamburger,chicken}); waiter.takeOrder(macroOrder); // 输出 菜花 接到新的订单 西餐厨师：John 正在烹饪 汉堡 中餐厨师：大锤 正在烹饪 宫保鸡丁 }} 宏命令也是一个具体命令，不过它包含了对其他命令对象的引用，在调用宏命令的execute()方法时，将递归调用它所包含的每个成员命令的execute()方法，一个宏命令的成员对象可以是简单命令，还可以继续是宏命令。执行一个宏命令将执行多个具体命令，从而实现对命令的批处理。 一会儿有两个客人同时来吃饭了，这时前台小妹可以先招待玩一位客人点完菜，再招待另一位点菜，再一起处理。但是必须保证先来的客人的菜先做！ 这时就可以通过队列来保证命令按顺序执行。 1234567891011121314151617181920212223242526272829public class Waiter { private String name; private Queue&lt;Order&gt; orders; private int size; public Waiter(String name) { this.name = name; } public Waiter takeOrder(Order newOrder) { System.out.println(name +&quot; 接到新的订单&quot;); if (orders == null) { orders = new LinkedBlockingDeque&lt;&gt;(); } orders.add(newOrder); size ++; return this; } public void orderUp() { while (size &gt; 0) { Objects.requireNonNull(orders.poll()).execute(); size --; } }} 假设客人A先点了宫保鸡丁,B客人后点了汉堡 123456789101112public static void main(String[] args) { Waiter waiter = new Waiter(&quot;菜花&quot;); Order hamburger = new Hamburger(new WesternChef(&quot;John&quot;)); Order chicken = new KungPaoChicken(new ChineseChef(&quot;大锤&quot;)); waiter.takeOrder(chicken).takeOrder(hamburger).orderUp(); // 输出 菜花 接到新的订单 菜花 接到新的订单 中餐厨师：大锤 正在烹饪 宫保鸡丁 西餐厨师：John 正在烹饪 汉堡} 一位客人一不下心点了两个汉堡，他想退了一个。所以需要提供一个撤回订单的功能（当然要在未煮的情况下） 所以在前台小妹Waiter中加了撤回订单的功能！ 12345678public boolean revoke(Order order) { System.out.println(name +&quot; 准备为客人撤回订单&quot;); if (size &gt; 0 &amp;&amp; orders.remove(order)) { size --; return true; } return false;} 调用 public static void main(String[] args) { Waiter waiter = new Waiter(&quot;菜花&quot;); Order hamburger = new Hamburger(new WesternChef(&quot;John&quot;)); Order hamburger1 = new Hamburger(new WesternChef(&quot;John&quot;)); waiter.takeOrder(hamburger1).takeOrder(hamburger); System.out.println(waiter.revoke(hamburger1)? &quot;订单撤回成功&quot; : &quot;订单撤回失败&quot;); waiter.orderUp(); System.out.println(waiter.revoke(hamburger)? &quot;订单撤回成功&quot; : &quot;订单撤回失败&quot;); // 输出 菜花 接到新的订单 菜花 接到新的订单 菜花 准备为客人撤回订单 订单撤回成功 西餐厨师：John 正在烹饪 汉堡 菜花 准备为客人撤回订单 订单撤回失败 } 请求日志功能 总结 在命令模式中，将一个请求封装为一个对象，从而使我们可用不同的请求对客户进行参数化；对请求排队或者记录请求日志，以及支持可撤销的操作。 命令模式包含四个角色： 抽象命令类中声明了用于执行请求的execute()等方法，通过这些方法可以调用请求接收者的相关操作； 具体命令类是抽象命令类的子类，实现了在抽象命令类中声明的方法，它对应具体的接收者对象，将接收者对象绑定其中，并由它来对接收者的调用； 调用者即请求的发送者，又称为请求者，它通过命令对象来执行请求； 接收者执行与请求相关的操作，它具体实现对请求的业务处理。 命令模式的本质是对命令进行封装，将发出命令的责任和执行命令的责任分割开。命令模式使请求本身成为一个对象。 命令模式适用情况包括： 需要将请求调用者和请求接收者解耦，使得调用者和接收者不直接交互； 需要在不同的时间指定请求、将请求排队和执行请求； 需要支持命令的撤销操作和恢复操作，需要将一组操作组合在一起，即支持宏命令。 优缺点优点 命令模式的主要优点在于降低系统的耦合度，将请求调用者和请求接收者解耦。调用者无需关注具体怎么处理。 增加新的命令很方便，而且可以比较容易地设计一个命令队列和宏命令，并方便地实现对请求的撤销和恢复； 缺点 主要缺点在于可能会导致某些系统有过多的具体命令类。","link":"/2019/08/07/2019-08-07-command-pattern/"},{"title":"jvm之运行时数据区","text":"Java虚拟机运行Java程序时会将所管理的内存分为不同的数据区域。这些区域的作用以及生命周期都不同。有的随虚拟机进程创建而创建，有的则随线程的启动和结束而建立和销毁。下图是 JDK1.7的运行时数据区 1.7运行时数据区 如上图所示，可分为线程共享（ 堆， 方法区）和线程私有数据区（ 虚拟机栈， 本地方法栈， 程序计数器）。 程序计数器 程序计数器是一块较小的空间，可以看成是 当前线程所执行字节码的行号指示器。字节码解释器就是通过改变计数器的值来选取下一个指令是什么。Java多线程应用是通过线程切换竞争CPU时间片实现的。在单核处理器中，同一确定时刻，只会执行一个线程中的指令。所以为了切换后能够恢复到正确的位置，每个线程都需要独自保存程序计数器。各线程计数器互不影响，独立存储。 如果线程执行的是一个Java方法，那么计数器记录的是字节码指令的地址；如果执行的是native方法，那么计时器为undefined。 此区域是唯一一块没有规定会发生内存溢出（OutOfMemoryError）的区域 Java虚拟机栈 和程序计数器一样Java虚拟机栈也是线程私有的，生命周期和线程相同。虚拟机栈描述的是Java方法的执行的内存模型：每个方法在执行的时候都会创建一个 栈帧，用于保存 局部变量， 操作数栈， 动态链接， 方法出口等。每一个方法的执行都对应着一个栈帧在虚拟机栈的的入栈与出栈过程。 我们平常说的栈内存就是指的虚拟机栈的局部变量表，局部变量表保存着 8大基本数据类型和 对象引用变量（可能指向堆内存的地址，也可能指向句柄地址）以及 returnAddress（返回类型？）类型 （指向一条字节码指令的地址）。 64长度的long和double会占用两个局部变量空间，其他都只会占用一个。当进入一个方法时，这个方法需要在帧中分配多少的空间是确定的，在方法执行期间不会改变局部变量表大小，即 栈内存不会动态改变。 在这区域规定了两种异常：当线程请求的栈深度大于虚拟机的所允许深度，将抛出 StackOverFlowError（内存泄露）异常；如果虚拟机可以动态扩展（Java虚拟机栈可以设置长度），如果动态扩展无法申请到足够的长度，那么将抛出OutOfMemoryError（内存溢出）异常。 本地方法栈 本地方法栈的作用与Java虚拟机栈类似，它们的区别是虚拟机栈为虚拟机执行Java方法（字节码）服务，而本地方法栈为虚拟机使用Native方法。在Sun HotSpot中（JDK使用的虚拟机），将Java虚拟机栈和本地方法栈合二为一。在本地方法栈中也会抛出 StackOverFlowError（内存泄露）异常和 OutOfMemoryError（内存溢出）异常。 堆内存 堆内存（Heap Memory）是虚拟机所管理的较大一块内存。也是被所有线程所共享的，在 虚拟机启动而创建。此内存区域的作用就是保存对象实例， 几乎所有的对象实例都在这里分配内存， 包括对象和数组。 Java堆也是垃圾回收器管理的主要区域。从辣鸡回收角度来说，由于基本都采用 分代收集算法，所以在Java堆中还可以分为： 新生代和老年代：再细致一点有Eden空间、From Survivor、To Survivor空间等。从内存分配角度来看，线程共享的堆内存可能划分出几个线程私有的分配缓存区。但 其存储的还是对象实例，只不过为了更好的回收内存或更快的分配内存。 堆内存可以处于在一块地址不连续的内存空间上。其大小可以通过-Xmx和-Xms控制，如果堆中没有内存完成实例分配，并且内存大小不可扩展那么将抛出OutOfMemoryError（内存溢出）异常。 方法区 方法区和Java堆一样，也是被所有线程共享的一块区域。用于存储已经被虚拟机加载的类信息，常量，静态变量，Jit编译后的代码，动态代理生成的字节码文件等数据。在HotSpot虚拟机中，方法区还被称为永久代，他们两个的关系可以看成是一种实现的关系。方法区是Java虚拟机的规范，而永久代是HotSpot对这个规范的实现。其它的虚拟机中是没有永久代这个东西的。如IBMJ9等。 永久代有 -XX:MaxPermSize大小限制 会抛出OOM异常。 在JDK1.7中，把字符串常量池从永久代中移出，放到了Java堆中。而在JDK1.8中，已经取消了永久代，而采用本地内存（Native Memory）来实现方法区了，下面会详细介绍。 方法区分配不要求连续的内存，可以选择固定的大小，还可以扩展，并且可以选择不实现垃圾收集 。当方法区无法满足内存分配时，将抛出 OutOfMemoryError（内存溢出）异常。 运行时常量池 运行时常量池是方法区的一部分。Class文件中（字节码文件.class）除了有类的版本，字段，方法，接口等信息外，还有常量池，用于存储编译期生成的各种 字面量（基本类型数据，字符串）和 符号引用 （即不知道引用的实际地址或不知道对象的值，而用 符号 来代替），这部分内容（常量池）将在类加载后进入 运行时常量池 存放。一般， 直接引用 （直接指向目标内存地址，指向可以找到目标的中间句柄，相对偏移量）也存储在运行时常量池中。 运行时常量池相对于Class文件常量池另一个特征是具备动态性 。即常量不一定在编译时产生，也就是并非在Class常量池中的内容才能进入运行时常量池。运行时也可能加入新的常量到常量池。如String类的intern()方法。 运行时常量池在无法申请到内存时会抛出 OutOfMemoryError（内存溢出）异常。 直接内存 对HotSpot来说，不受GC管理的内存都是Native Memory（即除了虚拟机用的内存，宿主机的其他内存都统称本地内存？）。在JDK1.4中引入NIO，它可以用Native函数直接分配堆外内存（本地内存），然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。所以”Direct Memory”可以看成：Java程序通过一组特定的API访问Native Memory，API就是DirectByteBuffer，被访问的内存就是Direct Memory 。 直接内存也会受到本机内存的限制，动态扩展也可能出现 OutOfMemoryError（内存溢出）异常。","link":"/2019/08/19/2019-08-19-understanding-jvm-1/"},{"title":"git简单使用","text":"git status查看git状态，如果本地有更改或者新增的文件（或有可执行的操作？）则会显示。并且会提示如何操作。 有更改或新增 有待提交文件 有带推送文件 git commit -mgit commit 提交修改到本地服务器。-m 参数为修改备注 git push推送到远程仓库中 git reset HEAD取消暂存文件 其他用到在说。。。。","link":"/2019/08/19/2019-08-19-use-git/"},{"title":"设计模式之外观模式","text":"定义 提供一个统一接口，用来访问一群的子系统接口。外观模式定义了一个高层接口，让子系统更容易使用。 在上一个适配者模式中，将一个对象包起来（当然也可以是多个）， 转换其接口。而外观模式则是将对象包起来，简化其接口。 类图 外观对象中有许多子系统的实例，在外观提供的接口中统一对子系统调用（上图对Sub1和Sub2接口调用），子系统也可能再调用用其它的子系统（Sub1对Sub3和Sub5的接口调用，Sub2对Sub4和Sub5），即使它内部再复杂，对于客户端来说只要调用外观Facade的接口就行了，内部是由谁实现的，怎么实现的并不关心。也就是 将客户端与子系统的类解耦。 来个栗子 如洗衣机，我们一般洗衣的步骤是加水 -&gt; 洗涤 -&gt; 漂洗 -&gt; 脱干 这几个步骤当然可能还会有烘干。如果不是全自动的洗衣机，可能我们把衣服放进去，然后按开始加水的按钮，慢慢等待加水，终于加满了！！放满后我们再按洗涤的按钮。。。一步步的。。更可怕的是有可能水还没加足我们就按了开始洗涤；水还没放完就按了开始脱干。。。 这时如果将这几个操作封装成一个操作，当加完水后洗衣机自己就会开始洗涤，接着漂洗。。。而且重要的是它知道何时开始脱干！ ps：这个类图和上面那个不是一软件画的。觉得上面那个太难用了。这个是一个在线网页上画的 diagrams.visual-paradigm.com 在上面的图中可以看到有一个洗衣机WashingMachine，里面有洗涤Washing，漂洗Rinse，脱干Dehydration，自动洗衣autoWashing（里面包含前面三个操作） 下面看代码： 先是加水，放水功能 12345678910111213141516171819202122232425262728293031323334353637/** * * 加水功能 * */public class Water { private int water = 0; public void turnOn() { while (water &lt;= 40) { water ++; System.out.println(String.format(&quot;加水中-------&gt;当前水量：%d&quot;, water)); try { Thread.sleep(200); } catch (InterruptedException e) { e.printStackTrace(); } } } public void turnOff() { while (water &gt;= 0) { water --; System.out.println(String.format(&quot;放水中-------&gt;当前水量：%d&quot;, water)); try { Thread.sleep(200); } catch (InterruptedException e) { e.printStackTrace(); } } } public int getWater() { return water; }} 洗涤功能 在洗涤之前应该要先加水，所以在洗涤类中应该有放水的功能 123456789101112131415161718192021222324252627282930/** * * 洗涤功能 * */public class Washing { public Washing(Water water) { this.water = water; } private Water water; public void start() { // 洗涤前先加水 water.turnOn(); System.out.println(&quot;开始洗涤。。。&quot;); try { Thread.sleep(2000); stop(); } catch (InterruptedException e) { e.printStackTrace(); } } public void stop() { System.out.println(&quot;结束洗涤。。。&quot;); }} 漂洗功能 在漂洗之前应该要先放水，再加水。 123456789101112131415161718192021222324252627282930313233/** * * 漂洗功能 * */public class Rinse { private Water water; public Rinse(Water water) { this.water = water; } public void start() { // 先放水 water.turnOff(); // 加水 water.turnOn(); System.out.println(&quot;开始漂洗。。。。&quot;); try { Thread.sleep(2000); stop(); } catch (InterruptedException e) { e.printStackTrace(); } } public void stop() { System.out.println(&quot;结束漂洗。。。。&quot;); }} 脱干功能 在脱干之前也应该要先放水 123456789101112131415161718192021222324252627282930/** * * 脱干功能 * */public class Dehydration { private Water water; public Dehydration(Water water) { this.water = water; } public void start() { // 放水 water.turnOff(); System.out.println(&quot;开始脱干。。。&quot;); try { Thread.sleep(2000); stop(); } catch (InterruptedException e) { e.printStackTrace(); } } public void stop() { System.out.println(&quot;脱干结束。。。&quot;); }} 洗衣机类 这里直接把功能的创建直接放在了洗衣机里，为了体现具体的功能实现和客户端是没有关系的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * * 洗衣机 * */public class WashingMachine { private static Water water = new Water(); private static Washing washing = new Washing(water); private static Rinse rinse = new Rinse(water); private static Dehydration dehydration = new Dehydration(water); /** * * 自动 * */ public void autoWashing() { washing.start(); rinse.start(); dehydration.start(); } /** * * 洗涤 * */ public void doWashing(){ washing.start(); } /** * * 漂洗 * */ public void doRinse() { rinse.start(); } /** * * 脱干 * */ public void doDehydration() { dehydration.start(); }} 最后是调用 123456public class Main { public static void main(String[] args) { new WashingMachine().autoWashing(); }} 就可以看到洗衣机自动的就完成了洗涤，漂白，脱干的功能，而不用一个一个去操作，等待，对于使用者来说更加的方便了，懒人福音！而且我们如果只想要洗涤或脱干任一功能也是可以的。 总结 最少知识原则：尽量减少类之间的耦合。意思就是如果一个类依赖了太多的类，那么系统会变得很复杂，而且修改其中一部分会影响到其他部分。比如有A，B，C三个类，如果能做到A依赖B，B依赖C，那么绝不要让A直接依赖B和C。如上面的洗衣机，对于客户端来说，只和洗衣机有关系。即使洗衣机内部再怎么修改，也不会影响到客户端。 适配器模式 将一个对象包装起来，改变其接口，以符合客户期望； 外观模式 将一群对象包起来，委托子系统们去执行，简化接口； 装饰者模式 将一个对象包装起来，增加新行为或责任 不要试图通过外观类为子系统增加新行为：外观模式的用意是集合子系统，简化接口。如果要新增新的行为，正确的做法是修改现有的子系统或增加新的子系统。 引入抽象外观：如果有新的子系统增加或删除子系统，那么势必会修改现有的外观。可以做的是客户端针对抽象外观进行编程，有的子系统更改则可以创建新的外观实现类。 优缺点缺点 增加新的子系统可能需要修改外观类或客户端的代码，违背了开闭原则。 优点 客户端与多个子系统之间存在很大的依赖性，引入外观类将子系统与客户端以及其他子系统解耦，可以提高子系统的独立性和可移植性。 其他感觉外观模式和命令模式的宏命令挺像的，都可以执行一组”命令”。不过命令模式将动作和执行者封装成一个命令，将请求发送者和执行者解耦。外观模式也可以看成是发送者和真正的执行者解耦。","link":"/2019/08/27/2019-08-27-facade-pattern/"},{"title":"设计模式之模板方法模式","text":"定义 在一个方法中定义一个算法的骨架，将一些步骤由子类来实现。模版方法使得子类可以在不改变算法结构的情况下，重复定义算法中的步骤。 在父类中定义一个模版方法，里面去调用其它的方法，如果被调用的方法是可以复用的话，则可以在父类内部实现，如果需要子类自己实现，则可以 声明成抽象的方法，由具体的子类实现。如果有一些不确定是否要执行的步骤或者在执行某一些步骤中需要做一些事情，则可以创建一些 钩子函数{:id=””} 类图![XQBF7ZO`JWXOM1H24L_7MK4.png](https://i.loli.net/2019/09/15/7NAg8T3nt4OIFxj.png) 此模式可以将一些相似的事物进行抽离成一个父类，就和我们平常封装差不多，但是和平常不同的是，我们会提供一个模版方法，并将不同的行为进行泛化，在模版方法中调用泛化后的方法 ，泛化的方法由不同的子类自己实现不同的操作。就比如茶和咖啡，它们有共同的地方如：要先煮开水；也有不同的地方：加入咖啡粉或者加入茶叶。这样我们就可以将煮开水的操作由父类来实现，进行复用，而加咖啡粉或者茶叶则可以泛化成一个方法，如 冲泡brew() ，茶或咖啡的类自己实现如何冲泡。冲泡之后可以选择加一些料，咖啡可以加奶啊，糖啊，茶叶也可以加奶啊，柠檬啊。这时我们就可以用上 hooks ，用来询问是否要执行加料的操作。当然如果是必须要加的也就没必要用钩子了。 来个例子 尝试一下上面说的茶和咖啡的例子，会有煮开水操作：boilWater() ，冲泡操作：brew() 和加料操作：addCondiments() 类图 代码父类AbstractBeverage： 1234567891011121314151617181920212223242526272829303132333435363738394041public abstract class AbstractBeverage { public final void prepare() { boilWater(); brew(); if (hooks()) { addCondiments(); } } /** * * 冲泡方法，由子类实现 * */ public abstract void brew(); public void boilWater() { System.out.println(&quot;开水煮沸了。。。&quot;); } /** * 询问是否需要加料，默认为false * * @return boolean */ public boolean hooks() { return false; } /** * * 具体加料方法 * */ public abstract void addCondiments();} 咖啡类： 123456789101112131415161718public class Coffee extends AbstractBeverage { @Override public void brew() { System.out.println(&quot;加入咖啡粉冲泡。。&quot;); } @Override public boolean hooks() { return true; } @Override public void addCondiments() { System.out.println(&quot;加奶。。&quot;); }} 茶类：不需要加料，所以不需要重写钩子，不管加料方法 123456789101112public class Tea extends AbstractBeverage { @Override public void brew() { System.out.println(&quot;将茶叶放入开水中冲泡。。&quot;); } @Override public void addCondiments() { // 茶不需要加料 所以不管 }} 执行12345678910111213141516public class Main { public static void main(String[] args) { AbstractBeverage tea = new Tea(); tea.prepare(); AbstractBeverage coffee = new Coffee(); coffee.prepare(); // 开水煮沸了。。。 //将茶叶放入开水中冲泡。。 // 开水煮沸了。。。 // 加入咖啡粉冲泡。。 // 加奶。。 }} jdk中的🌰 ArrayList 继承了抽象类 AbstractList ，有许多方法在AbstractList中已经有默认的实现了。如： indexOf,equals等等等，还有的是抽象方法等着子类去实现的，如：get public abstract E get(int index); 在AbstractList内部有Iterator的私有实现类，这是AbstractList提供的默认迭代器，也是继承者们默认拥有的迭代器（如果没有自己实现iterator(),listIterator()等方法）。在迭代器方法next(), remove()中，调用了get()，remove()方法。可以看到这两个方法在 AbstractList 内部并没有真正的实现，而是直接抛出异常！意思就是必须你自己去实现，不实现就用不了了！所以迭代器中方法也可以看做是模板方法，其中一部分等待子类去实现。 // AbstractList提供的迭代器中方法 public E next() { checkForComodification(); try { int i = cursor; // 调用抽象方法get E next = get(i); lastRet = i; cursor = i + 1; return next; } catch (IndexOutOfBoundsException e) { checkForComodification(); throw new NoSuchElementException(); } } // AbstractList提供的迭代器中方法 public void remove() { if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try { // 调用AbstractList的remove方法 AbstractList.this.remove(lastRet); if (lastRet &lt; cursor) cursor--; lastRet = -1; expectedModCount = modCount; } catch (IndexOutOfBoundsException e) { throw new ConcurrentModificationException(); } } // AbstractList中被调用的remove，默认实现直接抛异常 public E remove(int index) { throw new UnsupportedOperationException(); } 在上面例子中，就可以看到模板方法在jdk中的使用。父类实现公共的方法（indexOf,equals），让子类实现某些步骤即实现某些方法（get），或是父类提供一些默认的实现（迭代器）。 或许还可以把remove()等默认抛异常的看成是“钩子函数”！ 总结 模板方法定义了算法的步骤，并把这些步骤延迟到子类 模板方法中可以定义具体方法，抽象方法，钩子。抽象方法由子类实现，钩子是一种方法，在抽象类中不做事，或者只做默认的事。 为了子类修改模板方法，可以将模板方法申明为final 钩子函数 通俗一点讲，可以看成是在执行到某一个步骤时调用的方法，比如 React组件生命周期的钩子函数，当执行到的时候就会调用钩子，来询问是否要做些什么","link":"/2019/09/15/2019-09-15-template-method-pattern/"},{"title":"设计模式之迭代器模式","text":"定义 提供一种方法顺序访问一个聚合对象中的各个元素，而又不暴露其内部的表示 聚合对象 是什么呢？可以看成将多个对象用某种数据结构聚集在一起。可以是一个数组Array，可以是一个列表List，也可以是一个散列表Map。 实例 教育局要求每个学校提供一个接口，要求该接口返回每个学校的学生列表。由于没有统一的接口标准，所以该接口由学校自己设计实现。A校返回的是一个ArrayList， B校则是一个Array。具体的代码如下： 1234567891011121314151617/** * @author ddmcc */@Data@AllArgsConstructorpublic class Student { private String name; private int age; private String sex; private String address;} 12345678910111213141516171819/** * A校返回ArrayList * * @author ddmcc */public class ListSchool { private ArrayList&lt;Student&gt; list = new ArrayList&lt;&gt;(); public ListSchool addStudent(String name, int age, String sex, String address) { list.add(new Student(name, age, sex, address)); return this; } public ArrayList&lt;Student&gt; getStudentList() { return this.list; }} 1234567891011121314151617181920212223242526272829303132333435/** * B校返回Array * * @author ddmcc * * * B校返回Array * * @author ddmcc */public class ArraySchool { private Student[] students; private static final int MAX_LENGTH = 100; private int position = 0; public ArraySchool() { students = new Student[MAX_LENGTH]; } public ArraySchool addStudent(String name, int age, String sex, String address) { if (position &lt; MAX_LENGTH) { students[position] = new Student(name, age, sex, address); position += 1; } return this; } public Student[] getStudentArray() { return this.students; }} 由于两个学校返回的类型并不相同，教育局要遍历学生列表就不得不针对两个列表进行实现。遍历代码如下： 1234567891011121314151617181920212223242526272829/** * 教育局 * * @author ddmcc */public class EducationBureau { public void printStudent() { ListSchool listSchool = new ListSchool(); listSchool.addStudent(&quot;大锤&quot;, 22, &quot;男&quot;, &quot;上海&quot;) .addStudent(&quot;二狗&quot;, 23, &quot;男&quot;, &quot;北京&quot;) .addStudent(&quot;菜花&quot;, 22, &quot;女&quot;, &quot;东北&quot;); ArraySchool arraySchool = new ArraySchool(); arraySchool.addStudent(&quot;铁蛋&quot;, 24, &quot;男&quot;, &quot;厦门&quot;) .addStudent(&quot;铁锤&quot;, 23, &quot;男&quot;, &quot;上海&quot;); ArrayList&lt;Student&gt; arrayList = listSchool.getStudentList(); for (int i = 0; i &lt; arrayList.size(); i++) { System.out.println(arrayList.get(i)); } Student[] array = arraySchool.getStudentArray(); for (int i = 0; i &lt; array.length; i++) { System.out.println(array[i]); } }} 从上面的代码可以看出由于两个学校返回的类型不一致，所以写了两遍的循环代码。其实不单单是遍历，如有其它对两个列表进行操作，也必须进行不同的实现。 而且教育局还知道了两个学校存储对象的方式，这也不是学校想要的。后期还有学校加入，返回的又是不同的数据类型，还得再针对实现。。。如果我们让学校自己提供遍历的方法或遍历的工具呢？ 这样我们就不需要去考虑数据结构的问题了。但是这样又有新的问题，没有统一的标准，可能A校提供的方法叫printList ,B校提供的确叫printArray。 封装代码变化部分是我们代码设计的一个原则。针对上面的问题，也就是封装对象的遍历。现在由我们来提供遍历的工具（Iterator迭代器），由各自学校来实现这个迭代器，教育局在拿着迭代器自己去遍历， 这样也就可以解决了不同学校返回不同列表的问题，这样做也封装了遍历，教育局现在并不知道学校是如何维护对象的，也不知道迭代器是如何实现的。 123456789101112/** * 自定义迭代器接口 * * @author ddmcc */public interface Iterator&lt;T&gt; { boolean hasNext(); T next();} 上面是教育局提供的接口，由学校去实现。就像Java提供的JDBC接口，mysql，oracle去实现一样。。。下面是A校提供的迭代器： 123456789101112131415161718192021222324252627282930313233/** * A提供的列表迭代器 * * @author ddmcc */public class ArrayListIterator&lt;T&gt; implements Iterator&lt;T&gt; { /** * 下一个返回元素索引 */ private int cursor = 0; /** * 列表数据 */ private ArrayList&lt;T&gt; list; public ArrayListIterator(ArrayList&lt;T&gt; list) { this.list = list; } @Override public boolean hasNext() { return cursor &lt; list.size(); } @Override public T next() { T t = list.get(cursor); cursor += 1; return t; }} 这样A校就不在需要提供获取列表的接口，而提供一个返回迭代器的接口： public ArrayList getStudentList() { return this.list;} public Iterator getIterator() { return new ArrayListIterator&lt;&gt;(this.list);} 再利用迭代器进行遍历操作 123456789101112public void printStudent() { ListSchool listSchool = new ListSchool(); listSchool.addStudent(&quot;大锤&quot;, 22, &quot;男&quot;, &quot;上海&quot;) .addStudent(&quot;二狗&quot;, 23, &quot;男&quot;, &quot;北京&quot;) .addStudent(&quot;菜花&quot;, 22, &quot;女&quot;, &quot;东北&quot;); Iterator&lt;Student&gt; iterator = listSchool.getIterator(); while (iterator.hasNext()) { Student student = iterator.next(); System.out.println(student.toString()); }} 会发现现在教育局并不知道A校是怎么存储学生对象的，也不知道迭代器是怎么实现遍历的。。。让各个对象更专注在自己所应该专注的地方上。 下面为B校代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * 迭代器 * * @author ddmcc */public class ArrayIterator&lt;T&gt; implements Iterator&lt;T&gt; { private int position = 0; private T[] items; public ArrayIterator(T[] items) { this.items = items; } @Override public boolean hasNext() { return position &lt; items.length &amp;&amp; items[position] != null; } @Override public T next() { T t = items[position]; position += 1; return t; }}/** * B校返回Array * * @author ddmcc */public class ArraySchool { private Student[] students; private static final int MAX_LENGTH = 100; private int position = 0; public ArraySchool() { students = new Student[MAX_LENGTH]; } public ArraySchool addStudent(String name, int age, String sex, String address) { if (position &lt; MAX_LENGTH) { students[position] = new Student(name, age, sex, address); position += 1; } return this; } // 注意这个方法名 public Iterator&lt;Student&gt; iterator() { return new ArrayIterator(students); }}/** * 教育局 * * @author ddmcc */public class EducationBureau { public void printStudent() { ArraySchool arraySchool = new ArraySchool(); arraySchool.addStudent(&quot;铁蛋&quot;, 24, &quot;男&quot;, &quot;厦门&quot;) .addStudent(&quot;铁锤&quot;, 23, &quot;男&quot;, &quot;上海&quot;); Iterator&lt;Student&gt; iterator = arraySchool.iterator(); while (iterator.hasNext()) { Student student = iterator.next(); System.out.println(student.toString()); } }} 从上面的代码会发现其实还存在一些问题，比如两个学校因为没有一个标准，所以提供的迭代器的方法并不一致。用户同时依赖了具体的类ListSchool和ArraySchool。所以还可以创建一个学校接口，提供一个获取迭代器的方法，用户也就可以多态编程了。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/** * @author ddmcc */public interface School { Iterator iterator(); School addStudent(String name, int age, String sex, String address);}/** * A校返回ArrayList * * @author ddmcc */public class ListSchool implements School{ private ArrayList&lt;Student&gt; list = Lists.newArrayList(); @Override public School addStudent(String name, int age, String sex, String address) { list.add(new Student(name, age, sex, address)); return this; } @Override public Iterator iterator() { return new ArrayListIterator&lt;&gt;(this.list); }}/** * 教育局 * * @author ddmcc */public class EducationBureau { public void printStudent() { School arraySchool = new ArraySchool(); arraySchool.addStudent(&quot;铁蛋&quot;, 24, &quot;男&quot;, &quot;厦门&quot;) .addStudent(&quot;铁锤&quot;, 23, &quot;男&quot;, &quot;上海&quot;); Iterator&lt;Student&gt; iterator = arraySchool.iterator(); while (iterator.hasNext()) { Student student = iterator.next(); System.out.println(student.toString()); } }} 上面代码的类图 JDK中的迭代器 JDK中也有迭代器Iterator接口，位于java.util包下，该接口有方法：hasNext(),next(),remove(),forEachRemaining()。在集合顶层接口Collection中有一个方法Iterator&lt;E&gt; iterator() ，用来返回一个迭代器，所以Collection下的所有子类都支持迭代器遍历。而且在AbstractList也提供了默认的迭代器。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public Iterator&lt;E&gt; iterator() { return new Itr();} private class Itr implements Iterator&lt;E&gt; { int cursor = 0; int lastRet = -1; int expectedModCount = modCount; public boolean hasNext() { return cursor != size(); } public E next() { checkForComodification(); try { int i = cursor; E next = get(i); lastRet = i; cursor = i + 1; return next; } catch (IndexOutOfBoundsException e) { checkForComodification(); throw new NoSuchElementException(); } } public void remove() { if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try { AbstractList.this.remove(lastRet); if (lastRet &lt; cursor) cursor--; lastRet = -1; expectedModCount = modCount; } catch (IndexOutOfBoundsException e) { throw new ConcurrentModificationException(); } } final void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); } } 而且提供了向前遍历的迭代器，通过方法listIterator()来获得。当然子类也可以覆盖它的方法自己去实现迭代器。就比如ArrayList它就覆盖了iterator()，并且自己实现一个优化版本的迭代器。 ArrayList内部实现的迭代器 ： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * An optimized version of AbstractList.Itr */ private class Itr implements Iterator&lt;E&gt; { int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; // prevent creating a synthetic constructor Itr() {} public boolean hasNext() { return cursor != size; } @SuppressWarnings(&quot;unchecked&quot;) public E next() { checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; } public void remove() { if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try { ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; expectedModCount = modCount; } catch (IndexOutOfBoundsException ex) { throw new ConcurrentModificationException(); } } @Override public void forEachRemaining(Consumer&lt;? super E&gt; action) { Objects.requireNonNull(action); final int size = ArrayList.this.size; int i = cursor; if (i &lt; size) { final Object[] es = elementData; if (i &gt;= es.length) throw new ConcurrentModificationException(); for (; i &lt; size &amp;&amp; modCount == expectedModCount; i++) action.accept(elementAt(es, i)); // update once at end to reduce heap write traffic cursor = i; lastRet = i - 1; checkForComodification(); } } final void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException(); } } 会发现当我们使用list的迭代器的时候根本无需关注内部实现，要不是点进源码查看，还不知道它内部的迭代器是叫Itr。除了集合家族有迭代器，散列表中也有迭代器，不过因为数据结构的，它有键迭代器，值迭代器，以及元素迭代器。 总结 创建一个迭代器接口用于封装聚合对象的遍历，由聚合对象提供具体的遍历方式 将用户与具体的聚合对象解耦，无须关注其内部实现。 便于后期扩展，其实更改聚合对象，也无须更改处理对象的逻辑部分。只需提供不同的迭代器即可。","link":"/2019/10/08/2019-10-08-iterator-pattern/"},{"title":"设计模式之状态模式","text":"状态模式 我们用饮料贩卖机工作的例子来讲解状态模式的实现。把贩卖机的工作流程分解，可以分为一般为 待售 -&gt; 选择饮料 -&gt; 插入硬币 -&gt; 售出 -&gt; 退出饮料 -&gt; 回到待售状态。 每一次售出饮料都是这个步骤，贩卖机的状态始终在这些状态中游走。同时为了我们的贩卖机更加的安全，需要在每次请求的时候判断当前的状态是否允许这么做。比如贩卖机要退出饮料，要先确认当前是否为售出的状态？以及饮料的库存 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152/** * @author ddmcc */@Datapublic class DrinkMachine { /** * 售罄状态 */ private static final int SOLD_OUT = 0; /** * 没有硬币 也就是待售状态 */ private static final int NO_COIN = 1; /** * 选择饮料 */ private static final int CHOOSE_DRINK = 2; /** * 投入硬币 */ private static final int HAS_COIN = 3; /** * 售出状态 等待退出饮料 */ private static final int SOLD = 4; /** * 当前剩余饮料数量 */ int count = 0; /** * 当前状态 默认为售罄 */ int currentState = SOLD_OUT; public DrinkMachine(int count) { this.count = count; if (count &gt; 0) { currentState = NO_COIN; } } /** * 选择饮料 */ public void chooseDrink() { if (currentState == NO_COIN) { currentState = CHOOSE_DRINK; System.out.println(&quot;饮料选择成功，请投币！&quot;); } else if (currentState == SOLD_OUT) { System.out.println(&quot;饮料已售罄！&quot;); } else if (currentState == CHOOSE_DRINK) { System.out.println(&quot;请勿重复选择！&quot;); } else if (currentState == HAS_COIN) { System.out.println(&quot;请勿重复选择！&quot;); } else if (currentState == SOLD) { System.out.println(&quot;请勿重复选择！&quot;); } } /** * 投币操作 */ public void insertCoin() { if (currentState == CHOOSE_DRINK) { currentState = HAS_COIN; System.out.println(&quot;投币成功，请稍等！&quot;); } else if (currentState == SOLD_OUT) { System.out.println(&quot;投币失败，饮料已售罄&quot;); } else if (currentState == NO_COIN) { System.out.println(&quot;请先选择饮料！&quot;); } else if (currentState == HAS_COIN) { System.out.println(&quot;请勿重复投币！&quot;); } else if (currentState == SOLD) { System.out.println(&quot;请勿重复投币！&quot;); } } /** * 退币操作 */ public void ejectCoin() { if (currentState == HAS_COIN) { currentState = NO_COIN; System.out.println(&quot;退币成功！&quot;); } else if (currentState == SOLD_OUT) { System.out.println(&quot;退币失败！&quot;); } else if (currentState == CHOOSE_DRINK) { System.out.println(&quot;退币失败！&quot;); } else if (currentState == NO_COIN) { System.out.println(&quot;请先投币！&quot;); } else if (currentState == SOLD) { System.out.println(&quot;退币失败！&quot;); } } /** * 确认购买饮料操作 */ public void returnDrink() { if (currentState == HAS_COIN) { currentState = SOLD; System.out.println(&quot;请收好饮料！&quot;); dispense(); } else if (currentState == SOLD_OUT) { System.out.println(&quot;饮料已售罄！&quot;); } else if (currentState == CHOOSE_DRINK) { System.out.println(&quot;请勿重复选择！&quot;); } else if (currentState == NO_COIN) { System.out.println(&quot;请先投币！&quot;); } else if (currentState == SOLD) { System.out.println(&quot;饮料已退出！&quot;); } } /** * 退出饮料 */ public void dispense() { if (currentState == SOLD) { count -= 1; System.out.println(&quot;欢迎下次光临！&quot;); if (count &gt;= 1) { currentState = NO_COIN; } else { currentState = SOLD_OUT; } } else if (currentState == NO_COIN) { System.out.println(&quot;请先选择饮料！&quot;); } else if (currentState == CHOOSE_DRINK) { System.out.println(&quot;请先投币！&quot;); } else if (currentState == HAS_COIN) { System.out.println(&quot;请点击退出饮料！&quot;); } else if (currentState == SOLD_OUT) { System.out.println(&quot;饮料已售罄！&quot;); } }} 上面是根据贩卖机的工作流程编写的代码。首先贩卖机类内部有几种状态常量，还有记录当前状态和当前的数量的变量。然后是几个操作方法分别为：选择饮料，投币，退币，机器退出饮料，发放饮料。 选择饮料只有在待售的状态才能选择饮料，否则提示错误信息。选择饮料后则将状态转为选择饮料的状态等待投入硬币 投币只有在选择饮料状态下才能够投币，投币后变为已投币状态等待用户确认购买操作 退币在已投币状态下用户可以选择退币，退币后状态转为未投币 确认购买饮料在已投币状态下除了可以退币，还可以确认购买，确认购买后饮料机将状态转为SOLD状态并调用dispense()方法，执行发放饮料操作 发放饮料将count - 1并判断剩余数量，转为售罄或未投币 上面的实现代码满足了贩卖机的需求，但是并不是健壮的代码。首先违反了开-闭原则，当有新的状态加入我们就不得不重新打开代码这个类来修改，并且每个方法都会被影响。。。 而且代码可读性也不佳。if-else是很影响代码可读性的，state的变化也隐藏在if语句里，并不明显。很可能对后面维护这些代码的人带来麻烦。重要的是没有将变化的部分 封装 起来！ 引入状态模式 现在我们尝试将“变化的部分”封装起来，变化的部分就是 状态。利用面向对象的思想，将每个状态封装为一个具体的类，将状态各自的行为放到自己的类中，那么每个状态只要实现自己的规则即可。 然后将贩卖机委托给当前的状态对象，当调用某个方法时，再调用状态对象的方法。 首先，定义一个State接口。接口里有贩卖机每个动作的方法 然后每个状态创建一个具体的状态类，状态类实现State接口，并实现在这个状态下贩卖机的行为 最后将贩卖机的动作委托到状态类 封装的状态类图 我们已经知道每个状态下什么行为应该做什么，上面代码已经实现过了，现在只是把它们分散在各自类中。 定义状态接口 1234567891011121314151617181920212223242526272829303132333435363738 /** * 状态接口 * * @author ddmcc */public interface State { /** * 选择饮料 */ void chooseDrink(); /** * 投币操作 */ void insertCoin(); /** * 退币操作 */ void ejectCoin(); /** * 确认购买饮料操作 */ void returnDrink(); /** * 退出饮料 */ void dispense();} 实现待售状态类 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 待售状态 * * @author ddmcc */public class NoCoinState implements State { /** * 贩卖机实例变量，通过它控制贩卖机的状态，饮料数量 */ private DrinkMachine drinkMachine; public NoCoinState(DrinkMachine drinkMachine) { this.drinkMachine = drinkMachine; } @Override public void chooseDrink() { // 待售状态下 进行选择饮料操作 drinkMachine.setCurrentState(drinkMachine.getChooseDrinkState()); System.out.println(&quot;饮料选择成功！请投币&quot;); } @Override public void insertCoin() { System.out.println(&quot;请先选择饮料！&quot;); } @Override public void ejectCoin() { System.out.println(&quot;请先投币！&quot;); } @Override public void returnDrink() { System.out.println(&quot;请先选择饮料！&quot;); } @Override public void dispense() { System.out.println(&quot;请先选择饮料！&quot;); }} 新的贩卖机 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384/** * @author ddmcc */@Datapublic class DrinkMachine { /** * 状态常量都由状态对象代替了 */ private State noCoinState; private State hasCoinState; private State soldOutState; private State soldState; private State chooseDrinkState; private int count = 0; /** * 记录当前状态 */ private State currentState; public DrinkMachine(int count) { noCoinState = new NoCoinState(this); hasCoinState = new HasCoinState(this); soldOutState = new SoldOutState(this); soldState = new SoldState(this); chooseDrinkState = new ChooseDrinkState(this); if (count &gt; 0) { currentState = noCoinState; } else { currentState = soldOutState; } } /** * 选择饮料 */ public void chooseDrink() { // 动作都委托给当前的状态对象了 currentState.chooseDrink(); } /** * 投币操作 */ public void insertCoin() { currentState.insertCoin(); } /** * 退币操作 */ public void ejectCoin() { currentState.ejectCoin(); } /** * 退出饮料操作 */ public void returnDrink() { currentState.returnDrink(); } /** * 退出饮料 */ public void dispense() { currentState.dispense(); }} 上面只实现了一个状态的伪代码，其它状态类也差不多，也就是把一开始的代码“局部化”。上面版本中针对第一版本进行了以下优化： 将每个状态的行为封装进各自的类中 删除if语句，方便日后维护 让每一个状态“对修改关闭”，让贩卖机对“扩展开放”，因为可以加入新的状态，而我们也把所有的状态都放在贩卖机中了。对修改关闭怎么理解呢？其实之前已经说过了，修改关闭不是说不让修改，而是修改不应该对其它带来影响。而为什么要把所有状态放到贩卖机呢？首先贩卖机总是在这些状态中游走，其次可以降低状态类间的依赖。（如果把状态变化放到贩卖机中，则可以让状态类之间完全没有依赖，而我们选择了在状态类中转换状态，贩卖机提供getter方法，把状态之间的的依赖降到最低） 定义 允许对象在内部状态改变时改变它的行为，对象看起来好像改变了它的类。 允许对象在内部状态改变时改变它的行为 因为我们将状态封装成不同的类，并将动作委托到当前状态中，当状态改变时，行为也就变了 对象看起来好像改变了它的类 对于客户端而言，并不知道内部如何实现，看起来就好像一个新的实例。其实是通过引用不同状态来造成的假象 类图 状态模式的优缺点 优点 将不同行为“局部化”，符合开-闭原则 通过组合，委托的方式动态的改变行为 缺点 导致设计中的类数目大大增加 状态模式与策略模式 状态模式 我们将一群行为封装在状态对象中，Context可以随时委托到那些对象中的一个。随着时间流逝，当前状态在状态集合中游走，因此Context的行为也会跟着改变。但是对于Context的客户来说这是浑然不觉的。 状态模式可以看成在Context中不用放很多判断条件，将不同的行为封装到不同的状态中，通过改变当前状态来改变行为 在固定的状态集合中游走，用户浑然不觉 策略模式 客户通常主动的指定要组合的策略对象是哪一个，摆脱继承的束缚，通过组合的方式动态的改变策略。 客户指定组合策略，比继承的更有弹性替代方案","link":"/2019/10/31/2019-10-31-state-pattern/"},{"title":"事务的隔离级别","text":"在定义中有四种隔离级别，每一种级别都规定了一个事务中所做的修改，哪些在事务内和事务间是可见的，哪些是不可见的。通常来说， 一个事务在最终提交前，对其它事务是不可见的。较低级别的隔离通常可以执行更高的并发，系统的开销也更低。 四种隔离级别读未提交 READ UNCOMMITTED在此级别中，即使事务中的修改没有修改，对其它事务也是可见的，事务可以读取未提交的数据，这就叫“脏读”。如果事务读取到其它未提交事务的数据，而刚好该事务又回滚了，那么读取到的就是脏数据了 读提交 READ COMMITTED读提交，顾名思义一个事务开始时，只能看见已经提交的事务所做的修改。大多数数据库默认的隔离级别就是它。这个级别有时候也叫 不可重复读，因为一个事务内，执行两次相同的查询，可能会得到不一样的结果。因为在两次查询中间可能有其它事务修改了其中的数据。 可重复读 REPEATABLE READ该级别保证在同一事务内多次查询结果的一致的。但还是无法解决另外一个幻读的问题，指两次查询中间有其它事务插入数据，会产生 幻行。InnoDB和XtraDB存储引擎通过MVCC（多版本并发控制）解决了幻读的问题。 Mysql的默认隔离级别 可串行化 SERIALIZABLE强制事务串行化执行，避免了幻读的问题。它会在读取的每一行数据上加锁，所以可能造成大量超时和锁争用问题。 隔离级别表| 隔离级别 | 脏读 | 不可重复读 | 幻读 | 加锁读 || —- | —- || READ UNCOMMITTED | Yes | Yes | Yes | No || READ COMMITTED | No | Yes | Yes | No || REPEATABLE READ | No | No | Yes | No || SERIALIZABLE | No | No | No | Yes | MVCC 多版本并发控制大多数的数据库事务存储引擎为了提升并发性能，都会实现各自的多版本并发控制。MVCC可以看成是行级锁的表中，但在大多数情况下避免了加锁的操作，因此性能开销更低。MVCC基本都实现了非阻塞的读操作和只锁定目标行的写操作。 MVCC的实现是通过某个时间点的快照来实现的。也就是说，根据事务开始的时间点不同，不同的事务在同一张表，同一时刻读取的的数据有可能是不用的。不同的MVCC有自己的不同实现，典型的有乐观并发控制和悲观并发控制。 InnoDB的MVCC是通过在每一行记录后增加两个隐藏的列来实现的。一列保存行创建的事务版本号，一个保存行删除的事务版本号，每开始一个事务，事务版本号都会自动递增。在事务开始时刻的版本号会作为此次事务的版本号，用来和查询到的每行记录的版本号作比较。 下面为在Mysql默认隔离级别REPEATABLE READ下的MVVC是如何控制的： SELECT InnoDB只查找版本早于当前事务的数据行（版本号小于或等于事务的版本号），这样可以保证查询到的数据要么是之前已经存在，要么是当前事务自己插入或修改过的的 行的删除版本号要么未定义，要么版本号大于当前版本号 只有符合以上两个条件的记录，才会作为结果返回。 INSERT InnoDB为新插入的每一行保存当前的版本号为行创建版本号 DELETE InnoDB为删除的每一行保存当前的版本号为行删除版本号 UPDATE InnoDB为插入一行新纪录，保存当前的版本号为行的创建版本号，同时保存当前版本号到原来的行的删除版本号 保存这两个版本号使得大部分时候都可以不用加锁。这样设计性能很好，并且保证只会读取到符合结果的行。不足时会额外增加存储空间和做一些额外的维护工作。 MVCC只在READ COMMITTED和REPEATABLE READ两个隔离级别下工作，因为READ UNCOMMITTED总是读取最新的数据行，而不是符合当前事务版本的，SERIALIZABLE则会对读取的行加锁。","link":"/2019/11/18/2019-11-18-isolation-level-of-transaction/"},{"title":"debug调试Mysql源码","text":"再上一篇的事务隔离级别中说了mysql是通过两个隐藏列来实现的，后来又看了一些相关文章，发现好像和《高性能Mysql》说的有点不一样。mysql的mvcc是通过Read View和Undo log来实现的，read view来判断数据行是否可读，undo log用来找到最近的可见版本。为了搞清楚内部是如何实现的（虽然不一定看的懂），所以想debug看下read view是如何生成的。 需要下载的软件 Cmake：3.4.0 Bison：2.4.1 Perl: 5.26.3 Boost 1.59.1：下载后解压到任意目录 VS2013 mysql5.7.12源码 用cmake构建项目打开cmake-gui，选择源码，指定构建目录等 好了之后点击 Configure ，会弹窗让选择VS版本，选择对应版本 完成后会在输出框中看见 Configure Done，然后再点击 Generate，后会看见Generating Done 打开VS CODE生成方案打开build目录，双击根目录的 ALL_BUILD.vcxproj ，就会在VS2013中打开，之后将/sql/mysqld.cc中的test_lc_time_sz()函数，将其中的DBUG_ASSERT(0)改为DBUG_ASSERT(1)后按 F7 进行生成方案。 开始调试在启动之前先准备 my.ini 配置文件，放在C:/Windows或构建目录下，后进入构建目录下的/sql/Debug 打开cmd，输入mysqld --initialize初始化数据库后 输入 mysqld --skip-grant-tables 新打开一个对话框，输入mysql;，use mysql; UPDATE user SET authentication_string=PASSWORD(“123456”) WHERE User=”root”; flush privileges; 注：启动mysql调试也可以在VS2013中进行，但是会提示下载字符。。。 然后就可以到/client/Debug 打开cmd，输入 mysql -u root -p 输入密码123456连接 启动并连接成功后，打开VS2013点击调试，附加到进程，找到mysql进程附加进去。 然后就可以debug了！","link":"/2019/11/24/2019-11-24-debug-mysql-source/"},{"title":"深入理解jvm","text":"上一篇已经介绍了 jvm运行时数据区 ，下面将学习垃圾收集器和内存分配与回收策略 为什么要了解gc和内存分配呢？ 当需要排查各种内存溢出、内存泄漏问题时 当垃圾收集成为系统达到更高并发量的瓶颈时 灵魂拷问 哪些内存需要回收？ 什么时候回收？ 怎么回收？ 哪些内存需要回收 在运行区域中，程序计数器，虚拟机栈，本地方法栈三个区域随线程而生，随线程而灭，栈中的栈帧随着方法的进入和退出进行入栈出栈操作，每一个栈帧 在类结构确定下来就已确定了大小 因此这几个区域的分配和回收是具有确定性的。不考虑回收的问题 一个栈帧就是一个方法，方法内创建的对象只是引用变量在栈中并且大小基本是固定的。当类结构确定时，需要创建多少变量以及方法调用都已经确定了 Java堆和方法区则不一样，一个接口中的多个实现类需要的内存可能不一样，一个方法中的多个分支需要的内存也可能不一样，我们只有在程序处于运行期间才能知道会创建哪些对象，这部分内存的分配和回收都是动态的，垃圾回收器所以关注的也是这部分内存。 12A a = new A();B b = new B(); 如有接口A，有两个实现类B和C，B中有变量id，name，C中只有id，那么B，C对象所需的内存是不确定的。b，c在栈变量表大小是固定的，而new A(),new B(),引用类型在堆中不固定。 单个对象的内存都不固定，更别说方法和方法分支了 什么时候回收 对象回收肯定是回收“无用”的对象，还需要访问的对象总不能被回收了，那么如何判断一个对象是否无用了呢？ 引用计数法 很多教科书判断对象是否存活的算法是这样的：给对象添加一个引用计数其器，每当有一个地方引用它，计数器值就加1；当引用失效，计数器值就减1；任何时刻计数器都为0的对象就是不可能再被使用的。 可达性分析（根搜索算法） 这个算法的基本思路就是通过名为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为“引用链”，当一个对象到GC Root没有任何引用链相连（就是GC Roots到这个对象不可达）时，则这个对象是不可用的。 可作为GC Roots对象包括： - 虚拟机栈中的引用的对象（栈帧中的局部变量表）即**局部变量** - 法区中的类静态属性引用的对象即**静态变量** - 方法区中的常量引用的对象即**常量** - 本地方法栈中JNI的引用的对象（本地方法中引用对象） 什么是引用？ 在JDK1.2之前，JDK定义的引用是： 如果一个引用类型的数据存储的是另一块内存的地址就成这块内存代表着一个引用 在JDK1.2之后，将引用分为强引用、软引用、弱引用、虚引用 强引用 类似Object obj = new Object()这种, 软引用 软引用描述一些还有用，但不是必须的对象，当系统将要发生内存溢出时，将会把这些对象列为回收对象并进行回收，如果回收完这些对象内存还是不足，才会抛出异常。用SoftReference类来实现软引用 弱引用 弱引用也是用来描述非必需的对象，强度比软引用弱，被弱引用关联的对象只能生存到下一次垃圾回收之前，无论内存空间是否足够，都会被回收用WeakReference来实现 虚引用 是最弱的一种引用关系，无法通过虚引用来获取对象的实例，唯一的目的是在这个对象回收之前接到系统通知。用PhantomReference来实现 判断对象死亡的过程 对象真正的死亡要经历两次的标记过程，如果发现没有GC Roots相连接的引用链，那么会经历第一次筛选，筛选的条件是是否要执行finalize方法当没有重写finalize或方法已经执行过了，将视为没有必要，否则会将对象放置于一个队列之中，稍后会由一条虚拟机自动建立、低优先级的线程去执行。（需要注意的是虚拟机只会去触发这个方法，并不会一定等待方法运行结束）。finalize方法还是对象逃脱死亡的最后一次机会，在finalize将对象重新与RC Roots建立关联。然后在稍后GC对队列对象进行二次小规模标记时，将会被移除即将回收的对象列表。 对象回收的算法 标记-清除 先标记要回收的垃圾，在标记完成后统一回收对象 缺点： ①效率问题 ②会产生垃圾碎片，会导致后续提早触发垃圾回收动作。 复制算法 将内存按一定比例的大小分成两块，每次只用其中一块，当用完后，就将存活的对象复制到另一块。这种算法避免了垃圾碎片的问题 缺点： ①需要浪费一定大小的内存 标记-整理 分代收集算法 垃圾回收器","link":"/2019/12/30/2019-12-30-understanding-jvm-2/"},{"title":"设计模式之适配器模式","text":"定义将一个类的接口转换成使用者所期望的另一个接口。适配器让原本接口不兼容的类可以合作无间。 上面这句话解释就是使用者要求的接口与提供的接口不一致，所以在它们在中间加了一个适配器类，适配器的接口是符合要求的。在适配器接口中调用所提供的接口，并解决两者之间的差异 类适配器与对象适配器适配器模式分为对象适配器和类适配器，下面是类图 对象适配器 类适配器 适配器模式主要分为四个角色 Client：接口调用者 Target：目标接口（调用者需要的接口） Adapter：适配器用于包装转换被适配者 Adaptee：被适配者（实际提供的接口） 解释一下类图是这样的： 客户端：我要调用operation方法 Adaptee：没有，只有method方法 客户端：我就要operation方法！ Adaptee：没有，快滚！ 然后它们就打起来了…. 这时和事老适配器兄出现了，将它们拉开。 适配器实现了目标接口的operation方法，当收到请求时，再委托或转接给被适配者 两者区别它们两的区别就是对象适配器使用了组合的方式，而类适配器使用了继承的方式（因为不支持多继承，所以实现Target）。这两种实现的所产生的差异就是使用组合的对象适配器更有弹性，它不仅适配了该适配者，适配者的子类也适配了。而类适配者的好处是因为使用的是继承，所以可以不需要重新实现整个被适配者（这边的意思是如果有不需要适配的接口，即接口方法名，参数什么的都和Target里的一致，则不需要重写，如下代码）。当然也可以重写父类的方法。 123456789101112131415161718192021public class Main { public static void main(String[] args) { Adapter adapter = new Adapter(); adapter.operation(); } interface Target { void operation(); } static class Adaptee { public void operation(){ System.out.println(111); } } static class Adapter extends Adaptee implements Target { }} 来个栗子现在大部分手机产商都不送耳机了，而且也取消了3.5mm的耳机孔，改成充电器孔和耳机孔一起。。假如你买了一个新手机是无耳机孔，但是你的耳机是3.5mm接口的。这时就出现了手机与耳机无法适配的情况。 这时候就需要用转接器了！而适配器就相当于做了转接器的工作。类图如下代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344public interface Phone { void earPhonePlug();}public class Samsung implements Phone { @Override public void earPhonePlug() { System.out.println(&quot;三星手机Type-C接口&quot;); }}public interface EarPhoneAdapter { void earPhonePlugThreeMM(); void earPhonePlugTypeC();}public class SamsungAdapter implements EarPhoneAdapter { private Phone phone; public SamsungAdapter(Phone phone) { this.phone = phone; } @Override public void earPhonePlugThreeMM() { System.out.println(&quot;3.5mm接口&quot;); earPhonePlugTypeC(); } @Override public void earPhonePlugTypeC() { System.out.println(&quot;Type-C接口&quot;); phone.earPhonePlug(); }} 运行，即耳机插入适配器 123456789public static void main(String[] args) { Phone phone = new Samsung(); EarPhoneAdapter earPhoneAdapter = new SamsungAdapter(phone); earPhoneAdapter.earPhonePlugThreeMM(); // 输出 3.5mm接口 Type-C接口 三星手机Type-C接口} 如果有个华为的手机，也是Type-C接口的，这时并不需要修改适配器代码，就能够直接复用。这就是组合的威力 12345678910111213public class HuaWei implements Phone { @Override public void earPhonePlug() { System.out.println(&quot;华为Type-C接口&quot;); }}public static void main(String[] args) { Phone phone = new HuaWei(); EarPhoneAdapter earPhoneAdapter = new SamsungAdapter(phone); earPhoneAdapter.earPhonePlugThreeMM();} 如果用类适配器的实现方式的话，代码如下： 主要是适配器的变化，由组合的方式变成继承 1234567891011121314public class SamsungAdapter extends Samsung implements EarPhoneAdapter { @Override public void earPhonePlugThreeMM() { System.out.println(&quot;3.5mm接口&quot;); earPhonePlugTypeC(); } @Override public void earPhonePlugTypeC() { System.out.println(&quot;Type-C接口&quot;); super.earPhonePlug(); }} 运行 12345678public static void main(String[] args) { EarPhoneAdapter earPhoneAdapter = new SamsungAdapter(); earPhoneAdapter.earPhonePlugThreeMM(); // 输出 3.5mm接口 Type-C接口 三星手机Type-C接口} 这样你会发现这个三星的耳机适配器和三星手机已经绑死了，因为继承了三星手机。如果我这时想给华为手机用，就用不了了。就不得不再买一个华为的适配器。 实际一点的栗子早期HashTable的遍历是用Enumeration接口的，后来出了迭代器Iterator，现在想让HashTable也用上迭代器怎么做呢？？ Enumeration 接口： boolean hasMoreElements(); E nextElement(); Iterator 接口： boolean hasNext(); E next(); default void remove() { throw new UnsupportedOperationException(“remove”); } 在HashTable有一个Enumerator 类实现了Enumeration和Iterator接口，并对两个接口进行了实现。 实际上是只实现了Enumeration接口(也实现了remove方法)，Iterator的接口则直接调用对应的方法！ 1234567891011121314151617181920212223242526272829303132private class Enumerator&lt;T&gt; implements Enumeration&lt;T&gt;, Iterator&lt;T&gt; { /** * Indicates whether this Enumerator is serving as an Iterator * or an Enumeration. (true -&gt; Iterator). */ boolean iterator; Enumerator(int type, boolean iterator) { this.type = type; this.iterator = iterator; } public boolean hasMoreElements() { Entry&lt;?,?&gt; e = entry; int i = index; Entry&lt;?,?&gt;[] t = table; /* Use locals for faster loop iteration */ while (e == null &amp;&amp; i &gt; 0) { e = t[--i]; } entry = e; index = i; return e != null; } // Iterator methods public boolean hasNext() { return hasMoreElements(); } // 省略了其它方法... } Enumerator就相当于一个适配器，能够适配两种方式的遍历！通过判断属性iterator来判断是哪种迭代器。iterator属性由构造函数传入，下面代码为新建Enumerator 123456789101112131415private &lt;T&gt; Enumeration&lt;T&gt; getEnumeration(int type) { if (count == 0) { return Collections.emptyEnumeration(); } else { return new Enumerator&lt;&gt;(type, false); }}private &lt;T&gt; Iterator&lt;T&gt; getIterator(int type) { if (count == 0) { return Collections.emptyIterator(); } else { return new Enumerator&lt;&gt;(type, true); }} 适配器模式与装饰者模式装饰者模式 装饰者模式是利用组合的方式，动态的对对象增加新的行为或者增强，而无需更改现有的代码。 适配器模式 主要是对接口的转换，将旧的接口转换成新的符合客户要求的接口。 适配器模式主要是为了接口的转换，而装饰者模式是通过组合来动态的为被装饰者注入新的功能或修改行为。 总结 适配器模式用于将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作。适配器模式有类适配器和对象适配器。 适配器模式包含四个角色 目标抽象类（Target）即客户要用的接口 适配器类（Adapter）实现了目标类的接口，并拥有被适配者的抽象类型引用。作为一个转换器，对被适配者和抽象目标类进行适配 Adaptee是被适配的角色，它定义了一个已经存在的接口，这个接口需要适配 在客户类（Client）中针对目标抽象类进行编程，调用在目标抽象类中定义的业务方法 在类适配器模式中，适配器类实现了目标抽象类接口并继承了适配者类，并在目标抽象类的实现方法中调用所继承的适配者类的方法；在对象适配器模式中，适配器类实现了目标抽象类并定义了一个被适配者类的对象实例，在所实现的目标抽象类方法中调用被适配者类的相应业务方法。 适配器模式适用于系统需要使用现有的类，而这些类的接口不符合系统的需要，通常用于对原有的系统进行升级改造的时候。 优点和缺点优点 有更好的复用性。系统需要使用现有的类，但此类接口不符合系统需要，通过适配器模式让这些功能得到很好的复用 同时因为使用了组合的方式，所有被适配对象实例的子类都可以被适配，使系统更有弹性。 缺点 过多使用适配器会使得系统非常凌乱，明明调用的是A接口，内部却被适配成了B接口","link":"/2019/08/08/2019-08-08-adapter-pattern/"},{"title":"mybatis3源码学习","text":"核心的类MyBatis有几个核心的类如：SqlSessionFactoryBuilder，SqlSessionFactory，SqlSession，Configuration 等 使用 MyBatis 的主要 Java 接口就是 SqlSession。可以通过这个接口来执行命令，获取Mapper接口和管理事务。SqlSessions 是由 SqlSessionFactory 实例创建的。SqlSessionFactory 对象包含创建 SqlSession 实例的各种方法。而 SqlSessionFactory 本身是由 SqlSessionFactoryBuilder 创建的，它可以从 XML、注解或 Java 配置代码来创建 SqlSessionFactory。 Configuration在构建 SqlSessionFactory 之前先要配置 Configuration 实例，Mybatis所有的配置都在这个类里面，在运行时可以通过 SqlSessionFactory#getConfiguration() 来获得并检查配置。 以下删除了很多配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class Configuration { // 数据源，事务工厂 protected Environment environment; protected boolean safeRowBoundsEnabled; protected boolean safeResultHandlerEnabled = true; // 是否开启驼峰命名自动映射，即从经典数据库列名 A_COLUMN 映射到经典 Java 属性名 aColumn。 protected boolean mapUnderscoreToCamelCase; // 允许使用方法签名中的名称作为语句参数名称,必须采用 Java 8 编译，并且加上 -parameters 选项。（新增于 3.4.1） // 如果开启 在多个参数的接口 可以不用@Param注解或是arg0,param1等，直接用接口的参数名就能替换 // 如接口 listBooks(String title, String author) 在sql中直接使用 #{title} #{author} protected boolean useActualParamName = true; protected boolean returnInstanceForEmptyRow; protected String logPrefix; protected Class&lt;? extends Log&gt; logImpl; protected Class&lt;? extends VFS&gt; vfsImpl; // 本地缓存范围，默认SqlSession 级别,会缓存会话中执行的所有查询 protected LocalCacheScope localCacheScope = LocalCacheScope.SESSION; // properties 节点 protected Properties variables = new Properties(); protected ReflectorFactory reflectorFactory = new DefaultReflectorFactory(); protected ObjectFactory objectFactory = new DefaultObjectFactory(); protected ProxyFactory proxyFactory = new JavassistProxyFactory(); // #224 Using internal Javassist instead of OGNL protected String databaseId; /** * Configuration factory class. * Used to create Configuration for loading deserialized unread properties. * * @see &lt;a href='https://code.google.com/p/mybatis/issues/detail?id=300'&gt;Issue 300 (google code)&lt;/a&gt; */ protected Class&lt;?&gt; configurationFactory; // mapper注册器，存mapper接口的地方 protected final MapperRegistry mapperRegistry = new MapperRegistry(this); // 拦截器链 protected final InterceptorChain interceptorChain = new InterceptorChain(); // TypeHandler注册器，存TypeHandler接口的地方 protected final TypeHandlerRegistry typeHandlerRegistry = new TypeHandlerRegistry(this); // 存放类别名与类映射 protected final TypeAliasRegistry typeAliasRegistry = new TypeAliasRegistry(); protected final LanguageDriverRegistry languageRegistry = new LanguageDriverRegistry(); // mappedStatements protected final Map&lt;String, MappedStatement&gt; mappedStatements = new StrictMap&lt;MappedStatement&gt;(&quot;Mapped Statements collection&quot;) .conflictMessageProducer((savedValue, targetValue) -&gt; &quot;. please check &quot; + savedValue.getResource() + &quot; and &quot; + targetValue.getResource()); // 以下字如其名 protected final Map&lt;String, Cache&gt; caches = new StrictMap&lt;&gt;(&quot;Caches collection&quot;); protected final Map&lt;String, ResultMap&gt; resultMaps = new StrictMap&lt;&gt;(&quot;Result Maps collection&quot;); protected final Map&lt;String, ParameterMap&gt; parameterMaps = new StrictMap&lt;&gt;(&quot;Parameter Maps collection&quot;); protected final Map&lt;String, KeyGenerator&gt; keyGenerators = new StrictMap&lt;&gt;(&quot;Key Generators collection&quot;); // 已加载的文件 protected final Set&lt;String&gt; loadedResources = new HashSet&lt;&gt;(); protected final Map&lt;String, XNode&gt; sqlFragments = new StrictMap&lt;&gt;(&quot;XML fragments parsed from previous mappers&quot;);} SqlSessionFactory1，XML中构建 SqlSessionFactory下面是Mybatis配置文件（这里只是Mybatis的配置，没有与Spring结合） 123456789101112131415161718192021222324252627&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE configuration PUBLIC &quot;-//mybatis.org//DTD Config 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-config.dtd&quot;&gt;&lt;configuration&gt; &lt;environments default=&quot;development&quot;&gt; &lt;!-- environment 节点包含事务管理器和数据源的配置 --&gt; &lt;environment id=&quot;development&quot;&gt; &lt;transactionManager type=&quot;JDBC&quot;&gt; &lt;property name=&quot;&quot; value=&quot;&quot; /&gt; &lt;/transactionManager&gt; &lt;dataSource type=&quot;UNPOOLED&quot;&gt; &lt;property name=&quot;driver&quot; value=&quot;com.mysql.cj.jdbc.Driver&quot; /&gt; &lt;property name=&quot;url&quot; value=&quot;&quot; /&gt; &lt;property name=&quot;username&quot; value=&quot;&quot; /&gt; &lt;property name=&quot;password&quot; value=&quot;&quot;/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;!-- mappers --&gt; &lt;mappers&gt; &lt;mapper class=&quot;com.ddmcc.UserMapper&quot; /&gt; &lt;/mappers&gt;&lt;/configuration&gt; 1234567public static void main(String[] args) throws Exception{ // 构建SqlSessionFactory对象 SqlSessionFactory sqlSessionFactory; try (Reader reader = Resources.getResourceAsReader(&quot;com/ddmcc/mybatis-config.xml&quot;)) { sqlSessionFactory = new SqlSessionFactoryBuilder().build(reader); } } 读取配置文件创建流，利用SqlSessionFactoryBuilder对象将流构建成SqlSessionFactory对象。SqlSessionFactory其实是一个接口，调用build方法返回的是DefaultSqlSessionFactory对象，它是SqlSessionFactory的默认实现。在DefaultSqlSessionFactory中，有一个Configuration对象，基本上我们的所有配置属性，mapper接口等都会保存在这个对象中 build()1234567891011121314151617181920public SqlSessionFactory build(Reader reader, String environment, Properties properties) { try { // 构建解析对象 XMLConfigBuilder parser = new XMLConfigBuilder(reader, environment, properties); // 解析XML，parser.parse()返回的是Configuration对象 // 再调用build(Configuration对象) 返回一个DefaultSqlSessionFactory return build(parser.parse()); } catch (Exception e) { throw ExceptionFactory.wrapException(&quot;Error building SqlSession.&quot;, e); } finally { ErrorContext.instance().reset(); try { reader.close(); } catch (IOException e) { // Intentionally ignore. Prefer previous error. } }} 获得配置文件configuration节点下的所有的子节点，逐一解析，赋值到Configuration对象中对应的属性。 可以看到下面解析的方法都是XMLConfigBuilder类内部的方法，所有configuration的属性都是在各个方法里设置的，因为在XMLConfigBuilder中，有一个configuration属性。（parser.parse()方法会判断是否已经解析过了，如果执行不止一次parse()方法那么两次解析出来的配置就会窜在一起了？？？） parse()123456789101112131415161718192021222324252627282930313233343536public Configuration parse() { // 已解析 if (parsed) { throw new BuilderException(&quot;Each XMLConfigBuilder can only be used once.&quot;); } parsed = true; // 获取configuration节点下所有子节点 parseConfiguration(parser.evalNode(&quot;/configuration&quot;)); return configuration;}private void parseConfiguration(XNode root) { try { // properties // issue #117 read properties first propertiesElement(root.evalNode(&quot;properties&quot;)); Properties settings = settingsAsProperties(root.evalNode(&quot;settings&quot;)); loadCustomVfs(settings); loadCustomLogImpl(settings); typeAliasesElement(root.evalNode(&quot;typeAliases&quot;)); pluginElement(root.evalNode(&quot;plugins&quot;)); objectFactoryElement(root.evalNode(&quot;objectFactory&quot;)); objectWrapperFactoryElement(root.evalNode(&quot;objectWrapperFactory&quot;)); reflectorFactoryElement(root.evalNode(&quot;reflectorFactory&quot;)); settingsElement(settings); // read it after objectFactory and objectWrapperFactory issue #631 // 数据源，事务管理器 environmentsElement(root.evalNode(&quot;environments&quot;)); databaseIdProviderElement(root.evalNode(&quot;databaseIdProvider&quot;)); typeHandlerElement(root.evalNode(&quot;typeHandlers&quot;)); // 解析mapper mapperElement(root.evalNode(&quot;mappers&quot;)); } catch (Exception e) { throw new BuilderException(&quot;Error parsing SQL Mapper Configuration. Cause: &quot; + e, e); }} 2，Java代码构建SqlSessionFactory下面是直接拷贝的官方文档中Java代码构建SqlSessionFactory的示例 12345678910111213141516DataSource dataSource = BaseDataTest.createBlogDataSource();TransactionFactory transactionFactory = new JdbcTransactionFactory();Environment environment = new Environment(&quot;development&quot;, transactionFactory, dataSource);Configuration configuration = new Configuration(environment);configuration.setLazyLoadingEnabled(true);configuration.setEnhancementEnabled(true);configuration.getTypeAliasRegistry().registerAlias(Blog.class);configuration.getTypeAliasRegistry().registerAlias(Post.class);configuration.getTypeAliasRegistry().registerAlias(Author.class);configuration.addMapper(BoundBlogMapper.class);configuration.addMapper(BoundAuthorMapper.class);SqlSessionFactoryBuilder builder = new SqlSessionFactoryBuilder();SqlSessionFactory factory = builder.build(configuration); 对象的作用域 SqlSessionFactoryBuilder (局部方法) 一旦创建了 SqlSessionFactory，就不再需要它了。 因此 SqlSessionFactoryBuilder 实例的最佳作用域是方法作用域（也就是局部方法变量）。 可以重用 SqlSessionFactoryBuilder 来创建多个 SqlSessionFactory 实例，但最好还是不要一直保留着它，以便释放所有的 XML 资源文件 SqlSessionFactory (全局，整个应用) SqlSessionFactory 一旦被创建就应该在应用的运行期间一直存在,因此 SqlSessionFactory 的最佳作用域是应用作用域。 有很多方法可以做到，最简单的就是使用单例模式或者静态单例模式。 SqlSession (一次请求，局部方法或者说一个线程) 每个线程都应该有它自己的 SqlSession 实例。SqlSession 的实例不是线程安全的，因此是不能被共享的，所以它的最佳的作用域是请求或方法作用域。 绝对不能将 SqlSession 实例的引用放在一个类的静态域，甚至一个类的实例变量也不行。 也绝不能将 SqlSession 实例的引用放在任何类型的托管作用域中，比如 Servlet 框架中的 HttpSession。每次收到 HTTP 请求，就可以打开一个 SqlSession，返回一个响应后，就关闭它。 这个关闭操作很重要，为了确保每次都能执行关闭操作，你应该把这个关闭操作放到 finally 块中。 下面的示例就是一个确保 SqlSession 关闭的标准模式： try (SqlSession session = sqlSessionFactory.openSession()) { // 你的应用逻辑代码 } 解析mappers解析 mapper 的入口方法是 mapperElement ，取得 mappers 节点下的子节点循环添加。子节点有 package 和 mapper ，所以解析的入口也不同。一个先获取包下的接口，循环接口去寻找xml，解析，映射，一个通过namespace找接口 加载 package添加 mapper 接口的操作都是在 configuration 对象的 mapperRegistry 对象属性里进行的，解析出来的接口也都是以 key,value 形式存在一个HashMap中 会先判断扫出该包下所有超类为 Object.class 的class，存入Set列表中 Set列表过滤出所有接口 以 key 为该接口类型， value 为该接口的 MapperProxyFactory 代理工厂 TODO 加载解析该接口的xml文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public &lt;T&gt; void addMapper(Class&lt;T&gt; type) { // 是否接口 if (type.isInterface()) { // 接口已注册 if (hasMapper(type)) { throw new BindingException(&quot;Type &quot; + type + &quot; is already known to the MapperRegistry.&quot;); } boolean loadCompleted = false; try { // 以 key 为该接口类型， value 为该接口的 `MapperProxyFactory` 代理工厂 knownMappers.put(type, new MapperProxyFactory&lt;&gt;(type)); // It's important that the type is added before the parser is run // otherwise the binding may automatically be attempted by the // mapper parser. If the type is already known, it won't try. MapperAnnotationBuilder parser = new MapperAnnotationBuilder(config, type); // 解析整个xml文件, parser.parse(); loadCompleted = true; } finally { if (!loadCompleted) { knownMappers.remove(type); } } }} public void parse() { String resource = type.toString(); // 改接口的xml文件是否已经加载 if (!configuration.isResourceLoaded(resource)) { // 搜索解析xml文件 loadXmlResource(); configuration.addLoadedResource(resource); assistant.setCurrentNamespace(type.getName()); // CacheNamespace注解 parseCache(); // CacheNamespaceRef parseCacheRef(); // mapper接口的方法 Method[] methods = type.getMethods(); for (Method method : methods) { try { // issue #237 if (!method.isBridge()) { // 解析语句成MappedStatement 对象，存到configuration中id为namespace + . + 方法名 parseStatement(method); } } catch (IncompleteElementException e) { configuration.addIncompleteMethod(new MethodResolver(this, method)); } } } // 解析失败的方法重试一遍 parsePendingMethods();}private void loadXmlResource() { // Spring may not know the real resource name so we check a flag // to prevent loading again a resource twice // this flag is set at XMLMapperBuilder#bindMapperForNamespace if (!configuration.isResourceLoaded(&quot;namespace:&quot; + type.getName())) { String xmlResource = type.getName().replace('.', '/') + &quot;.xml&quot;; // #1347 InputStream inputStream = type.getResourceAsStream(&quot;/&quot; + xmlResource); if (inputStream == null) { // Search XML mapper that is not in the module but in the classpath. try { // classpath找一遍 inputStream = Resources.getResourceAsStream(type.getClassLoader(), xmlResource); } catch (IOException e2) { // ignore, resource is not required } } if (inputStream != null) { XMLMapperBuilder xmlParser = new XMLMapperBuilder(inputStream, assistant.getConfiguration(), xmlResource, configuration.getSqlFragments(), type.getName()); // 解析xml sql xmlParser.parse(); } }} 其它SQL注入 仅在字符串中发生 仅在不使用准备好的语句时发生 仅在用户输入时发生 ${}与#{} ${} 可以看成是动态sql，在Mybatis中，会在获得Statement执行对象之前对sql进行映射替换 如 SELECT * FROM USER WHERE USER_NAME = ${name} ${name}= “111 OR 1 = 1” 那么在Mybatis解析完sql后，就会变成 SELECT * FROM USER WHERE USER_NAME = 111 OR 1 = 1 #{} 是静态sql，Mybatis在解析sql的时候会将 #{name} 替换成 占位符 ? 然后用sql生成 PreparedStatement 对象 pstm 再对对象设查询参数 pstm.setStringParameter(“111 OR 1 = 1”); 这是真正执行的sql 就会变成 SELECT * FROM USER WHERE USER_NAME = ‘111 OR 1 = 1’ Mybatis的参数映射问题args0,args1…param1,param2… @Param注册 JDK1.8后不使用Param注解，直接用参数名映射","link":"/2020/03/24/2020-03-24-mybatis-3-source-learning/"},{"title":"React组件生命周期","text":"React组件的一生主要分为四个阶段分别为初始化（Initialization），挂载（Mounting），更新（Updation），卸载（Unmounting）， 下面介绍挂载和更新的一些方法。 16.3之前 挂载阶段 挂载是指组件渲染并构造dom元素，然后将元素插入到页面的过程， 这是一个从无到有的过程 。在这个过程中主要有以下几个方法： componentWillMount 它会在组件渲染之前执行，在此方法之后，组件将渲染挂载，在组件的整个生命周期中，这个方法只会被执行一次，并且是服务端渲染唯一会调用的生命周期函数。所以可以在此方法中做一些初始化的工作。 render 将组件渲染到页面中。它是一个纯方法，输出由输入决定 componentDidMount 此方法在生命周期中也只会执行一次，并且在组件挂载之后。在此方法中，可以获取到页面的dom节点。子组件的方法会在父组件之前调用 更新阶段 更新阶段是指调用setState或props改变而触发，或者调用forceUpdate来重新渲染组件并把组建的变化显示到DOM元素的过程。 这是一个变化的过程 1,当通过调用setState更新状态触发 shouldComponentUpdate 此方法在收到新的props或者状态更新的过程中会被调用。见名知意，此方法可以控制组件是否要被重新渲染。方法返回true或false，当返回false时，组件不会被渲染。默认返回为true componentWillUpdate此方法将在shouldComponentUpdate返回为true后被调用，类似componentWillMount，只不过它是在每次被重新渲染的时候都会被调用一次 render 同上 componentDidUpdate当组件被更新到DOM之后会被触发 2,父组件传入新的props componentWillReceiveProps(nextProps) 当组件收到新的props会被触发，在首次渲染不会被调用。可以判断是否要根据父类传入新的props而要更改state，在此方法中调用setState不会重复渲染 shouldComponentUpdate 同上 componentWillUpdate 同上 render 同上 componentDidUpdate 同上 3,调用forceUpdate 当不想通过state或props进行改变触发重新渲染时，可以调用forceUpdate，此操作会跳过该组件的 shouldComponentUpdate。但其子组件会触发正常的生命周期方法，包括shouldComponentUpdate方法。通常不推荐使用该方法。 16.3版本 在16.3中新增static getDerivedStateFromProps(nextProps, prevState)和getSnapshotBeforeUpdate(prevProps, prevState)，同时废弃了componentWillMount，componentWillUpdate，componentWillReceiveProps三个方法。 先说为什么要废弃这三个在渲染之前的方法 在React v16中发布了React Fiber，简单说一下Fiber 在现有React中，更新过程是同步的，这可能会导致性能问题。在更新的过程中要做很多事情，比如调用各个组件的生命周期函数，计算和比对Virtual DOM，最后更新DOM树，这整个过程是同步进行的。 Fiber就是把所有任务分成一片一片，每当执行完一片，就会给其他任务执行的机会（感觉可以看成线程竞争cpu时间片😄），所以有可能当一个更新任务还没完成，就被其它优先级高的任务给中断了。重点是被中断的任务不是在中断的地方接着执行，它是重新开始的…重新开始… Fiber分为两个阶段，第一个阶段允许被打断，而第二阶段则不允许。以render为界，可以被打断的生命周期函数就有 componentWillMount componentWillReceiveProps shouldComponentUpdate componentWillUpdate render 所以如果在componentWillMount和componentWillUpdate做一些后台数据的获取或者定时器的初始化等的工作，将不再适合，因为这些方法有可能会被执行多次。官方为了不让大家在上面的方法中继续做不正确的事情， 干脆把这几个方法给废弃了，并推出了新的方法getDerivedStateFromProps，如果之前在废弃的方法中的实现比较纯，那么直接移到新的方法中即可。它是一个静态方法，不能使用this，结果只由输入去改变，返回的结果直接传给setState。 如果是要去后台获取数据或者初始化的工作则可以移到componentWillMount和componentWillUpdate方法中。这两个方法不会有被执行多次的可能。 图来源于http://projects.wojtekmaj.pl/react-lifecycle-methods-diagram/ 挂载阶段 getDerivedStateFromProps static getDerivedStateFromProps(nextProps, prevState) 这是一个静态的方法，会传入新的props和原来的状态值。在调用 render 方法之前调用，它应返回一个对象来更新 state，如果返回 null 则不会更新内容。 官方文档也说了这个方法适用于state的值在任何时候都取决于props，如果要做 有副作用的操作则应使用DidMounting，DidUpdate或在prop更改时“重置”某些state则可以考虑使用受控的组件 如图此方法也可能会被暂停，中断或重新开始，因为处于Render 阶段 render 方法也可能会被暂停，中断或重新开始 componentDidMount 同上 更新阶段 1,调用setState更新状态触发 shouldComponentUpdate 调用setState触发的渲染并不会调用getDerivedStateFromProps，而是直接到了shouldComponentUpdate。需要注意的是shouldComponentUpdate也有执行多次的可能 render 同挂载阶段，也可能多次执行 getSnapshotBeforeUpdate(prevProps, prevState) 这也是在V16.3中新加入的方法，只有在更新阶段，render方法和真正DOM被改变之间执行，如图这时可以读取dom，此方法不会被中止或重新开始。方法返回的值会在componentDidUpdate方法的第三个参数传入 componentDidUpdate(prevProps, prevState, snapshot) componentDidUpdate 同上 2,父组件传入新的props getDerivedStateFromProps 在更新阶段的setState方法而触发的重新渲染会被调用，解释如上。**需要注意的是，此方法也在“纯净不包含副作用”阶段，所以可能会被执行多次。 shouldComponentUpdate 同上 render 同上 componentDidUpdate 同上 3,调用forceUpdate 此过程和16.3之前一样会直接到render方法 ^16.4 在16.3中新加入的getDerivedStateFromProps方法只有在挂载和由setState引起的Updating才会被执行，这可能会给开发人员带来困扰，父组件传入新的props和forceUpdate并不会Updating。 **在16.4中，React官方改正了这一点，无论是挂在阶段或是任何动作引起的Updating都会触发getDerivedStateFromProps**。 所以16.4的生命周期方法如下图： 解释：略 总结 用静态函数getDerivedStateFromProps来取代被deprecate的几个生命周期函数，就是强制开发者在render之前只做无副作用的操作，而且能做的操作局限在根据props和state决定新的state，因为在render阶段生命周期方法可能会被执行多次。废弃的方法会在17版本中删除 参考https://zh-hans.reactjs.org/docs/react-component.html React v16.3之后的组件生命周期函数 React Fiber是什么 http://projects.wojtekmaj.pl/react-lifecycle-methods-diagram/","link":"/2019/09/02/2019-09-02-react-component-lifecycle-methods/"},{"title":"SpringBoot参数配置的坑","text":"问题在yml中配置参数，用 @ConfigurationProperties 注解来注入，发现配置以0开头的字符串得到的结果是错的，比如配置的01010807，实际的值变为 1010807.0 解决配置的值用加引号 &quot;&quot;即可，原因是值被当成了数字类型 123the-one: todo: region: &quot;01010807&quot; https://github.com/spring-projects/spring-boot/issues/9389","link":"/2020/04/17/2020-04-17-springboot-configuration-properties/"},{"title":"mybatis3 一级缓存","text":"mybatis 一级缓存mybatis有一级，二级缓存机制，一级缓存是默认开启的本地缓存，且不可关闭 本文主要介绍一级缓存。通过本文你将了解： 什么是一级缓存？使用一级缓存的好处 一级缓存是如何设计的？ Cache接口的设计以及CacheKey的定义 一级缓存的生命周期 使用一级缓存值得注意的点 什么是一级缓存？使用一级缓存的好处说到 一级缓存 那就不得不说 SqlSession 对象。顾名思义，session 代表与数据库的会话。每当我们使用MyBatis执行sql时，MyBatis 会创建出一个 SqlSession 对象表示一次数据库会话。 在一次会话中，我们有可能会很多的语句，或反复地执行完全相同的语句。对于反复执行相同的语句且返回的结果是相同的话就没必要每次都去查询数据库了，这么做不但效率低且浪费资源。 为了解决这一问题，减少资源的浪费，MyBatis会在表示会话的 SqlSession 对象中建立一个简单的缓存，将每次查询到的结果结果缓存起来，当下次查询的时候，如果判断先前有个完全一样的查询，会直接从缓存中直接将结果取出，返回给用户，不需要再进行一次数据库查询了。 这个缓存是一个本地缓存(local cache)，存储在 SqlSession 对象中。对于每一次查询，都会尝试根据查询的条件去本地缓存中查找是否在缓存中，如果在缓存中，就直接从缓存中取出，然后返回给用户；否则，从数据库读取数据，将查询结果存入缓存并返回给用户 对于会话（Session）级别的数据缓存，我们称之为一级缓存 一级缓存是如何设计的？简单示意图： 当创建新的 SqlSession时，Mybatis也会为这个 SqlSession 创建一个 Executor 执行器，它是实际执行数据库操作的对象。一级缓存就维护在 Executor 对象中。而对缓存和缓存相关的操作，Mybatis将它封装在 Cache接口中。 所以SqlSession,Executor,Cache三者的关系类图如下： 如上述的类图所示，Executor 接口的实现类 BaseExecutor 中拥有一个 Cache 接口的实现类 PerpetualCache ，所以它将使用 PerpetualCache 对象维护缓存。 综上，SqlSession，Executor,Cache 三个对象之间的关系图如下： 工作流程一级缓存执行的时序图，如下图所示。 我们已经知道 一级缓存就是 PerpetualCache 对象维护的 ，那么 PerpetualCache 如何实现的也就是 一级缓存 的原理了 PerpetualCache 对象一级缓存内部实现其实就是用 HashMap 来实现的，以 Key,Value 形式维护缓存，下面是 PerpetualCache 提供的一些接口，对一级缓存的操作实则是对HashMap的操作。 Cache接口的设计以及CacheKey的定义Cache 接口有很多的实现，一级缓存只会涉及到这一个 PerpetualCache 子类。通过阅读 PerpetualCache 源码我们知道，缓存内部使用 Map来维护的，key 是本次查询的特征值， value 是本次查询的查询结果。 什么是 本次查询特征值？ 也就是能代表本次查询的，它不能单单是查询的sql，也不能是查询的参数，它应该是本次查询所有条件的集合！所以如何确定本次查询的特征值就是一级缓存的重点，也就是如何确定两次查询是否是一样的？ Mybatis认为，对于是两次查询是否是相同的，需要满足以下的条件： 相同的statementId 结果集中的要求的结果范围 （结果的范围通过rowBounds.offset和rowBounds.limit表示）相同 经过参数解析过后的字符串（boundSql.getSql()）要相同 给java.sql.Statement设置的参数值 分别解释上述四个条件： 对于Mybatis而言，你要执行哪个接口，哪条sql，必须传入对应的statementId MyBatis自身提供的分页功能是通过RowBounds来实现的，它通过rowBounds.offset和rowBounds.limit来过滤查询出来的结果集，这种分页功能是基于查询结果的再过滤，而不是进行数据库的物理分页。如果查询有传入分页条件，那么两次查询分页条件也必须一致，因为也会造成结果不一致。 Mybatis底层还是调用的JDBC API去访问数据库。对于JDBC而言，两次查询的sql和参数都一致，那么结果也可以认为是一致的。 第四点就是保证参数要一致 boundSql.getSql() 返回的是经过映射参数解析过的sql，比如#{}会解析成？，${}则已经把参数替换上。然后MyBatis拿着这个sql创建JDBC的PreparedStatement对象，对于这个PreparedStatement对象，还需要对它设置参数，调用setXXX()来完成设值，第4条，就是要求对设置JDBC的PreparedStatement的参数值也要完全一致。 综上所述，就是要满足 调用JDBC API的时候，传入的SQL语句要完全相同，传递的参数值也要完全相同，如果有用RowBounds分页，那么分页参数也要一致，CacheKey 也就由 statementId + RowBounds + 传递给JDBC的SQL + 传递给JDBC的参数值 决定。 CacheKey 作为本次查询的 特征值 它的作用就是在查询的时候去缓存 Map 中查找缓存，如果查找到缓存，那么直接返回，如果缓存中没查到，那么就去数据库查询，查询后将这个 CacheKey 作为 key，查询结果作为 value 存储到 缓存Map中。 CacheKeyCacheKey 的构建方法在 BaseExecutor 中，源码如下： 1234567891011121314151617181920212223242526272829@Override public CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql) { ............ CacheKey cacheKey = new CacheKey(); // statementId cacheKey.update(ms.getId()); // rowBounds.offset 分页参数 cacheKey.update(rowBounds.getOffset()); // rowBounds.limit 分页参数 cacheKey.update(rowBounds.getLimit()); // boundSql.getSql() SQL语句 cacheKey.update(boundSql.getSql()); for (ParameterMapping parameterMapping : parameterMappings) { if (parameterMapping.getMode() != ParameterMode.OUT) { Object value; String propertyName = parameterMapping.getProperty(); ......... // 每一个参数值 cacheKey.update(value); } } // 运行环境 if (configuration.getEnvironment() != null) { // issue #176 cacheKey.update(configuration.getEnvironment().getId()); } return cacheKey; } CacheKey hashcode算法一级缓存内部实现本质还是用 Map&lt;K,V&gt; 来实现的，而构建 CacheKey 目的也就是作为 Map 的key，所以构建 CacheKey 的过程也可以看成是构建 hashcode 的过程（map 的 key值取得是hashcode） 下面是构建 hashcode 的过程： 1234567891011121314public void update(Object object) { // 得到对象的hashcode int baseHashCode = object == null ? 1 : ArrayUtil.hashCode(object); // 更新计数++ count++; // 所有的baseHashCode相加的值 checksum += baseHashCode; // baseHashCode乘以 count倍 baseHashCode *= count; // hashcode = 拓展因子（默认37）* 当前hashcode * baseHashCode hashcode = multiplier * hashcode + baseHashCode; updateList.add(object); } CacheKey 重写的 equals 方法 123456789101112131415161718192021222324252627282930@Override public boolean equals(Object object) { if (this == object) { return true; } if (!(object instanceof CacheKey)) { return false; } final CacheKey cacheKey = (CacheKey) object; if (hashcode != cacheKey.hashcode) { return false; } if (checksum != cacheKey.checksum) { return false; } if (count != cacheKey.count) { return false; } for (int i = 0; i &lt; updateList.size(); i++) { Object thisObject = updateList.get(i); Object thatObject = cacheKey.updateList.get(i); if (!ArrayUtil.equals(thisObject, thatObject)) { return false; } } return true; } 一级缓存的生命周期从上面内容我们知道一级缓存是维护在 SqlSession 对象里的 Executor 对象中，那么它的最大生命周期也就是 PerpetualCache &lt;= Executor &lt;= SqlSession， MyBatis在开启一个数据库会话时，会创建一个新的SqlSession对象；当会话结束时，SqlSession对象及其内部的Executor对象还有PerpetualCache对象也一并释放掉。 如果SqlSession调用了close()方法，会释放掉一级缓存PerpetualCache对象 如果SqlSession调用了rollback()方法，会清空PerpetualCache对象中的数据，但是该对象并未释放； 如果SqlSession调用了clearCache()，会清空PerpetualCache对象中的数据，但是该对象并未释放； 如果SqlSession调用了commit()，会清空PerpetualCache对象中的数据，但是该对象并未释放； SqlSession中执行了任何一个update操作(update()、delete()、insert()) ，都会清空PerpetualCache对象的数据，但是该对象并未释放掉；在查询操作中，如果该 MappedStatement 设置了强制刷新缓存 (flushCache=true)，那么在去查询缓存map之前，会先清空PerpetualCache对象的数据；Mybatis一级缓存默认的 scope 是 SESSION 级别的， 如果设置成 STATEMENT 级别 ，那么在每次查询之后都会去清除缓存数据（相当于关闭一级缓存）。 在Spring中使用Mybatis，SqlSession的生命周期是线程级别的，SqlSessionUtils 类中会将获取的SqlSession绑定到当前上下文中（内部使用ThreadLocal），所以SqlSession中的一级缓存最大生命周期也就是当前线程 使用一级缓存值得注意的点 一级缓存没有更新缓存的概念，在查询中，只要命中缓存，那么直接返回缓存中的结果，不会再去数据库中查询。一级缓存也不会过期，如不清除缓存数据或缓存对象一直未被释放，那么它会一直存在。 对于更新频繁的，并且需要高时效准确性的数据，使用 SqlSession 查询的时候，要控制好对象生存时间，生存时间越长，它其中缓存的数据有可能就越旧，从而造成和真实数据的误差；同时对于这种情况，可以手动地适时清空SqlSession中的缓存，或设置强制刷新缓存，或设置一级缓存为STATEMENT 一级缓存直接返回对象的唯一引用，如果直接修改将会影响缓存中的值 由于一级缓存的范围是 SqlSession 的，所以当有多个 SqlSession 同时进行读写操作，可以会读取到脏数据 其它Mybatis文档对本地缓存的介绍： Mybatis 使用到了两种缓存：本地缓存（local cache）和二级缓存（second level cache）。每当一个新 session 被创建，MyBatis 就会创建一个与之相关联的本地缓存。任何在 session 执行过的查询结果都会被保存在本地缓存中，所以，当再次执行参数相同的相同查询时，就不需要实际查询数据库了。本地缓存将会在做出修改、事务提交或回滚，以及关闭 session 时清空。 默认情况下，本地缓存数据的生命周期等同于整个 session 的周期。由于缓存会被用来解决循环引用问题和加快重复嵌套查询的速度，所以无法将其完全禁用。但是你可以通过设置 localCacheScope=STATEMENT 来只在语句执行时使用缓存。 注意，如果 localCacheScope 被设置为 SESSION，对于某个对象，MyBatis 将返回在本地缓存中唯一对象的引用。对返回的对象（例如 list）做出的任何修改将会影响本地缓存的内容，进而将会影响到在本次 session 中从缓存返回的值。因此，不要对 MyBatis 所返回的对象作出更改，以防后患。","link":"/2020/05/11/2020-05-11-mybatis-first-level-cache/"},{"title":"mybatis3 二级缓存","text":"二级缓存的机制与工作模式在上一篇 mybatis3 一级缓存 中提到一级缓存的最大共享范围是 SqlSession ，如果需要多个 SqlSession 共享，就需要使用二级缓存。二级缓存是默认开启的，当开启后（ cacheEnabled=true ）会使用 CachingExecutor 装饰 Executor 。CachingExecutor 是 Executor 的装饰者，以增强Executor的功能，使其具有缓存查询的功能。 类图如下： 以下是Configuration类初始化 Executor 对象代码片段： 123456...........if (cacheEnabled) { executor = new CachingExecutor(executor);}executor = (Executor) interceptorChain.pluginAll(executor);return executor; 注：本文参照3.5.5-SNAPSHOT版本，cacheEnabled 属性是默认开启的，在 XMLConfigBuilder 中 249行 1configuration.setCacheEnabled(booleanValueOf(props.getProperty(&quot;cacheEnabled&quot;), true)); 开启二级缓存后，SqlSession 就使用 CachingExecutor 对象来完成操作请求，对于查询的请求CachingExecutor 会先去二级缓存查找是否有缓存，如果有，那么直接返回给用户缓存数据，如果没有，则由 Executor 对象去完成查询操作，再把Executor 查询返回的数据存入到二级缓存中，再返回给用户。 在查询一级缓存之前会先在 CachingExecutor 中查询二级缓存中是否有数据，具体查询工作流程如图： 查询的工作流程为 二级缓存 -&gt; 一级缓存 -&gt; 数据库 当开启二级缓存后，同一个 namespace 下的所有sql影响着同一个 Cache ，即同一个 namespace下的所有 MappedStatement 影响着同一个 Cache ，这个 Cache 被多个 SqlSession 共享，相当于一个全局变量 二级缓存的特点MyBatis自身提供了丰富的，并且功能强大的二级缓存的实现，它拥有一系列的Cache接口装饰者，可以满足各种对缓存操作和更新的策略。在 CachingExecuter 内部有一个用来管理二级缓存的 TransactionalCacheManager 对象，TransactionalCacheManager 内部只有一个成员变量，key 是 mapper 中定义的cache对象，value 是暂时存缓存数据的对象。 TransactionalCache 装饰的对象就是定义的 cache 对象 ： 123456789// key 是 `mapper` 中定义的cache对象，value 是暂时存缓存数据的对象private final Map&lt;Cache, TransactionalCache&gt; transactionalCaches = new HashMap&lt;&gt;();// ...... 其它方法// 如果缓存对象不存在，就创建一个装饰着cache的TransactionalCache对象private TransactionalCache getTransactionalCache(Cache cache) { return transactionalCaches.computeIfAbsent(cache, TransactionalCache::new);} TransactionalCache 也是cache接口的装饰者之一，主要作用是保存SqlSession在事务中需要向某个二级缓存提交的缓存数据（因为事务过程中的数据可能会回滚，所以不能直接把数据就存入二级缓存，而是暂存在TransactionalCache中，在事务提交后再将过程中存放在其中的数据提交到二级缓存，如果事务回滚，则将数据清除掉）。 TransactionalCache 对象有四个属性： 12345678// 被装饰对象private final Cache delegate;// 清除标记，在commit时会清空二级缓存private boolean clearOnCommit;// 需要在commit时存入二级缓存的临时数据private final Map&lt;Object, Object&gt; entriesToAddOnCommit;// 缓存未命中的数据，commit时，也会放入二级缓存（key,null）private final Set&lt;Object&gt; entriesMissedInCache; get12345678910111213141516@Overridepublic Object getObject(Object key) { // issue #116 Object object = delegate.getObject(key); if (object == null) { // 记录未命中的key entriesMissedInCache.add(key); } // issue #146 if (clearOnCommit) { return null; } else { return object; }} put12345@Overridepublic void putObject(Object key, Object object) { // 记录要存入二级缓存的key和值 entriesToAddOnCommit.put(key, object);} commit1234567891011121314151617181920212223public void commit() { // 清空二级缓存，update或flushCache=true时会设为true if (clearOnCommit) { delegate.clear(); } // 将未命中和命中的数据写入缓存 flushPendingEntries(); // 重置值 reset();}private void flushPendingEntries() { for (Map.Entry&lt;Object, Object&gt; entry : entriesToAddOnCommit.entrySet()) { delegate.putObject(entry.getKey(), entry.getValue()); } for (Object entry : entriesMissedInCache) { if (!entriesToAddOnCommit.containsKey(entry)) { delegate.putObject(entry, null); } }} 二级缓存的划分每个 mapper 都有一个自己的 Cache 对象，也可以多个mapper共享一个 Cache 对象 为mapper配置一个Cache对象： 在mapper.xml中配置 &lt;cache&gt; 节点或在接口添加 @CacheNamespace 注解 为多个mapper配置一个Cache对象： 配置 &lt;cache-ref&gt; 节点或在接口添加 @CacheNamespaceRef 注解 通过 &lt;cache-ref&gt; 标签，定义 namespace 来指定要引用的缓存的命名空间。这句话有点绕，也就是 123&lt;mapper namespace=&quot;com.ddmcc.UserMapper&quot;&gt; &lt;cache-ref namespace=&quot;com.ddmcc.AdminUserMapper&quot; /&gt;&lt;/mapper&gt; 这时就要求 AdminUserMapper 必须定义了 &lt;cache&gt; 节点。如果用注解的方式如下： 12345678910111213141516171819@CacheNamespacepublic interface UserMapper { // ... }// 定义要引用的命名空间的类型@CacheNamespaceRef(AdminUserMapper.class)public interface UserMapper { // ...}或者 // 定义要引用的命名空间的name@CacheNamespaceRef(name = &quot;AdminUserMapper&quot;)public interface UserMapper { // ...} 通过以上配置，就可以让多个 Mapper 公用一个 Cache 使用二级缓存要具备的条件Mybatis二级缓存粒度很细，可以精确到每一条查询语句是否使用缓存 在Mybatis配置中开启了缓存，并且在 mapper 中配置了 节点，这并不意味着每一条查询语句都会使用到缓存，还需要指定 select 语句是否开启缓存 ， useCache=&quot;true&quot; 声明这条语句开启缓存后，才会使用缓存。 1&lt;select id=&quot;listUser&quot; resultType=&quot;com.ddmcc.User&quot; useCache=&quot;true&quot;&gt; 要想使用二级缓存，那么需要满足以下三个条件： 开启二级缓存的总开关：全局配置变量参数 cacheEnabled=true （默认开启） 为mapper配置了cache、cacheRef节点或者mapper接口中配置了 @CacheNamespace、@CacheNamespaceRef注解 该select语句节点开启了缓存useCache=”true” （默认开启） 对于我们平常使用来说，如果为一个mapper配置的cache节点，那么此mapper中的查询语句将会使用二级缓存 二级缓存的生命周期回顾一级缓存在 一级缓存 中，数据从数据库查询返回后，就会存入缓存中，SqlSession 执行 commit（提交），close（关闭），rollback（回滚），任何一个 update 操作或是 clearLocalCache 会清空一级缓存。 二级缓存生命周期二级缓存则不同，上面说了在缓存创建的时候并不是直接put进缓存对象中，而是会先暂存在 TransactionalCache 对象中。当 sqlSession 关闭或提交时，再把数据刷入缓存中，如果rollback 那么不会把数据刷入缓存中，但也不会清空已有的二级缓存。sqlSession 执行任何一个 update 操作时，在事务提交后，都会去清空二级缓存。sqlSession的 clearCache 只清除自己会话中的一级缓存，并不会清除二级缓存。 但，并不是 select 语句就会存入二级缓存，update 就会清除二级缓存。主要有标签的两个参数决定：useCache和flushCache。select标签默认是 userCache=true 、flushCache=false，所以会将数据存入二级缓存。而 insert、update、delete 标签默认是 userCache=false、flushCache=true，如果将flushCache改成false，那么也不会去清除缓存 总的来说： sqlSession调用commit或close才会把二级缓存存入 sqlSession执行任一update操作会清除缓存 sqlSession回滚不会清除缓存，但本次事务内的查询数据也不会写入缓存中 由userCache、flushCache两个参数决定语句是否使用缓存或清除缓存 需要注意的点数据需要sqlSession关闭或提交才会将数据写入缓存同一事务执行多次相同查询或是多个session执行同一mapper下的同一查询语句并不会命中二级缓存，因为二级缓存在 sqlSession close或commit的时候才会写入 代码示例： 1234567891011121314151617public static void main(String[] args) throws Exception{ SqlSessionFactory sqlSessionFactory; try (Reader reader = Resources.getResourceAsReader(&quot;com/ddmcc/mybatis-config.xml&quot;)) { sqlSessionFactory = new SqlSessionFactoryBuilder().build(reader); } // 开启sqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); UserMapper userMapper = sqlSession.getMapper(UserMapper.class); System.out.println(&quot;第一次查询：&quot; + userMapper.getUser(&quot;1&quot;).toString()); System.out.println(&quot;第二次查询：&quot; + sqlSession.getMapper(UserMapper.class).getUser(&quot;1&quot;)); // sqlSession未close或commit SqlSession sqlSession1 = sqlSessionFactory.openSession(); sqlSession1.getMapper(UserMapper.class).getUser(&quot;1&quot;);} 运行结果分析： 让sqlSession1提交，数据写入二级缓存 12345...........System.out.println(&quot;第一次查询：&quot; + userMapper.getUser(&quot;1&quot;).toString()); System.out.println(&quot;第二次查询：&quot; + sqlSession.getMapper(UserMapper.class).getUser(&quot;1&quot;));sqlSession.commit();.......... 运行结果，sqlSession2命中二级缓存： 二级缓存的实体类需要实现序列化接口cache 节点有一个 readOnly 属性，默认为false，这个属性决定缓存值是只读的还是读写的。当readOnly = false 时，Mybatis会用 SerializedCache 序列化缓存类来装饰 cache 对象，通过序列化和反序列化来保证通过缓存取出来的是一个新的对象。如果配置为只读缓存，MyBatis就会使用Map来存储缓存值（可读写缓存内部也是用PerpetualCache，在SerializedCache的put和get中进行了序列化化和反序列化），这种情况下，从缓存中获取的对象就是同一个实例。 序列化缓存 好处：先将对象序列化成2进制，再缓存，将对象压缩了，省内存。并且线程安全 坏处：是速度慢了（因为对象需要进行序列化） Mybatis通过序列化得到对象的新实例，保证多线程安全（因为是从缓存中取数据，速度还是比从数据库获取要快）。具体说就是对象序列化后存储到缓存中，从缓存中取数据时是通过反序列化得到新的实例。 CacheBuilder类初始化缓存对象源码片段： 1234567..........if (readWrite) { cache = new SerializedCache(cache);}cache = new LoggingCache(cache);cache = new SynchronizedCache(cache);.......... SerializedCache类序列化反序列化源码片段： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// put@Overridepublic void putObject(Object key, Object object) { if (object == null || object instanceof Serializable) { // 序列化 delegate.putObject(key, serialize((Serializable) object)); } else { // 未实现序列化接口抛出异常 throw new CacheException(&quot;SharedCache failed to make a copy of a non-serializable object: &quot; + object); }}// get@Overridepublic Object getObject(Object key) { Object object = delegate.getObject(key); // 反序列化 return object == null ? null : deserialize((byte[]) object);}private byte[] serialize(Serializable value) { try (ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream oos = new ObjectOutputStream(bos)) { oos.writeObject(value); oos.flush(); return bos.toByteArray(); } catch (Exception e) { throw new CacheException(&quot;Error serializing object. Cause: &quot; + e, e); }}private Serializable deserialize(byte[] value) { Serializable result; try (ByteArrayInputStream bis = new ByteArrayInputStream(value); ObjectInputStream ois = new CustomObjectInputStream(bis)) { result = (Serializable) ois.readObject(); } catch (Exception e) { throw new CacheException(&quot;Error deserializing object. Cause: &quot; + e, e); } return result;} readOnly 默认为false的情况下，二级缓存取出的是一个新的对象： 将 readOnly 修改为 true，缓存取出对象为同一对象： 1&lt;cache readOnly=&quot;true&quot;/&gt; Cache 节点配置属性12345&lt;cache eviction=&quot;LRU&quot; flushInterval=&quot;&quot; size=&quot;1024&quot; readOnly=&quot;false&quot;/&gt; 在默认的情况下： 映射语句文件中的所有 select 语句的结果将会被缓存。映射语句文件中的所有 insert、update 和 delete 语句会刷新缓存。缓存会使用最近最少使用算法（LRU, Least Recently Used）算法来清除不需要的缓存。缓存不会定时进行刷新（也就是说，没有刷新间隔）。缓存会保存列表或对象的1024个引用。缓存会被视为读/写缓存，这意味着获取到的对象并不是共享的，可以安全地被调用者修改，而不干扰其他调用者或线程所做的潜在修改 可用的清除策略有： LRU – 最近最少使用：移除最长时间不被使用的对象。FIFO – 先进先出：按对象进入缓存的顺序来移除它们。SOFT – 软引用：基于垃圾回收器状态和软引用规则移除对象。WEAK – 弱引用：更积极地基于垃圾收集器状态和弱引用规则移除对象。默认的清除策略是 LRU。 flushInterval（刷新间隔）属性可以被设置为任意的正整数，设置的值应该是一个以毫秒为单位的合理时间量。 默认情况是不设置，也就是没有刷新间隔，缓存仅仅会在调用语句时刷新。 size（引用数目）属性可以被设置为任意正整数，要注意欲缓存对象的大小和运行环境中可用的内存资源。默认值是 1024。 readOnly（只读）属性可以被设置为 true 或 false。只读的缓存会给所有调用者返回缓存对象的相同实例。 因此这些对象不能被修改。这就提供了可观的性能提升。而可读写的缓存会（通过序列化）返回缓存对象的拷贝。 速度上会慢一些，但是更安全，因此默认值是 false。 提示：二级缓存是事务性的。这意味着，当 SqlSession 完成并提交时，或是完成并回滚，但没有执行 flushCache=true 的 insert/delete/update 语句时，缓存会获得更新。","link":"/2020/05/18/2020-05-18-mybatis-second-level-cache/"},{"title":"多线程与高并发","text":"synchronizedsynchronized 锁的是对象而不是代码，synchronized(this) 和 synchronized方法 是一样的，都是锁定当前对象。锁升级从偏向锁到自旋锁再到重量级锁。 123456789101112// thispublic void test() { synchronized(this) { .... }}// 同步方法public synchronized void test() { ....} 静态方法没有 this 对象，如果是 静态的同步方法，那么锁对象就是类对象 12345678public class T { // 静态同步方法 等同于 synchronized(T.class) public synchronized static void test() { .... }} synchronized可重入可重入锁 通俗来讲，当线程请求一个由其它线程持有的对象锁时，该线程会阻塞，而当线程请求由自己持有的对象锁时，如果该锁是重入锁，请求就会成功，否则阻塞。 事例代码： 12345678910111213141516171819202122232425262728293031323334package com.soft.ot;/** * @author jiangrz */public class ReentrantLockTest extends SuperReentrantLockTest { @Override public synchronized void doSomething() { System.out.println(&quot;doSomething&quot;); doAnotherThing(); } public void doAnotherThing() { super.doSomething(); System.out.println(&quot;doAnotherThing&quot;); } public static void main(String[] args) { ReetrantLockTest reetrantLock = new ReentrantLockTest(); reetrantLock.doSomething(); }}class SuperReentrantLockTest { public synchronized void doSomething() { System.out.println(&quot;super doSomething&quot;); }} 运行输出： 123doSomethingsuper doSomethingdoAnotherThing 在上面的事例中，锁对象只有一个，那就是 reetrantLock 对象。当执行 reetrantLock.doSomething() 时，该线程获得 reetrantLock 对象的锁，在 doSomething 方法中调用 doAnotherThing 方法时，再次请求 ** reetrantLock** 对象的锁，因为 synchronized 是可重入锁，所以可以得到该锁，继续在 doSomething 中调用父类的方法时，第三次请求这把锁同样可以得到。如果 synchronized 不是可重入锁，那么后面这两次请求会被一直阻塞，从而导致死锁。同一线程在调用锁对象中其他 synchronized 方法/块或调用父类的 synchronized 方法/块都不会阻碍该线程的执行。就是说同一线程对同一个对象锁是可重入的，而且同一个线程可以获取同一把锁多次，也就是可以多次重入 synchronized原理 早期，底层实现是 重量级锁 ，需要找系统去申请锁，这就造成效率非常低下 改进，后来改进了有了锁升级的概念。 第一个去访问某把锁的线程先在锁对象的头上面记录一下这个线程（如果只有一个线程访问的时候，实际没有给锁对象加锁，只是记录一下这个线程的ID（偏向锁）） 偏向锁如果有线程争用的话，就升级为 自旋锁 。也就是后面来的线程不会到cpu就绪队列里去，而是进行自旋操作，等待着占用的cpu，等待锁释放。 在自旋10次（jdk1.6）或有一定等待的数量线程（超过cpu内核数的一半）之后，升级为 重量级锁。 synchronized 锁对象不能用Stringvolatilevolatile 有什么用？总结一句话是：volatile 保证线程的可见性，同时防止指令重新排序。 说得详细点就是：对 volatile 的写具有与锁释放相同的效果，它可以确保在对变量赋值之后将其从高速缓存中刷新到主内存，以便变量的值立即对其它线程可见；同样的，对volatile 的读具有与获得锁相同的存储效果，在读取 volatile 变量之前，会高速缓存中的变量值失效，以便重新去主内存中获取值，而不是缓存中的。volatile 变量不能相互重新排序，并且对前后的非volatile也进行严格的限制，在线程A对 volatile 变量 f 进行赋值时所有可见的内容，在线程B读取 f 时都可见。 简单事例： 123456789101112131415class VolatileExample { int x = 0; volatile boolean v = false; public void writer() { x = 42; v = true; } public void reader() { if (v == true) { //uses x - guaranteed to see 42. } }} 假定一个线程正在调用 writer ，而另一个线程正在调用 reader 。在 writer 中将true赋值给 v ，如果 reader 读取到的 v 为 true 那么此时读取x的值必然是 42。这是因为 v 被 volatile 修饰了，如果v没被volatile修饰，则编译器可以对 writer 的写入进行重排序，而reader对x的读取可能会为0。 jdk官方形容说： “volatile 的语义几乎达到了同步的水平。出于可见性目的，对易失性字段的每次读取或写入都像“半”同步一样” 重新排序是什么意思？在许多情况下，对变量的赋值，访问与程序指定的顺序是不一致的。编译器会以 “优化” 的名义对指令进行重新排序。比如 123456789101112class Reordering { int x = 0, y = 0; public void writer() { x = 1; y = 2; } public void reader() { int r1 = y; // 读取到值为2 int r2 = x; }} 假设此代码在两个线程中同时执行，并且y的读取看到值2。由于x的赋值在y之后，所以会认为x的值必定为1。但是，写入可能已重新排序。如果发生这种情况，则可能发生对y的写入，随后是两个变量的读取，然后可能发生对x的写入。结果将是r1的值为2，而r2的值为0。","link":"/2020/07/26/2020-07-26-multithreading-and-high-concurrency/"},{"title":"自动拆箱与装箱","text":"有了基本类型为什么还需要包装类型呢？因为 Java 是一种面向对象语言，很多地方都需要使用对象而不是基本数据类型。比如… 概念 在说到拆箱和装箱之前，需要了解Java中有八种基本的数据类型，分别是：byte、short、char、int、long、float、double和boolean。这八种基本类型在Java中都有对应的包装类型：Byte、Short、Character、Integer、Long、Float、Double以及Boolean。 有了基本类型为什么还需要包装类型呢？因为 Java 是一种面向对象语言，很多地方都需要使用对象而不是基本数据类型。比如，在集合类中，我们是无法将 int 、double 等类型放进去的。因为集合的容器要求元素是 Object 类型。为了让基本类型也具有对象的特征，就出现了包装类型，它相当于将基本类型“包装起来”，使得它具有了对象的性质，并且为其添加了属性和方法，丰富了基本类型的操作。 自动拆装箱（1.5引入）什么是自动拆、装箱？ 自动装箱：就是将基本数据类型自动转换成对应的包装类 自动拆箱：就是将包装类自动转换成对应的基本数据类型 自动拆装箱有什么好处 方便 基本类型和包装类型转换无需手动编码实现，由编译器帮我们完成 节省空间 大部分包装类型都会有缓存的存在，缓存一定范围大小的数据。如果是在范围内大小，则直接返回缓存内的对象，而不新建。节省的存储空间 自动装箱与自动拆箱的实现原理自动装箱都是通过包装类的 valueOf() 方法来实现的,自动拆箱都是通过包装类对象的 xxxValue() 来实现的 如有以下代码： 1234public static void main(String[] args) { Integer a = 1; int b = a;} 进行反编译后的代码如下： 1234public static void main(String[] args) { Integer a = Integer.valueOf((int)1); int b = a.intValue();} 从上面反编译后的代码可以看出，int 的自动装箱是通过 Integer.valueOf() 方法来实现的。Integer 的自动拆箱是通过 integer.intValue 来实现的。 如果将八种类型都反编译一遍就会发现上面说的规律：装箱valueOf(),拆箱XXXValue 自动拆装箱带来的问题==比较问题123456789101112public static void main(String[] args) { Integer a = 100; Integer b = 100; Integer c = 200; Integer d = 200; Integer e = new Integer(100); Integer f = Integer.valueOf(100); System.out.println(a == b); System.out.println(c == d); System.out.println(a == e); System.out.println(a == f);} 上面代码运行后输出： 1234truefalsefalsetrue 由于自动装箱机制，在一定数值范围内返回的是同一个对象，在==比较时为true。但如果未命中缓存则会为false。所以比较时应该使用equals或先转成基本类型在进行 空指针问题自动拆箱时返回的是包装对象里 的xxxValue的值，如果包装类型为null，那么在拆箱的时候就会抛NPE 1234public static void main(String[] args) { Integer a = null; int b = a;} 循环内自动装箱问题123456public static void main(String[] args) { Integer num = 0; for (int i = 0; i &lt; 5000; i++) { num += 1; }} 反编译后代码： 123456public static void main(String[] args) { Integer num = Integer.valueOf((int)0); for (int i = 0; i &lt; 5000; ++i) { num = Integer.valueOf((int)(num.intValue() + 1)); }} 在循环中自动装箱，相当于循环内创建对象，会造成大量的对象产生，浪费系统资源同时加重垃圾回收的工作。 Integer缓存机制再回到上面 ==比较问题 中的代码： 12345678public static void main(String[] args) { Integer a = 100; Integer b = 100; Integer c = 200; Integer d = 200; System.out.println(a == b); System.out.println(c == d);} 实际执行结果 a = b 和 c != d。原因就和 Integer 中的缓存机制有关。在JDK1.5中引入缓存来节省使用包装类时内存和提高性能。实现原理是在包装内部有一个静态内部类，在内部类加载时会先创建好一定范围内的对象存到一个数组里。在调用valueOf时，在判断是不是在缓存范围内，如果是直接返回缓存数组里的，否则再新建一个对象。 缓存在第一次使用时被初始化。大小可以由JVM参数-XX:AutoBoxCacheMax=size来指定。JVM初始化时此值被设置成java.lang.Integer.IntegerCache.high属性并作为私有的系统属性保存在sun.misc.vm.class中。 在Java语言规范规定 缓存的范围 如果一个变量p的值是： -128至127之间的整数(§3.10.1) true 和 false的布尔值 (§3.10.3) ‘\\u0000’至 ‘\\u007f’之间的字符(§3.10.4) 将p包装成a和b两个对象时，可以直接使用a==b判断a和b的值是否相等。也就是这个范围中的值会使用缓存 缓存的对象 有ByteCache用于缓存Byte对象 有固定范围: -128 到 127 有ShortCache用于缓存Short对象 有固定范围: -128 到 127 有LongCache用于缓存Long对象 有固定范围: -128 到 127 有CharacterCache用于缓存Character对象 范围是 0 到 127 缓存行为不仅适用于Integer对象。针对所有的整数类型的类都有类似的缓存机制 其它约束 除了Integer以外，缓存范围都不能改变 参考资料Java的自动拆装箱 Java中整型的缓存机制","link":"/2021/02/08/2021-02-08-boxing-unboxing/"},{"title":"扒一扒String","text":"String不可变性什么是不可变对象 众所周知，String 对象是不可变的。那么什么是不可变对象呢？在 Java教程 中定义：如果一个对象在构造后状态无法改变，则该对象被视为不可变。 不能改变状态的意思是，不能改变对象内的成员变量，包括基本数据类型的值不能改变，引用类型的变量不能指向其他的对象，引用类型指向的对象的状态也不能改变。 不可变对象的遵守策略（规则） 类中所有字段都被 final 修饰，并且是私有的，也就是被 private 修饰。 不能提供修改字段或字段引用对象的 setter 方法。 不允许子类重写方法 如果实例字段包含对可变对象的引用，则不允许更改这些对象： 不要提供修改可变对象的方法。 不要直接返回可变对象的引用。如有必要，可创建内部可变对象的副本，并返回可变对象的副本。 String的不可变性定义一个字符串 s： 1234567public static void main(String[] args) { String s = &quot;abcd&quot;; System.out.println(s); s = &quot;abcdef&quot;; System.out.println(s);} 输出结果为： 12abcdabcdef 首先创建一个对象 s ，并赋值为abcd，然后再赋值为abcdef。从输出结果可以看出s的值确实改变了，那么为什么还说String对象是不可变的呢？ 其实 s 变量只是保存了对象的引用，该引用指向了堆中具体的对象。如下图： 然后又创建了一个新的对象abcdef， 而引用s重新指向了这个新的对象，原来的对象abcd还在内存中存在，并没有改变。如下图： 所以上面的代码仅仅改变了s的引用地址 为什么String对象是不可变的？要理解String的不可变性，先看String类的源码： 1234567public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence { /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 通过源码可以发现：String类内部是用一个字符数组来维护值的，并且被申明为 private final ，也就是value字符数组在对象被构造后就不允许重新赋值了。并且类内部也没有提供可以修改value数组值的setter方法，所以可以认为String对象是不可变的。 可能会发现String类还提供了很多方法可以修改字符串值的方法：substring , concat , replace , replaceAll 等等。 比如 substring 方法： 12345public static void main(String[] args) { String s = &quot;abcd&quot;; System.out.println(s); System.out.println(s.substring(1));} 执行后输出结果： 12abcdbcd 可以看到s的值确实是变了。这解释起来也很容易，看 substring 方法实现就明白了： 12345678910public String substring(int beginIndex) { if (beginIndex &lt; 0) { throw new StringIndexOutOfBoundsException(beginIndex); } int subLen = value.length - beginIndex; if (subLen &lt; 0) { throw new StringIndexOutOfBoundsException(subLen); } return (beginIndex == 0) ? this : new String(value, beginIndex, subLen);} 源码并不是去修改的 value 数组里的值，而是重新创建一个新的对象，并把新的对象引用赋值给s。其它修改字符串值的方法也都一样，都是创建新的对象返回。 String的值真的不可变吗？value变量是 private final 的，也就是初始化后不可修改。它是引用变量，虽然不能重新指向其它堆内存地址，但可以通过反射去修改堆里的值。 12345678public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException { String s = &quot;abcd&quot;; Field field = String.class.getDeclaredField(&quot;value&quot;); field.setAccessible(true); char[] chars = (char[]) field.get(s); chars[1] = '%'; System.out.println(s);} 输出结果为： 1a%cd 这个反射的实例还可以说明一个问题：如果一个对象，他组合的其他对象的状态是可以改变的，那么这个对象很可能不是不可变对象。例如一个Car对象，它组合了一个Wheel对象，虽然这个Wheel对象声明成了private final 的，但是这个Wheel对象内部的状态可以改变， 那么就不能很好的保证Car对象不可变。 JDK6与JDK7中String类实现的区别JDK 6 String是通过字符数组实现的。在jdk 6 中，String类包含三个成员变量：char value[]， int offset，int count。他们分别用来存储真正的字符数组，数组的第一个位置索引以及字符串中包含的字符个数。 下面是jdk1.6中的源码: 12345678910111213141516171819202122232425262728293031323334353637public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence{ /** The value is used for character storage. */ private final char value[]; /** The offset is the first index of the storage that is used. */ private final int offset; /** The count is the number of characters in the String. */ private final int count; /** Cache the hash code for the string */ private int hash; // Default to 0 // JDK 6 // Package private constructor which shares value array for speed. String(int offset, int count, char value[]) { this.value = value; this.offset = offset; this.count = count; } // substring public String substring(int beginIndex, int endIndex) { //check boundary return new String(offset + beginIndex, endIndex - beginIndex, value); } // concat public String concat(String str) { return new String(0, count + otherLen, buf); } // 其它修改字符串的方法} 当调用substring,concat等方法的时候，会创建一个新的string对象，但是这个string的值仍然指向堆中的同一个字符数组。这两个对象中只有count和offset 的值是不同的 JDK 6中的substring导致的问题如果你有一个很长很长的字符串，但是当你使用substring进行切割的时候你只需要很短的一段。这可能导致性能问题，因为你需要的只是一小段字符序列，但是你却引用了整个字符串（因为这个非常长的字符数组一直在被引用，所以无法被回收，就可能导致内存泄露）。在JDK 6中，一般用以下方式来解决该问题，原理其实就是生成一个新的字符串并引用他。 1x = x.substring(x, y) + &quot;&quot; 内存泄露：在计算机科学中，内存泄漏指由于疏忽或错误造成程序未能释放已经不再使用的内存。 内存泄漏并非指内存在物理上的消失，而是应用程序分配某段内存后，由于设计错误，导致在释放该段内存之前就失去了对该段内存的控制，从而造成了内存的浪费。 concat也同样有该问题，如果对象a只需要很小一段字符序列，对象b去拼接了很长的字符串，这就导致如果a对象没被回收，这个很长很长的字符数组就一直不会被释放。 JDK 7上面提到的问题，在jdk 7中得到解决。在jdk 7 中，substring，concat等方法会在堆内存中创建一个新的数组。 Java源码中关于这部分的主要代码如下： 1234567891011public String substring(int beginIndex, int endIndex) { // check boundary int subLen = endIndex - beginIndex; return ((beginIndex == 0) &amp;&amp; (endIndex == value.length)) ? this : new String(value, beginIndex, subLen);}public String(char value[], int offset, int count) { // check boundary this.value = Arrays.copyOfRange(value, offset, offset+count);} 以上是JDK 7中的subString方法，其使用new String创建了一个新字符串，避免对老字符串的引用。从而解决了内存泄露问题。 所以，如果你的生产环境中使用的JDK版本小于1.7，当你使用String的subString方法时一定要注意，避免内存泄露。 编译器对String字符串拼接的优化 String s = “a” + “b”，编译器会进行常量折叠(因为两个都是编译期常量，编译期可知)，即变成 String s = “ab” 对于能够进行优化的(String s = “a” + var 等)用 StringBuilder 的 append() 方法替代，最后调用 toString() 方法 (底层就是一个 new String()) 字符串拼接的几种方式和区别字符串不变性与字符串拼接其实，所有的所谓字符串拼接，都是重新生成了一个新的字符串。下面一段字符串拼接代码： 12String s = &quot;abcd&quot;;s = s.concat(&quot;ef&quot;); 其实最后我们得到的s已经是一个新的字符串了。如下图 s中保存的是一个重新创建出来的String对象的引用。 1.使用 + 拼接字符串在Java中，拼接字符串最简单的方式就是直接使用符号 + 来拼接。如： 123String a = &quot;a&quot;;String b = &quot;b&quot;;String c = a + &quot;,&quot; + b; 其实使用 + 拼接字符串，只是Java提供的一个语法糖， 那么，我们就来解一解这个语法糖，看看他的内部原理到底是如何实现的。 上面代码经过反编译后如下： 123String a = &quot;a&quot;;String b = &quot;b&quot;;String c = new StringBuilder().append((String)a).append((String)&quot;,&quot;).append((String)b).toString(); 通过查看反编译以后的代码，我们可以发现，原来字符串常量在拼接过程中，是将String转成了StringBuilder后，使用其append方法进行处理的。 那么也就是说，Java中的 + 对字符串的拼接，其实现原理是使用 StringBuilder.append。 2.concat还可以使用String类中的方法concat方法来拼接字符串。如： 123String a = &quot;a&quot;;String b = &quot;b&quot;;String c = a.concat(&quot;,&quot;).concat(b); 我们再来看一下concat方法的源代码，看一下这个方法又是如何实现的。 12345678910public String concat(String str) { int otherLen = str.length(); if (otherLen == 0) { return this; } int len = value.length; char buf[] = Arrays.copyOf(value, len + otherLen); str.getChars(buf, len); return new String(buf, true);} 这段代码首先创建了一个字符数组，长度是已有字符串和待拼接字符串的长度之和，再把两个字符串的值复制到新的字符数组中，并使用这个字符数组创建一个新的String对象并返回。 通过源码我们也可以看到，经过concat方法，其实是new了一个新的String，这也就呼应到前面我们说的字符串的不变性问题上了。 3.StringBuffer和StringBuilder关于字符串，Java中除了定义了一个可以用来定义 字符串常量 的 String 类以外，还提供了可以用来定义 字符串变量 的 StringBuffer和 StringBuilder 类，它的对象是可以扩充和修改的。 使用 StringBuffer 和 StringBuilder 可以方便的对字符串进行拼接。如： 1234567StringBuffer buffer = new StringBuffer(&quot;a&quot;);String b = &quot;b&quot;;String c = buffer.append(&quot;,&quot;).append(b).toString();StringBuilder builder = new StringBuilder(&quot;a&quot;);String b = &quot;b&quot;;String c = builder.append(&quot;,&quot;).append(b).toString(); 接下来我们看看 StringBuffer和 StringBuilder 的实现原理。 和 String 类类似， StringBuffer和 StringBuilder 类也封装了一个字符数组，定义如下： 1char[] value; 其append源码如下： 1234public StringBuilder append(String str) { super.append(str); return this;} 该类继承了AbstractStringBuilder类，看下其append方法： 123456789public AbstractStringBuilder append(String str) { if (str == null) return appendNull(); int len = str.length(); ensureCapacityInternal(count + len); str.getChars(0, len, value, count); count += len; return this;} append会直接拷贝字符到内部的字符数组中，如果字符数组长度不够，会进行扩展。 StringBuffer 和 StringBuilder 类似，最大的区别就是 StringBuffer 是线程安全的，看一下 StringBuffer 的append方法。 12345public synchronized StringBuffer append(String str) { toStringCache = null; super.append(str); return this;} 该方法使用 synchronized 进行声明，说明是一个线程安全的方法。而 StringBuilder 则不是线程安全的。 常用的字符串拼接方式有五种，分别是使用+、使用concat、使用StringBuilder、使用StringBuffer以及使用StringUtils.join。 由于字符串拼接过程中会创建新的对象，所以如果要在一个循环体中进行字符串拼接，就要考虑内存问题和效率问题。 因此，经过对比，我们发现，直接使用StringBuilder的方式是效率最高的。因为StringBuilder天生就是设计来定义可变字符串和字符串的变化操作的。 但是，还要强调的是： 1、如果不是在循环体中进行字符串拼接的话，直接使用+就好了。 2、如果在并发场景中进行字符串拼接的话，要使用StringBuffer来代替StringBuilder。 switch对String的支持Java 7中，switch的参数可以是String类型了。到目前为止switch支持这样几种数据类型：byte short int char String 还是先上代码： 12345678910111213public static void main(String[] args) { String str = &quot;world&quot;; switch (str) { case &quot;hello&quot;: System.out.println(&quot;hello&quot;); break; case &quot;world&quot;: System.out.println(&quot;world&quot;); break; default: break; }} 对代码进行反编译： 123456789101112131415161718public static void main(String args[]){ String str = &quot;world&quot;; String s; switch((s = str).hashCode()) { default: break; case 99162322: if(s.equals(&quot;hello&quot;)) System.out.println(&quot;hello&quot;); break; case 113318802: if(s.equals(&quot;world&quot;)) System.out.println(&quot;world&quot;); break; }} 看到这个代码，你知道原来字符串的switch是通过equals()和hashCode()方法来实现的。记住，switch中只能使用整型，比如byte。short，char(ackii码是整型)以及int。还好hashCode()方法返回的是int，而不是long。通过这个很容易记住hashCode返回的是int这个事实。仔细看下可以发现，进行switch的实际是哈希值，然后通过使用equals方法比较进行安全检查，这个检查是必要的，因为哈希可能会发生碰撞。因此它的性能是不如使用枚举进行switch或者使用纯整数常量，但这也不是很差。因为Java编译器只增加了一个equals方法，如果你比较的是字符串字面量的话会非常快，比如”abc” ==”abc”。如果你把hashCode()方法的调用也考虑进来了，那么还会再多一次的调用开销，因为字符串一旦创建了，它就会把哈希值缓存起来。因此如果这个switch语句是用在一个循环里的，比如逐项处理某个值，或者游戏引擎循环地渲染屏幕，这里hashCode()方法的调用开销其实不会很大。 参考资料定义不可变对象的策略 为什么String在java中是不可变的？","link":"/2021/02/14/2021-02-14-string/"},{"title":"String字符串的长度限制","text":"字符串的长度限制最近和第三方进行系统对接，约定是我们这边将文件的转成BASE64编码后，进行传输。对方反馈将BASE64字符串转成文件后，文件损坏打不开，怀疑是BASE64字符串的问题。我在本地尝试将base64字符串嵌入到img标签，图片正常显示，说明字符串是没问题的。然后尝试将字符串保存为文件： 12String a = &quot;data:image/jpg;base64,/9j/4AAQSkZJRgABAQAASABIAAD/4QBARXhpZgAA......&quot;; // 7w多个字符// 省略转文件代码... 点击运行时，在编译阶段提示 字符串常量 过长提示报错： 1Error:(6, 20) java: constant string too long 常量池限制在 Java语言规范 3.10.5 中定义，被 &quot;&quot; 括起来的字符称为 字符串字面常量，通常叫它字面量。字面量会被放到 字符串常量池 中。 在.java文件编译成.class的过程中，编译器会进行类型检查，数值范围检查等一系列的检查操作。并且编译成的字节码文件也是有一定格式的，才能被虚拟机正确的执行。 根据 Java虚拟机规范 4.4 常量池中定义，CONSTANT_String_info 用于表示 java.lang.String 类型的常量对象，格式如下： 1234CONSTANT_String_info { u1 tag; u2 string_index;} 其中，string_index 项的值必须是对常量池的有效索引， 常量池在该索引处的项必须是 CONSTANT_Utf8_info 结构，表示一组 Unicode 码点序列，这组 Unicode 码点序列最终会被初始化为一个 String 对象。 CONSTANT_Utf8_info 结构用于表示字符串常量的值： 12345CONSTANT_Utf8_info { u1 tag; u2 length; u1 bytes[length];} 其中，length 代表数组 bytes[] 的长度，其类型为u2，bytes[] 是表示字符串值的byte数组。 在规范 Java虚拟机规范 4 中说明：u2表示两个字节的无符号数，那么1个字节有8位，2个字节就有16位。16位无符号数可表示的最大值位2^16 - 1 = 65535。在 Java虚拟机规范 4.11 中也说明：字段和方法名称、字段和方法描述符以及其他常量字符串值（包括由ConstantValue（§4.7.2）属性引用的值）的长度被常量信息结构（§4.4.7）的16位无符号长度项限制为65535个字符。 也就是说，Class文件中常量池的格式规定了，其字符串常量的长度不能超过65535。 尝试定义长度为65535的字符串，来证实以上： 1234public static void main(String[] args) { String a = &quot;a111111111111111111111111111111111111111111111111111111.............&quot;; // 65535个 System.out.println(a);} 尝试使用javac编译，同样会得到”错误: 常量字符串过长”，那么原因是什么呢？ 其实在编译时是有对字符串常量长度进行检查的。在javac代码中可以找到，具体在 Gen 类 checkStringConstant 方法： 123456private void checkStringConstant(DiagnosticPosition var1, Object var2) { if (this.nerrs == 0 &amp;&amp; var2 != null &amp;&amp; var2 instanceof String &amp;&amp; ((String)var2).length() &gt;= 65535) { this.log.error(var1, &quot;limit.string&quot;, new Object[0]); ++this.nerrs; }} 可以看到当类型为String，并且长于 &gt;= 65535 时，就会导致编译失败。尝试65534个字符，则可以正常编译通过。 那为什么是 65534 ，而不是 65535 呢？在 Java虚拟机规范 也有做解释： if the Java Virtual Machine code for a method is exactly 65535 bytes long and ends with an instruction that is 1 byte long, then that instruction cannot be protected by an exception handler. A compiler writer can work around this bug by limiting the maximum size of the generated Java Virtual Machine code for any method, instance initialization method, or static initializer (the size of any code array) to 65534 bytes. 这其实是为了弥补java虚拟机的一个bug，所以将长度限制为65534 运行时限制上面提到的这种String长度的限制是编译期的限制，也就是使用String s= “”; 这种字面值方式定义的时候才会有的限制。 String在运行期有没有限制呢，答案是有的，进入String类源码。里面有许多重载的构造方法，其中有的是支持用户输入长度的 12345public String(byte bytes[], int offset, int length) public int length() { return value.length;} length 使用 int 类型来定义的，也就说 length 最大长度也就是 Integer.MAX_VALUE。 int 是一个 32 位变量类型，取正数部分来算的话，他们最长可以有 2^31-1 = 2147483647 个 。这个值约等于4G，在运行期，如果String的长度超过这个范围，就可能会抛出异常。(在jdk 1.9之前） 1234567-2^31 ~ 2^31-1 = -2147483648 ~ 2147483647 个 16-bit Unicodecharacter2147483647 * 16 = 34359738352 位34359738352 / 8 = 4294967294 (Byte)4294967294 / 1024 = 4194303.998046875 (KB)4194303.998046875 / 1024 = 4095.9999980926513671875 (MB)4095.9999980926513671875 / 1024 = 3.99999999813735485076904296875 (GB) 总结字符串有长度限制，在编译期，要求字符串常量池中的常量不能超过65535，并且在javac执行过程中控制了最大值为65534。 在运行期，长度不能超过Int的范围，否则会抛异常","link":"/2021/02/27/2021-02-27-length-limit-on-string/"},{"title":"Java中各种关键字","text":"transientJava语言的关键字，变量修饰符，如果用transient声明一个实例变量，当对象存储时，它的值不需要维持。 这里的对象存储是指，Java的serialization提供的一种持久化对象实例的机制。当一个对象被序列化的时候，transient型变量的值不包括在序列化的表示中，然而非transient型的变量是被包括进去的。使用情况是：当持久化对象时，可能有一个特殊的对象数据成员，我们不想用serialization机制来保存它。为了在一个特定对象的一个域上关闭serialization，可以在这个域前加上关键字transient。 简单点说，就是被transient修饰的成员变量，在序列化的时候其值会被忽略，在被反序列化后， transient 变量的值被设为初始值， 如 int 型的是 0，引用类型的是 null。 ArrayList 中的保存数据的数组被 transient 所修饰。 1transient Object[] elementData; // non-private to simplify nested class access 如果如上面的所说，被 transient 修饰的变量值在序列化、反序列化后，列表的数据将会为空： 1234567891011121314151617public static void main(String[] args) throws IOException, ClassNotFoundException { ArrayList&lt;String&gt; arrayList = new ArrayList&lt;&gt;(); arrayList.add(&quot;2&quot;); arrayList.add(&quot;3&quot;); FileOutputStream fileOut = new FileOutputStream(&quot;./array.txt&quot;); ObjectOutputStream out = new ObjectOutputStream(fileOut); out.writeObject(arrayList); out.close(); fileOut.close(); FileInputStream fileIn = new FileInputStream(&quot;./array.txt&quot;); ObjectInputStream in = new ObjectInputStream(fileIn); ArrayList&lt;String&gt; arrayList1 = (ArrayList&lt;String&gt;) in.readObject(); System.out.println(arrayList1.toString()); in.close(); fileIn.close();} 执行后输出： 1[2, 3] 数据并没有被设为初始值，这是因为 ArrayList 中定义了 writeObject 和 readObject 方法，实现了自定义序列化。序列化的时候ObjectStream会判断类中有没有自定义序列化方法？如果有，使用自定义序列化方法，否则使用默认的序列化方法。进一步了解 序列化、反序列化 如果是默认的序列化方法是不会序列化 transient 字段的： 定义实体类： 123456789public class Person implements Serializable { private String name; private transient int age; private transient String[] tags;} 12345678910111213141516171819public static void main(String[] args) throws IOException, ClassNotFoundException { Person person = new Person(); person.setAge(11); person.setName(&quot;ddmcc&quot;); person.setTags(new String[]{&quot;1&quot;, &quot;2&quot;}); FileOutputStream fileOut = new FileOutputStream(&quot;./object.txt&quot;); ObjectOutputStream out = new ObjectOutputStream(fileOut); System.out.println(&quot;序列化前：&quot; + person.toString()); out.writeObject(person); out.close(); fileOut.close(); FileInputStream fileIn = new FileInputStream(&quot;./object.txt&quot;); ObjectInputStream in = new ObjectInputStream(fileIn); Person person1 = (Person) in.readObject(); System.out.println(&quot;序列化后：&quot; + person1.toString()); in.close(); fileIn.close();} 输出结果为： 12序列化前：Person{name='ddmcc', age=11, tags=[1, 2]}序列化后：Person{name='ddmcc', age=0, tags=null} transient 变量的值被设为初始值，int 型的是 0，对象型的是 null instanceofinstanceof 是 Java 的保留关键字。它的作用是测试它左边的对象是否是它右边的类的实例，返回 boolean 的数据类型。 Java语言规范关于 instanceof 关键字的解释： 举个🌰： 1234567891011if (a instanceof b) {}if (o instanceof Vector) { System.out.println(&quot;对象是 java.util.Vector 类的实例&quot;);} else if (o instanceof ArrayList) { System.out.println(&quot;对象是 java.util.ArrayList 类的实例&quot;);} else { System.out.println(&quot;对象是 &quot; + o.getClass() + &quot; 类的实例&quot;);} a的类型必须是 引用类型 或 空类型(null)，否则就会产生编译时错误。 如果b表示的是不可具化的引用类型，也是一个编译时错误 如果 a 强转成 b 作为编译时错误，那么 a instanceof b 关系表达式也就同样的产生编译时错误。 运行时 a 不为 null，并且a可以被强转成b而不会产生 ClassCastException ，那么 instanceof 操作符的结果是 true，否则 false instanceof、isInstance和isAssignableFrom的区别final使用 final 可以定义 ：变量、方法、类 final类如果把任何一个类声明为 final，则不能有子类 final方法如果任何方法声明为 final，则不能覆盖它。即不能被重写 final变量变量可以被声明为 final ，而 final 变量只能被赋值一次。如果对 final 变量赋值，除非在赋值之前该变量是明确未赋值的，否则就是一种编译时错误。 一旦 final 变量被赋值，那么它就始终持有同一个值。如果一个 final 变量持有的是对象的引用，那么该对象的状态可以被修改，但是该变量会始终指向这个对象。这条规则也同样试用于数组，因为数组也是对象。我们可以修改数组的元素，但是该变量会始终指向该数组。 staticJava的静态形式有5中类型：静态变量、静态方法、静态块、内部静态类和静态接口方法（Java8以上支持） 静态变量我们用 static 表示变量的级别，一个类中的静态变量，不属于类的对象或者实例。因为该类所有的对象实例共享静态变量，因此他们不具线程安全性。 通常，静态变量常用 final 关键字来修饰，表示通用资源或可以被所有的对象所使用。如果静态变量未被私有化，可以用 类名.变量名 的方式来使用。 静态方法与静态变量一样，静态方法是属于类而不是实例。 在静态方法中只能使用静态变量和调用静态方法。通常静态方法用于想给其他的类使用而不需要创建实例。例如：Collections。 Java的包装类和实用类包含许多静态方法。main() 方法就是Java程序入口点，是静态方法。 从Java8以上版本开始接口也可以有静态方法了 静态代码块Java的静态代码块是一组指令，类装载的时候在内存中由Java ClassLoader执行。 静态块常用于初始化类的静态变量。大多时候还用于在类装载时候创建静态资源。 不允许在静态代码块中使用非静态变量。一个类中可以有多个静态块。静态块只在类装载入内存时，执行一次。 12345static { // 在类装载的时候初始化一些资源，如：jdbc驱动等 // 只能访问静态变量和静态方法} 静态类Java可以嵌套使用静态类，但是静态类不能用于嵌套的顶层。 让我们来看一个使用静态的样例程序： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class StaticExample { //static block static{ //can be used to initialize resources when class is loaded System.out.println(&quot;StaticExample static block&quot;); //can access only static variables and methods str=&quot;Test&quot;; setCount(2); } //multiple static blocks in same class static{ System.out.println(&quot;StaticExample static block2&quot;); } //static variable example private static int count; //kept private to control it's value through setter public static String str; public int getCount() { return count; } //static method example public static void setCount(int count) { if(count &gt; 0) { StaticExample.count = count; } } //static util method public static int addInts(int i, int...js){ int sum=i; for(int x : js) { sum+=x; } return sum; } //static class example - used for packaging convenience only public static class MyStaticClass{ public int count; } 接下来，在测试程序中使用这些静态变量、静态方法和静态类。 1234567891011121314151617181920212223 public static void main(String[] args) { StaticExample.setCount(5); //non-private static variables can be accessed with class name StaticExample.str = &quot;abc&quot;; StaticExample se = new StaticExample(); System.out.println(se.getCount()); //class and instance static variables are same System.out.println(StaticExample.str +&quot; is same as &quot;+se.str); System.out.println(StaticExample.str == se.str); //static nested classes are like normal top-level classes StaticExample.MyStaticClass myStaticClass = new StaticExample.MyStaticClass(); myStaticClass.count=10; StaticExample.MyStaticClass myStaticClass1 = new StaticExample.MyStaticClass(); myStaticClass1.count=20; System.out.println(myStaticClass.count); System.out.println(myStaticClass1.count); }} 执行结果如下： 1234567StaticExample static blockStaticExample static block25abc is same as abctrue1020 可见，静态块代码是最先被执行的，而且是只在类载入内存时执行。 静态import一般，Java允许用静态成员使用类引用，从Java1.5开始，我们可以使用静态import而不用类引用。下面是一个简单的静态import的例子。 12345678public class A { public static int MAX = 1000; public static void foo(){ System.out.println(&quot;foo static method&quot;); }} 1234567891011import static A.MAX;import static A.foo;public class B { public static void main(String args[]){ System.out.println(MAX); //normally A.MAX foo(); // normally A.foo() }} 第2段代码用了import语句，导入静态类使用import static，后面跟着的是静态成员的全称。 为了导入一个类中的所有的静态成员，可以这样写“import static A.*”，这只有在我们要多次使用一个类的静态变量时，才这样写，但这种写法的可读性不好。 synchronizedvolatile","link":"/2021/03/03/2021-03-03-java-keywords/"},{"title":"常用计算机词汇表","text":"常用计算机词汇表… A 英文 译法 1 译法 2 译法 3 a block of pointers 一块指针 一组指针 abbreviation 缩略语 abstract 抽象的 abstract syntax tree, AST 抽象语法树 abstraction 抽象 abstraction barrier 抽象屏障 抽象阻碍 abstraction of function calls 函数调用抽象 access 访问 存取 access function 访问函数 存取函数 accumulator 累加器 activate 激活 ad hoc 专设 adapter 适配器 address 地址 algebraic data type 代数数据类型 algorithm 算法 alias 别名 allocate 分配 配置 alternative 备选 amortized analysis 平摊分析 anaphoric 指代 annotation 注解 anonymous function 匿名函数 antecedent 前提 前件 先决条件 append 追加 拼接 application 应用 应用程序 application framework 应用框架 application program interface, API 应用程序编程接口 application service provider, ASP 应用程序服务提供商 applicative 应用序 argument 参数 自变量 实际参数/实参 arithmetic 算术 array 数组 artificial intelligence, AI 人工智能 assemble 组合 assembly 汇编 assignment 赋值 assignment operator 赋值操作符 associated 关联的 association list, alist 关联列表 atom 原子 atomic 原子的 atomic value 原子型值 attribute 属性 特性 augmented 扩充 automatic memory management 自动内存管理 automatically infer 自动推导 autometa theory 自动机理论 auxiliary 辅助 B 英文 译法 1 译法 2 译法 3 backquote 反引用 backtrace 回溯 backward compatible 向下兼容 bandwidth 带宽 base case 基本情形 base class 基类 Bayes’ theorem 贝叶斯定理 best viable function 最佳可行函式 最佳可行函数 Bezier curve 贝塞尔曲线 bignum 大数 binary operator 二元操作符 binary search 二分查找 二分搜索 二叉搜索 binary search tree 二叉搜索树 binary tree 二叉树 binding 绑定 binding vector 绑定向量 bit 位 比特 bit manipulation 位操作 black box abstraction 黑箱抽象 block 块 区块 block structure 块结构 区块结构 block name 代码块名字 Blub paradox Blub 困境 body 体 主体 boilerplate 公式化 样板 bookkeeping 簿记 boolean 布尔 border 边框 bottom-up design 自底向上的设计 bottom-up programming 自底向上编程 bound 边界 bounds checking 边界检查 box notation 箱子表示法 brace 花括弧 花括号 bracket 方括弧 方括号 branch 分支 跳转 breadth-first 广度优先 breadth-first search, BFS 广度优先搜索 breakpoint 断点 brevity 简洁 buffer 缓冲区 buffer overflow attack 缓冲区溢出攻击 bug 臭虫 building 创建 built-in 内置 byte 字节 bytecode 字节码 C 英文 译法 1 译法 2 译法 3 cache 缓存 call 调用 callback 回调 CamelCase 驼峰式大小写 candidate function 候选函数 capture 捕捉 case 分支 character 字符 checksum 校验和 child class 子类 choke point 滞塞点 chunk 块 circular definition 循环定义 clarity 清晰 class 类 类别 class declaration 类声明 class library 类库 client 客户 客户端 clipboard 剪贴板 clone 克隆 closed world assumption 封闭世界假定 closure 闭包 clutter 杂乱 code 代码 code bloat 代码膨胀 collection 收集器 复合类型 column 行 栏 column-major order 行主序 comma 逗号 command-line 命令行 command-line interface, CLI 命令行界面 Common Lisp Object System, CLOS Common Lisp 对象系统 Common Gateway Interface, CGI 通用网关接口 compatible 兼容 compilation 编译 compilation parameter 编译参数 compile 编译 compile inline 内联编译 compile time 编译期 compiled form 编译后的形式 compiler 编译器 complex 复杂 complexity 复杂度 compliment 补集 component 组件 composability 可组合性 composition 组合 组合函数 compound value 复合数据 复合值 compression 压缩 computation 计算 computer 计算机 concatenation 串接 concept 概念 concrete 具体 concurrency 并发 concurrent 并发 conditional 条件式 conditional variable 条件变量 configuration 配置 connection 连接 cons 构造 cons cell 构元 cons 单元 consequent 结果 推论 consistent 一致性 constant 常量 constraint 约束 constraint programming 约束式编程 container 容器 content-based filtering 基于内容的过滤 context 上下文 语境 环境 continuation 延续性 continuous integration, CI 持续集成 control 控件 cooperative multitasking 协作式多任务 copy 拷贝 corollary 推论 coroutine 协程 corruption 程序崩溃 crash 崩溃 create 创建 crystallize 固化 curly 括弧状的 curried 柯里的 currying 柯里化 cursor 光标 curvy 卷曲的 cycle 周期 D 英文 译法 1 译法 2 译法 3 dangling pointer 迷途指针 野指针 Defense Advanced Research Projects Agency, DARPA 美国国防部高级研究计划局 data 数据 data structure 数据结构 data type 数据类型 data-driven 数据驱动 database 数据库 database schema 数据库模式 datagram 数据报文 dead lock 死锁 debug 调试 debugger 调试器 debugging 调试 declaration 声明 declaration forms 声明形式 declarative 声明式 说明式 declarative knowledge 声明式知识 说明式知识 declarative programming 声明式编程 说明式编程 declarativeness 可声明性 declaring 声明 deconstruction 解构 deduction 推导 推断 default 缺省 默认 defer 推迟 deficiency 缺陷 不足 define 定义 definition 定义 delegate 委托 delegation dellocate 释放 demarshal 散集 deprecated 废弃 depth-first 深度优先 depth-first search, BFS 深度优先搜索 derived 派生 derived class 派生类 design pattern 设计模式 designator 指示符 destructive 破坏性的 destructive function 破坏性函数 destructuring 解构 device driver 硬件驱动程序 dimensions 维度 directive 指令 directive 指示符 directory 目录 disk 盘 dispatch 分派 派发 distributed computing 分布式计算 DLL hell DLL 地狱 document 文档 dotted list 点状列表 dotted-pair notation 带点尾部表示法 带点尾部记法 duplicate 复本 dynamic binding 动态绑定 dynamic extent 动态范围 dynamic languages 动态语言 dynamic scope 动态作用域 dynamic type 动态类型 E 英文 译法 1 译法 2 译法 3 effect 效果 efficiency 效率 efficient 高效 elaborate elucidating embedded language 嵌入式语言 emulate 仿真 encapsulation 封装 enum 枚举 enumeration type 枚举类型 enumrators 枚举器 environment 环境 equal 相等 equality 相等性 equation 方程 equivalence 等价性 error message 错误信息 error-checking 错误检查 escaped 逃脱 溢出 escape character 转义字符 evaluate 求值 评估 evaluation 求值 event 事件 event driven 事件驱动 exception 异常 exception handling 异常处理 exception specification 异常规范 exit 退出 expendable 可扩展的 explicit 显式 exploratory programming 探索式编程 export 导出 引出 expression 表达式 expressive power 表达能力 extensibility 可扩展性 extent 范围 程度 external representation 外部表示法 extreme programming 极限编程 F 英文 译法 1 译法 2 译法 3 factorial 阶乘 family （类型的）系 feasible 可行的 feature 特色 field 字段 栏位 file 文件 file handle 文件句柄 fill pointer 填充指针 fineo-grained 细粒度 firmware 固件 first-class 第一类的 第一级的 一等的 first-class function 第一级函数 第一类函数 一等函数 first-class object 第一类的对象 第一级的对象 一等公民 fixed-point 不动点 fixnum 定长数 定点数 flag 标记 flash 闪存 flexibility 灵活性 floating-point 浮点数 floating-point notation 浮点数表示法 flush 刷新 fold 折叠 font 字体 force 迫使 form 形式 form 表单 formal parameter 形参 formal relation 形式关系 forward 转发 forward references fractal 分形 fractions 派系 framework 框架 freeware 自由软件 function 函数 function literal 函数字面常量 function object 函数对象 functional arguments 函数型参数 functional programming 函数式编程 functionality 功能性 G 英文 译法 1 译法 2 译法 3 game 游戏 garbage 垃圾 garbage collection 垃圾回收 garbage collector 垃圾回收器 generalized 泛化 generalized variable 广义变量 generate 生成 generator 生成器 generic 通用的 泛化的 generic algorithm 通用算法 泛型算法 generic function 通用函数 generic programming 通用编程 泛型编程 genrative programming 生产式编程 global 全局的 global declaration 全局声明 glue program 胶水程序 goto 跳转 graphical user interface, GUI 图形用户界面 greatest common divisor 最大公因数 Greenspun’s tenth rule 格林斯潘第十定律 H 英文 译法 1 译法 2 译法 3 hack 破解 hacker 黑客 handle 处理器 处理程序 句柄 hard disk 硬盘 hard-wire hardware 硬件 hash tables 哈希表 散列表 header 头部 header file 头文件 heap 堆 helper 辅助函数 辅助方法 heuristic 启发式 high-order 高阶 higher-order function 高阶函数 higher-order procedure 高阶过程 hyperlink 超链接 HyperText Markup Language, HTML 超文本标记语言 HyperText Transfer Protocol, HTTP 超文本传输协议 I 英文 译法 1 译法 2 译法 3 identical 一致 identifier 标识符 identity 同一性 ill type 类型不正确 illusion 错觉 imperative 命令式 imperative programming 命令式编程 implement 实现 implementation 实现 implicit 隐式 import 导入 incremental testing 增量测试 indent 缩排 缩进 indentation 缩排 缩进 indented 缩排 缩进 indention 缩排 缩进 infer 推导 infinite loop 无限循环 infinite recursion 无限递归 infinite precision 无限精度 infix 中序 information 信息 information technology, IT 信息技术 inheritance 继承 initialization 初始化 initialize 初始化 inline 内联 inline expansion 内联展开 inner class 内嵌类 inner loop 内层循环 input 输入 instances 实例 instantiate 实例化 instructive 教学性的 instrument 记录仪 integer 整数 integrate 集成 interactive language 交互式语言 interactive programming environment 交互式编程环境 interactive testing 交互式测试 interacts 交互 interface 接口 intermediate form 过渡形式 中间形式 internal 内部 internet 互联网 因特网 interpolation 插值 interpret 解释 interpreter 解释器 interrupt 中止 中断 intersection 交集 inter-process communication, IPC 进程间通信 invariants 约束条件 invoke 调用 item 项 iterate 迭代 iteration 迭代的 iterative 迭代的 iterator 迭代器 J 英文 译法 1 译法 2 译法 3 jagged 锯齿状的 job control language, JCL 作业控制语言 judicious 明智的 K 英文 译法 1 译法 2 译法 3 kernel 核心 kernel language 核心语言 keyword argument 关键字参数 keywords 关键字 kludge 蹩脚 L 英文 译法 1 译法 2 译法 3 lambda calculus lambda 演算 larval startup 雏形创业公司 laser 激光 latitude layout 版型 lazy 惰性 lazy evaluation 惰性求值 legacy software 历史遗留软件 leverage 杠杆 (动词)利用 lexical 词法的 lexical analysis 词法分析 lexical closure 词法闭包 lexical scope 词法作用域 Language For Smart People, LFSP 聪明人的语言 library 库 函数库 函式库 lifetime 生命期 linear iteration 线性迭代 linear recursion 线性递归 link 链接 连接 linker 连接器 list 列表 list operation 列表操作 literal 字面 literal constant 字面常量 literal representation 字面量 load 装载 加载 loader 装载器 加载器 local 局部的 局域的 local declarations 局部声明 local function 局部函数 局域函数 local variable 局部变量 局域变量 locality 局部性 loop 循环 lvalue 左值 M 英文 译法 1 译法 2 译法 3 machine instruction 机器指令 machine language 机器语言 machine language code 机器语言代码 machine learning 机器学习 macro 宏 mailing list 邮件列表 mainframes 大型机 maintain 维护 manifest typing 显式类型 manipulator 操纵器 mapping 映射 mapping functions 映射函数 marshal 列集 math envy 对数学家的妒忌 member 成员 memorizing 记忆化 memory 内存 memory allocation 内存分配 memory leaks 内存泄漏 menu 菜单 message 消息 message-passing 消息传递 meta- 元- meta-programming 元编程 metacircular 元循环 method 方法 method combination 方法组合 方法组合机制 micro 微 middleware 中间件 migration （数据库）迁移 minimal network 最小网络 mirror 镜射 mismatch type 类型不匹配 model 模型 modem 调制解调器 modifier 修饰符 modularity 模块性 module 模块 monad 单子 monkey patch 猴子补丁 monomorphic type language 单型语言 Moore’s law 摩尔定律 mouse 鼠标 multi-task 多任务 multiple values 多值 mutable 可变的 mutex 互斥锁 Multiple Virtual Storage, MVS 多重虚拟存储 N 英文 译法 1 译法 2 译法 3 namespace 命名空间 native 本地的 native code 本地码 natural language 自然语言 natural language processing 自然语言处理 nested 嵌套 nested class 嵌套类 network 网络 newline 换行 新行 non-deterministic choice 非确定性选择 non-strict 非严格 non-strict evaluation 非严格求值 nondeclarative nondestructive version 非破坏性的版本 number crunching 数字密集运算 O 英文 译法 1 译法 2 译法 3 object 对象 object code 目标代码 object-oriented 面向对象 object-oriented programming 面向对象编程 Occam’s razor 奥卡姆剃刀原则 on the fly 运行中 执行时 online 在线 open source 开放源码 operand 操作对象 operating system, OS 操作系统 operation 操作 operator 操作符 optimization 优化 optimization of tail calls 尾调用优化 option 选项 optional 可选的 选择性的 optional argument 选择性参数 ordinary 常规的 orthogonality 正交性 overflow 溢出 overhead 额外开销 overload 重载 override 覆写 P 英文 译法 1 译法 2 译法 3 package 包 pair 点对 palindrome 回文 paradigm 范式 parallel 并行 parallel computer 并行计算机 param 参数 parameter 参数 形式参数/形参 paren-matching 括号匹配 parent class 父类 parentheses 括号 Parkinson’s law 帕金森法则 parse 解析 parse tree 解析树 分析树 parser 解析器 partial application 部分应用 partial applied 分步代入的 partial function application 部分函数应用 particular ordering 部分有序 pass by adress 按址传递 传址 pass by reference 按引用传递 传引用 pass by value 按值传递 传值 path 路径 pattern 模式 pattern match 模式匹配 perform 执行 performance 性能 performance-critical persistence 持久性 phrenology 相面 physical 物理的 pipe 管道 pixel 像素 placeholder 占位符 planning 计画 platform 平台 pointer 指针 pointer arithmetic 指针运算 poll 轮询 polymorphic 多态 polymorphism 多态 polynomial 多项式的 pool 池 port 端口 portable 可移植性 portal 门户 positional parameters 位置参数 postfix 后序 precedence 优先级 precedence list 优先级列表 preceding 前述的 predicate 判断式 谓词 preemptive multitasking 抢占式多任务 premature design 过早设计 preprocessor 预处理器 prescribe 规定 prime 素数 primitive 原语 primitive recursive 主递归 primitive type 原生类型 principal type 主要类型 print 打印 printed representation 打印表示法 printer 打印机 priority 优先级 procedure 过程 procedurual 过程化的 procedurual knowledge 过程式知识 process 进程 process priority 进程优先级 productivity 生产力 profile 评测 profiler 评测器 性能分析器 programmer 程序员 programming 编程 programming language 编程语言 project 项目 prompt 提示符 proper list 正规列表 property 属性 property list 属性列表 protocol 协议 prototype 原型 pseudo code 伪码 pseudo instruction 伪指令 purely functional language 纯函数式语言 pushdown stack 下推栈 Q 英文 译法 1 译法 2 译法 3 qualified 修饰的 带前缀的 qualifier 修饰符 quality 质量 quality assurance, QA 质量保证 query 查询 query language 查询语言 queue 队列 quote 引用 quoted form 引用形式 R 英文 译法 1 译法 2 译法 3 race condition 条件竞争 竞态条件 radian 弧度 Redundant Array of Independent Disks, RAID 冗余独立磁盘阵列 raise 引起 random number 随机数 range 范围 区间 rank （矩阵）秩 排名 rapid prototyping 快速原型开发 rational database 关系数据库 raw 未经处理的 read 读取 read-evaluate-print loop, REPL 读取-求值-打印循环 read-macro 读取宏 record 记录 recursion 递归 recursive 递归的 recursive case 递归情形 refactor 重构 refer 参考 reference 引用 参考 referential transparency 引用透明 refine 精化 reflection 反射 映像 register 寄存器 registry creep 注册表蠕变 regular expression 正则表达式 represent 表现 request 请求 resolution 解析度 resolve 解析 rest parameter 剩余参数 return 返回 回车 return value 返回值 reuse of software 代码重用 right associative 右结合 Reduced Instruction Set Computer, RISC 精简指令系统计算机 robust 健壮 robustness 健壮性 鲁棒性 routine 例程 routing 路由 row-major order 列主序 remote procedure call, RPC 远程过程调用 run-length encoding 游程编码 run-time typing 运行期类型 runtime 运行期 rvalue 右值 S 英文 译法 1 译法 2 译法 3 S-expression S-表达式 save 储存 Secure Sockets Layer, SSL 安全套接字层 scaffold 脚手架 鹰架 scalar type 标量 scan 扫描 schedule 调度 scheduler 调度程序 scope 作用域 SCREAMING_SNAKE_CASE 尖叫式蛇底大写 screen 屏幕 scripting language 脚本语言 search 查找 搜寻 segment of instructions 指令片段 semantics 语义 semaphore 信号量 semicolon 分号 sequence 序列 sequential 循序的 顺序的 sequential collection literals serial 串行 serialization 序列化 series 串行 级数 server 服务器 shadowing 隐蔽了 sharp 犀利的 sharp-quote 升引号 shortest path 最短路径 SICP 《计算机程序的构造与解释》 side effect 副作用 signature 签名 simple vector 简单向量 simulate 模拟 Single Point of Truth, SPOT 真理的单点性 single-segment 单段的 sketch 草图 初步框架 slash 斜线 slot 槽 smart pointer 智能指针 snake_case 蛇底式小写 snapshot 屏幕截图 socket 套接字 software 软件 solution 方案 source code 源代码 space leak 内存泄漏 spaghetti 面条式代码 意面式代码 spaghetti stack 意面式栈 面条式栈 spam 垃圾邮件 spec 规格 special form 特殊形式 special variable 特殊变量 specialization 特化 specialize 特化 specialized array 特化数组 specification 规格说明 规范 splitter 切分窗口 sprite 精灵图 square 平方 square root 平方根 squash 碰撞 stack 栈 stack frame 栈帧 stakeholder standard library 标准函式库 state machine 状态机 statement 陈述 语句 static type 静态类型 static type system 静态类型系统 status 状态 store 保存 stream 流 strict 严格 strict evaluation 严格求值 string 字串 字符串 string template 字串模版 strong type 强类型 structural recursion 结构递归 structured values 结构型值 subroutine 子程序 subset 子集 substitution 代换 substitution model 代换模型 subtype 子类型 superclass 基类 superfluous 多余的 supertype 超集 support 支持 suspend 挂起 swapping values 交换变量的值 symbol 符号 symbolic computation 符号计算 syntax 语法 system administrator 系统管理员 system administrator disease 系统管理员综合症 System Network Architecture, SNA 系统网络体系 T 英文 译法 1 译法 2 译法 3 (database)table 数据表 table 表格 tag 标签 标记 tail-recursion 尾递归 tail-recursive 尾递归的 TAOCP 《计算机程序设计艺术》 target 目标 taxable operators 需节制使用的操作符 taxonomy 分类法 template 模版 temporary object 临时对象 testing 测试 text 文本 text file 文本文件 thread 线程 thread safe 线程安全 three-valued logic 三值逻辑 throw 抛出 丢掷 引发 throwaway program 一次性程序 timestamp 时间戳 token 词法记号 语义单位 语元 top-down design 自顶向下的设计 top-level 顶层 trace 追踪 trailing space 行尾空白 transaction 事务 transition network 转移网络 transparent 透明的 traverse 遍历 tree 树 tree recursion 树形递归 trigger 触发器 tuple 元组 Turing machine 图灵机 Turing complete 图灵完备 typable 类型合法 type 类型 type constructor 类构造器 type declaration 类型声明 type hierarchy 类型层级 type inference 类型推导 type name 类型名 type safe 类型安全 type signature 类型签名 type synonym 类型别名 type variable 类型变量 typing 类型指派 输入 U 英文 译法 1 译法 2 译法 3 user interface, UI 用户界面 unary 一元的 underflow 下溢 unification 合一 统一 union 并集 universally quantify 全局量化 unqualfied 未修饰的 unwinding uptime 运行时间 Uniform Resource Locator, URL 统一资源定位符 user 用户 utilities 实用函数 V 英文 译法 1 译法 2 译法 3 validate 验证 validator 验证器 value constructor 值构造器 vaporware 朦胧件 variable 变量 variable capture 变量捕捉 variadic input 可变输入 variant 变种 venture capitalist, VC 风险投资商 vector 向量 viable function 可行函数 video 视频 view 视图 virtual function 虚函数 virtual machine 虚拟机 virtual memory 虚内存 volatile 挥发 vowel 元音 W 英文 译法 1 译法 2 译法 3 warning message 警告信息 web server 网络服务器 weight 权值 权重 well type 类型正确 wildcard 通配符 window 窗口 word 单词 字 wrapper 包装器 包装 What You See Is What You Get, WYSIWYG 所见即所得 What You See Is What You Want, WYSIWYW 所见即所想 XY 英文 译法 1 译法 2 译法 3 Y combinator Y组合子 Z 英文 译法 1 译法 2 译法 3 Z-expression Z-表达式 zero-indexed 零索引的 专业名词 英文 译法 1 译法 2 译法 3 The Paradox of Choice 选择谬论","link":"/2020/08/11/2020-08-11-words/"},{"title":"mysql索引","text":"索引原理和字典、通讯录原理一样，索引的目的在于提高查询效率，通过不断的缩小数据的查找范围来筛选出最终想要的结果。比如要查找 “张三” 这个人，会先定位在姓 “张” 的人中，然后再从中查找 “三” 这个名。如果没有索引，那么可能需要把整个通讯录都翻一遍才能找到想要的。 在 mysql 中存储引擎用类似的方法使用索引，其先通过值在索引中找到匹配的索引记录，然后根据索引记录来找到对应的数据行。又因为是 B+ 树索引，按照顺序存储数据，所以可以用来做 ORDER BY 和 GROUP BY 操作。最后，因为索引存储了实际的列值，所以某些查询只使用索引就能够完成全部查询。据此特性总结索引有以下优点： 索引大大减少服务器需要扫描的数据量 索引可以帮助服务器避免排序和临时表 索引可以将随机IO变为顺序IO 这边要注意的是B+树索引并不能找到一个给定键值的具体行，能找到的只是被查找数据行所在的页。然后数据库通过把页从磁盘读入到内存，再在内存中进行查找，最后得到要査找的数据。 磁盘IO与预读在介绍索引之前，先了解一下磁盘IO，数据库数据文件是存在磁盘上的，磁盘读取数据靠的是机械运动，每次读取数据花费的时间可以分为寻道时间、旋转延迟、传输时间三个部分。 寻道时间Tseek是指将读写磁头移动至正确的磁道上所需要的时间。寻道时间越短，I/O操作越快，目前磁盘的平均寻道时间一般在3-15ms，主流磁盘一般在5ms以下 旋转延迟Trotation是指盘片旋转将请求数据所在的扇区移动到读写磁盘下方所需要的时间。通常用磁盘旋转一周所需时间的1/2表示，比如一个磁盘7200转表示：每分钟能转7200次，也就是说1秒钟能转120次，旋转延迟就是1 / 120 / 2 = 4.17ms 传输时间指的是从磁盘读出或将数据写入磁盘的时间，一般在零点几毫秒，相对于前两个时间可以忽略不计。 那么访问一次磁盘的时间，即一次磁盘IO的时间约等于5+4.17 = 9ms左右。考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO 索引数据结构在 mysql 中，索引是 存储引擎 层实现的，并没有统一的标准。不同的存储引擎的索引实现并不一样。即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不同。如：InnoDB 中根据主键引用被索引的行，而 MyISAM 则通过数据的物理位置引用被索引的行 在 InnoDB 中，表中数据都是根据主键顺序存放的，这种存储方式的表叫 索引组织表。而 聚簇索引 就是按照每张表的主键构造的一棵 B+ 树，树中同时保存了索引和数据行，数据行被存放在索引的叶子页中。也将 聚簇索引 的叶子节点称为 数据页。因为无法同时把数据行存放在两个不同的地方，所以一个表只能有一个 聚簇索引。如果没有定义主键，InnoDB 会选择一个唯一非空索引字段代替，如果没有这样的索引，那么 InnoDB会隐式定义一个 6 字节的 rowId 来作为 聚簇索引 唯一非空索引：是唯一索引（unique key）并且该字段的值not null 并且 **每一个索引 InnoDB 都会为其单独维护一颗 B+树**。 假设我们有这么一张表： 1234567CREATE TABLE `user` ( `id` bigint(20) NOT NULL, `name` varchar(50) DEFAULT NULL, `age` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `IDX_AGE` (`age`) USING BTREE) ENGINE=InnoDB; 那么 InnoDB 就会为主键 id 维护一个 聚簇索引 ，为索引 age 维护一个非主键索引，也叫 二级索引。 在上图中可以看出，二级索引的叶子页存的是主键的值，而聚簇索引叶子页存的是整行数据，聚簇索引的叶子节点也称为数据页。非叶子节不存储数据行，只存储指向下层叶子的指针和索引键值的虚记录，如19并不真实存在于数据表中 一页16kb就如上面所说的，磁盘IO是非常高昂的操作。为了减少磁盘IO次数，存储引擎也做了很多的优化。比如会整页读取数据并把一些热数据放在缓冲池中。不同存储引擎缓存单位并不相同，InnoDB在 默认情况下是 16kb ，如果 InnoDB做一个单行查找需要读取磁盘，就需要把包含该行的整个页面读入缓冲池进行缓存。假设要随机访问100字节的行， InnoDB 将用掉很多缓冲池中额外的内存来缓存这些行，因为每一次都必须读取和缓存一个完整的 16kb 页面。而InnoDB 索引页大小也是 16kb ，意味着访问一个100字节的行可能一共要使用32kb的缓存空间（有可能更多，取决于索引树有多深） B+ 索引在数据库中有一个特点是 高扇出性 ，因此在数据库中，B+ 树的高度一般都在2 ～ 4层，也就是说查找某一键值的行记录时最多只需要2 ～ 4次IO。前面分析了一次IO大概9ms，意味着查询时间只需0.02 ～ 0.04秒。 扇出：是指该模块直接调用的下级模块的个数 以一个整数字段（bigint）索引为例，字段长度为8b ，另外每个索引还跟着 6b 指向子树的指针，则每页可以存放索引 16kb * 1024 / 14 b = 1170。这棵树高是 4 的时候，就可以存 1170 的 3 次方个值，这已经 16亿了。考虑到树根的数据块总是在内存中的，那么一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要3 次磁盘IO。这也就是 索引字段越小越好 的原因。因为磁盘块的大小也就是一个数据页的大小，是固定的（默认16k），那么在总数据量固定的情况下，如果数据项占的空间越小，则每页能存的数据项数量越多，那么树的高度越低。这也是为什么 b+ 树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，每个磁盘块的数据项会大幅度下降，导致树增高 索引页、数据页逻辑连续每一层的页通过双向链表链接（Page Header中的PAGE_PREV和PAGE_NEXT记录上、下页的位置），页按照索引键的顺序排序；另外每个页中的记录也是通过单向链表进行维护的（Recorder Header的最后两个字节记录下一行偏移量）。按照索引键排序的好处就是对于索引键的排序查找和范围查找非常快。如要查找年龄最大的两个人id，由于b+树索引是双向链表的，可以快速找到最后一个数据页，并取最后两条数据 123456789101112131415mysql&gt; EXPLAIN SELECT id FROM user ORDER BY id LIMIT 2\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user partitions: NULL type: indexpossible_keys: NULL key: PRIMARY key_len: 8 ref: NULL rows: 2 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) 可以看到 Extra 信息中并没有 Using filesort，说明并没有进行额外的排序操作。如果排序字段不是索引字段，那么就会在内存中进行额外的排序操作 未使用索引进行排序： 123456789101112131415mysql&gt; EXPLAIN SELECT ID FROM user ORDER BY name LIMIT 2\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: user partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 9 filtered: 100.00 Extra: Using filesort1 row in set, 1 warning (0.00 sec) 通过二分+遍历查找B+ 树页内索引使用类似 跳表 查找。在定位到了页之后，通过页目录（Page Directory）来进行 二分查找 ，定位到距离数据较近的槽点（Slot） // todo 描述页内查找过程及图 索引的查找基于聚簇索引和普通二级索引的查询有什么区别呢？ 如下面语句，根据 id 查找： 1SELECT * FROM user WHERE id = 1; 即主键查询的方式，则只需要搜索 id 这棵 B+ 树。在多数情况下，查询优化器倾向于采用聚簇索引，因为聚簇索引索引能够在 B+ 树索引的叶子节点上直接找到数据。此外，由于定义了数据的逻辑顺序，聚簇索引能够很快地访问针对范围值的查询。查询优化器能够快速发现某一段范围的数据页需要扫描 再如下面语句，根据 age 来查找： 1SELECT * FROM user WHERE age = 15; 即普通索引查询方式，则需要先搜索 age索引树，得到 id 的值为 33，再拿着33到 id 索引树搜索一次，这个过程称为回表 也就是说，基于非主键索引的查询需要多扫描一棵索引树。因此，我们在应用中应该尽量使用主键查询。当然也不是每次用非主键索引查询都会回表，如果要查询的字段都在这棵索引书上，就不需要回表操作 索引的维护 页分裂B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。以上面这个图为例，第一种情况： 如果插入点的新值为6，则只需要4后面插入一条新的记录。第二种情况： 如果插入新的值为20，这时无法直接在25后面直接插入，需要逻辑上挪动后面的数据，空出位置。更糟的情况是，如果所在的页已经满了，这时就要新建一个新的页，并把部分数据挪过去，这个过程称为页分裂。在这种情况下，不管性能还是空间利用率都会受到影响 所以通常都会建议使用 AUTO_INCREMENT 自增列作为主键 ，这样可以保证数据行是按顺序写入的。最好避免随机的聚簇索引，比如使用 UUID作为主键，这样使得聚簇索引的插入变得完全随机。 如果使用自增列作为主键，每次插入都是追加操作，就如上面说的第一种情况。当达到页的最大填充因子时（InnoDB默认最大填充因子是页大小的 15/16，留出部分空间用于后续修改），下一条记录就会写入新的页中。不涉及到挪动其它记录，也不会触发叶子节点的分裂 而有业务逻辑的字段做主键或者使用UUID，则不能保证有序插入，这样写入数据的成本相对较高。总结以下是一些缺点： 写入目标页可能已经刷到磁盘上并从缓存中清除，或者是还没有被加载到缓存中，InnoDB在插入之前不得不先找到并从磁盘读取目标页到内存。这将导致大量的随机I/O 因为写入是乱序的，InnoDB不得不频繁地做页分裂操作，以便为新的行分配空间，页分裂会导致移动大量的数据，一次插入最少需要修改三个页而不是一个页 由于频繁的页分裂，页会变得稀疏并被不规则的填充，最终数据会有碎片 而且通常业务主键都比较大，比如用身份证号做主键，那么每个二级索引的叶子节点都需要存储，这也大大的增加了索引的空间占用。而如果使用整型做主键，则只需4个字节，长整型（bigint）也只需8字节。所以，从性能和存储空间方面考量，自增主键往往是更合理的选择 页合并删除记录时，不会实际删除该记录。相反，它将记录标记为已删除，并且它所使用的空间可以被其它记录声明使用 图1 当页中删除的记录达到MERGE_THRESHOLD（默认页体积的50%），InnoDB会开始寻找最靠近的页（前或后）看看是否可以将两个页合并以优化空间使用 图2 在本例中，页面 图2 占用的空间不足一半。图1 又有足够的删除数量，现在使用率也不到50%。从InnoDB的角度来看，它们是可合并的 图3 合并操作的结果（图3）是图1包含它以前的数据加上图2的数据，图2变成一个空页，可以接纳新数据 合并后的图2 当我们进行UPDATE操作，并且使页中记录数量低于阈值时，InnoDB也会进行一样的操作 规则是：页合并发生在删除或更新操作中，关联到当前页的相邻页。如果合并操作成功，在INFOMATION_SCHEMA.INNODB_METRICS中的index_page_merge_successful将会增加 了解更多关于页合并与分裂 建索引的原则和使用时注意点以下表结构来说明： 123456789CREATE TABLE `t` ( `a` varchar(32) NOT NULL, `b` varchar(50) DEFAULT NULL, `c` varchar(20) DEFAULT NULL, `d` varchar(20) DEFAULT NULL, `e` varchar(100) DEFAULT NULL, PRIMARY KEY (`a`), KEY `IDX_B_C_D` (`b`,`c`,`d`) USING BTREE) ENGINE=InnoDB; 原则最左前缀匹配原则mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如b = 2 and c &gt; 3 and d = 4 如果建立(b,c,d)顺序的索引，d是用不到索引的，如果建立(b,d,c)的索引则都可以用到，b,d的顺序可以任意调整，因为查询优化器会帮你优化成索引可以识别的形式 总结为以下三点： 如果不是按照索引的最左列开始查找，则无法使用索引 下面查询语句，不是最左列开始查找： 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM t WHERE c = 'bbb'\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: t partitions: NULL type: ALLpossible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 3 filtered: 33.33 Extra: Using where1 row in set, 1 warning (0.00 sec) 可以看到并没有使用索引。如果从最左列开始查找，并且使用右模糊，也是可以使用索引进行匹配的，因为可以按左前缀字符查找 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM t WHERE b LIKE 'aa%'\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: t partitions: NULL type: rangepossible_keys: IDX_B_C_D key: IDX_B_C_D key_len: 153 ref: NULL rows: 1 filtered: 100.00 Extra: Using index condition1 row in set, 1 warning (0.01 sec) 不能跳过索引中的列 也就是说上面的索引无法用于查找 b = ‘xxx’ and d = ‘xxx’ 的记录。如果不指定b的值，则 mysql 只能使用索引的第一列。在索引 IDX_B_C_D 中匹配到所有 b = ‘xxx’ 的记录，再拿着这些记录的主键去聚簇索引中查找 如果查询中有某个列是范围查询，则其右边所有列都无法使用索引 例如查询 b = ‘xxx’ and c &gt;/&lt;/between/like ‘xxx’ and d = ‘xxx’ 这个查询只能用前面两列，因为列c是一个范围查询。但是存储引擎会用d列进行查询优化（索引下推）。如果范围查询的列值有限，那么可以通过使用多个等于条件来代替范围条件 尽量选择区分度高的列作为索引区分度的公式是 *count(distinct col) / count()**，表示字段不重复的比例，比例越大查询效率越高，唯一键的区分度是1。而一些状态、性别字段可能在大数据面前区分度就是0 存在非等号和等号混合判断条件时，在建索引时，等号条件的列前置。如：where c &gt; ? and d = ? 那么即使 c 的区分度更高，也必须把 d 放在索引的最前列，即建立组合索引 idx_d_c。当然这也只是“经验法则”，还需要考虑到索引的复用能力 根据美团技术团队博客文章，需要join的字段这个值一般都要求是0.1以上，即平均1条扫描10条记录 尽量的扩展索引，不要新建索引 空间：InnoDB 会为每个索引都建立 B+ 树索引，所以会占用更多的空间 性能：每次修改数据时要对索引进行维护，多棵索引树无疑增加了维护成本。并且在查询上，多列索引有机会使用 覆盖索引 和 索引下推 来提升查询效率 使用时注意点 索引列不能参与计算：比如 from_unixtime(create_time) = ’2014-05-29’ 或 left(code, 6) = ‘010108’ 或 score + 1 = 80 就不能使用到索引。所以语句应该写成create_time = unix_timestamp(’2014-05-29’) 和 code LIKE ‘010108%’ 索引列与参数类型不匹配：比如字符串字段索引 phone = 13024532432 最左前缀匹配：比如左模糊查询 … 索引的使用联合索引联合索引 也是一棵 B+ 树，不同的是联合索引的键值数量不是 1，而是大于等于 2。并且和单个键值的 B+ 树一样，键值都是排序的，通过叶子节点可以逻辑上读出所有的数据。 如有以下表： 1234567CREATE TABLE t1 ( a INT, b INT, c INT, PRIMARY KEY ( a ), KEY IDX_B_C ( b, c ) ) ENGINE = INNODB; 索引先按字段b排序，如果b字段值相等，再按c字段排序，即(1, 1)、(1, 2)、(2, 1)、(2, 4)、(3, 1)、(3, 2) 数据按(b, c) 的顺序进行了存放。因为对第二字段进行了排序，在很多场景下可以利用这个特性来避免多一次排序操作。如查询一个人的最近3笔订单，则可以建立(user_id, create_time) 的联合索引，建表、查询语句为： 1234567891011121314CREATE TABLE `buy_order` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `user_id` bigint(20) DEFAULT NULL, `create_time` datetime DEFAULT NULL, PRIMARY KEY (`id`), KEY `IDX_USER_CREATE_TIME` (`user_id`,`create_time`) USING BTREE, KEY `IDX_USER` (`user_id`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=6;INSERT INTO `user`.`buy_order`(`id`, `user_id`, `create_time`) VALUES (1, 1, '2021-04-04 23:14:27');INSERT INTO `user`.`buy_order`(`id`, `user_id`, `create_time`) VALUES (2, 1, '2021-04-07 23:14:35');INSERT INTO `user`.`buy_order`(`id`, `user_id`, `create_time`) VALUES (3, 1, '2021-04-07 23:14:43');INSERT INTO `user`.`buy_order`(`id`, `user_id`, `create_time`) VALUES (4, 1, '2021-04-30 23:14:52');INSERT INTO `user`.`buy_order`(`id`, `user_id`, `create_time`) VALUES (5, 2, '2021-04-14 23:15:49'); 为了做比较还加了一个单独的IDX_USER索引，先查询用户为1的所有订单，这个语句应该是两个索引都可以使用： 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM buy_order WHERE user_id = 1\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: buy_order partitions: NULL type: refpossible_keys: IDX_USER_CREATE_TIME,IDX_USER key: IDX_USER_CREATE_TIME key_len: 9 ref: const rows: 4 filtered: 100.00 Extra: Using index1 row in set, 1 warning (0.00 sec) 可以看到上面的执行计划，优化器最终选择了单个索引的IDX_USER，因为该索引的节点包含单个键值，所以理论上一个页能存放更多的记录 接着假设要查询用户为1的最近三个订单： 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM buy_order WHERE user_id = 1 ORDER BY create_time DESC LIMIT 3\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: buy_order partitions: NULL type: refpossible_keys: IDX_USER_CREATE_TIME key: IDX_USER_CREATE_TIME key_len: 9 ref: const rows: 4 filtered: 100.00 Extra: Using where; Using index1 row in set, 1 warning (0.00 sec) 同样的，对于这个查询既可以使用 IDX_USER 索引，也可以使用 IDX_USER_CREATE_TIME 索引。但是这次优化器使用了 IDX_USER_CREATE_TIME 的联合索引，因为在这个联合索引中 create_time 已经排序好了，根据该联合索引取出数据，无须再对create_time 做一次额外的排序操作，若强制使用 IDX_USER索引，则执行计划如下图： 123456789101112131415mysql&gt; EXPLAIN SELECT * FROM buy_order FORCE INDEX(IDX_USER) WHERE user_id = 1 ORDER BY create_time DESC LIMIT 3\\G*************************** 1. row *************************** id: 1 select_type: SIMPLE table: buy_order partitions: NULL type: refpossible_keys: IDX_USER key: IDX_USER key_len: 9 ref: const rows: 4 filtered: 100.00 Extra: Using index condition; Using filesort1 row in set, 1 warning (0.00 sec) 在Extra选项中可以看到 Using filesort，即需要额外的一次排序操作才能完成查询。而这次显然需要对列 create_time 排序，因为索引 IDX_USER 中的 create_time 是未排序的 正如前面所介绍的那样，联合索引(b, c)其实是根据列b、c进行排序，因此下列语句可以直接使用联合索引得到结果： 1SELECT * FROM TABLE WHERE a = ? ORDER BY b 然而对于联合索引(a,b,c)来说，下列语句同样可以直接通过联合索引得到结果： 12SELECT * FROM TABLE WHERE a = ? ORDER BY bSELECT * FROM TABLE WHERE a = ? AND b = ? ORDER BY c 覆盖索引如果执行的语句是 : 1SELECT ID FROM t WHERE k BETWEEN 3 AND 5; 这时只需要查 ID 的值，而 ID 的值已经在 k 索引树上了，因此可以直接提供查询结果，不需要回表。也就是说从二级索引就可以得到查询的字段，而不需要查询聚簇索引中的记录（mysql5.0或以下版本不支持） 使用覆盖索引另一个好处是针对某些统计，如下面查询语句： 1SELECT COUNT(*) FROM t WHERE b = ?; 假设 b 字段也是表的主键，并且b字段上还有其它的索引，InnoDB存储引擎并不会通过聚簇索引来进行统计。因为二级索引远小于聚簇索引，选择二级索引可以减少IO操作 此外，通常情况下，如有索引(a, b)的联合索引，是不可以选择作为列b的查询索引。但是如果是统计操作，并且是覆盖索引，则优化器会选择它 比如以下查询语句： 1SELECT COUNT(a) FROM t WHERE b = ?; 索引下推（ICP）从 mysql5.6 开始支持的一种根据索引进行查询的优化方式 如有索引(a, b)的联合索引和以下查询语句： 1SELECT * FROM t WHERE a LIKE 'a%' AND b = ?; 在 mysql5.6 之前，根据最左前缀匹配规则，存储引擎只能查询出符合条件 LIKE 'a%' 的记录，然后把结果集返回给 Server 层，再拿着一个个主键去聚簇索引查找，然后在Server 层过滤条件 b = ? 在引入下推优化之后（mysql5.6），如果部分WHERE条件能使用索引中的字段，Server 层会把这部分（上面b条件）下推到引擎层（调用引擎接口的时候把这部分也传过去）。存储引擎在二级索引遍历过程中对 b 字段先做判断，直接过滤掉不满足条件的记录。ICP 减少了 Server 层访问存储引擎的次数和引擎层访问聚簇索引的次数（减少回表次数）。总之是 ICP 的优化在引擎层就能够过滤掉大量的数据，减少 IO 次数，提高查询语句性能 控制参数：SET @@optimizer_switch=”index_condition_pushdown=on” MRR优化（Multi-range Read）从 mysql5.6 开始支持 MRR 优化，MRR 优化的目的是为了减少磁盘的随机访问，并将随机访问转化为较为顺序的数据访问。 在不使用 MRR 时，优化器根据二级索引返回的记录来进行回表，因为这个记录是根据二级索引键排序的，一般会有较多的随机IO。比如在二级索引中查询到的数据集为：(key=1, pk=99)，(key=2, pk=88)，(key=14, pk=875)，(key=15, pk=76)，这时如果按这个顺序去聚簇索引查找，因为主键无序，可能会有更多的随机IO和缓冲区（buffer pool）中的页被替换出，然后又不断地被读入缓冲区，若是按照主键顺序进行访问，则可以将此重复行为降到最低 使用MRR时，SQL语句的执行过程是这样的： 优化器将二级索引查询到的记录放到一块缓冲区中 如果二级索引查询完成或者缓冲区已满，则使用快速排序对缓冲区中的内容按照主键进行排序 根据主键的排序来访问聚簇索引 当缓冲区中的列表取完数据后，则继续调用过程 2) 3)，对超过缓存区大小的部分继续排序查询，直至扫描结束 通过上述过程，优化器将二级索引随机的 IO 进行排序，转化为主键的有序排列，从而实现了随机 IO 到顺序 IO 的转化，提升性能 控制参数： 用optimizer_switch 的标记来控制是否使用MRR.设置mrr=on时，表示启用MRR优化。 mrr_cost_based表示是否通过cost base的方式来启用MRR. 当mrr=on,mrr_cost_based=on,则表示cost base的方式还选择启用MRR优化,当发现优化后的代价过高时就会不使用该项优化 当mrr=on,mrr_cost_based=off,则表示总是开启MRR优化 1SET @@optimizer_switch='mrr=on,mrr_cost_based=on'; 缓冲区参数： 参数 read mnd_ bufter_size 用来控制键值的缓冲区大小，当大于该值时，则执行器对已经缓存的数据根据主键进行排序，并通过 主键来取得行数据。该值默认为256K 怎么给字符串字段加索引有时需要给较长的字符串字段加索引，这会使得索引变得大且慢。通常，可以定义字符串一部分前缀作为索引，这样可以大大节约索引空间，从而提高索引效率。但这样也会降低索引的区分度，还可能使查询变得更慢 为什么说减小索引大小就能提高查询效率呢？首先较长的字符串在索引排序和值比较时势必会更慢。还有索引页大小都是固定的，较小的话每一页能够存放更多的索引，这样也能减少IO次数。重要的是节省空间 假设，现在维护一个支持邮箱登录的系统，用户表是这么定义的： 1234mysql&gt; create table SUser(Id bigint unsigned primary key,email varchar(64) ) engine=innodb; 我们可以直接给 email 字段添加索引： 1alter table SUser add index index1(email); 这样创建的索引键值包含整个 email 字符串。也可以指定只取前面几个字符创建索引，如： 1mysql&gt; alter table SUser add index index2(email(6)); 这样两种定义方式的索引的数据结构和存储是这样的： index1 index2 从图中可以看到，由于 email(6) 这个索引结构中邮箱只取前6个字符，所以占用空间会更小，这就是前缀索引的优势。但同时带来的问题是，可能额外增加回表次数 根据下面这条查询语句，来分析上面两种索引的执行过程： 1mysql&gt; select id,name,email from SUser where email='aaaaaab@gmail.com'; 如果使用的是 index1（即整个字符串），执行顺序是这样的： 从二级索引中找到满足值是 aaaaaab@gmail.com 的记录，取得 id = 200 到聚簇索引上找到 id 为 200 的行 普通索引和唯一索引怎么选择其它索引哈希索引全文索引参考《高性能MySQL第3版》 《MySQL技术内幕InnoDB存储引擎第2版》 MySQL索引原理及慢查询优化 磁盘I/O那些事","link":"/2021/03/31/2021-03-31-mysql-index/"},{"title":"mysql执行过程","text":"一条SQL查询语句是如何执行的下面是 mysql 查询的基本执行路径示意图： 大体来说，mysql可以分为 Server 层和存储引擎层两部分： Server 层包括：连接器、查询缓存、语法解析器、优化器、执行器等大部分核心服务功能，以及所有的内置函数（如日期、时间、数学加密函数等）。所有跨存储引擎的功能都在这一层实现：如存储过程、触发器、视图等 存储引擎 层负责数据的存储和提取。其架构是插件式的，支持 InnoDB , MyISAM , Memory 等多个存储引擎。从 mysql 5.5.5 版本开始默认引擎为 InnoDB 插件式的也就意味着存储引擎是可插拔的。我们可以在表 a 使用 InnoDB，在表 b 使用 memory 。不同存储引擎的表数据存取方式不同，支持的功能也不同。我们也可以自定义存储引擎 15.9示例存储引擎 从上面图中可以看出，不同的存储引擎公用一个 Server 层，也就是从 连接器 到 执行器 的部分。接下来看看每个组件都做了什么： 连接器首先需要连接到数据库，这时接待你的就是连接器。连接器负责跟客户端建立连接、校验账号、获取权限、维持和管理连接。每个客户端都会在服务器中拥有一个线程，这个连接的查询只会在这个单独的线程中执行。从 mysql 5.5 开始支持线程池，所以服务器不需要为每一个新建的连接创建或销毁线程 连接命令一般这么写： 1mysql -h $ip -P $port -u $user -p 在连接上后，连接器就会对其进行身份认证。认证基于用户名、密码和原始主机信息。如果使用了安全套接字（ssl）的方式连接，还可以使用证书认证。如果用户名、密码不对或主机没有权限，就会收到 Access denied for user 'xxx@ip' 的错误。一旦连接成功，连接器会继续查出该连接所拥有的权限（例如：是否允许对 user 库的 t 表执行 select 语句）。这个连接后续的权限判断都依赖于此时读到的权限，这也就意味着一个用户成功连接后，即使修改了它的权限，也不会受影响，只有重新新建连接才会使用新的权限设置 连接完成后，如果没有做后续的动作，这个连接就处于空闲的状态，通过 show processlist 命令可以看到 Command 为 sleep 的就是空闲连接。如果空闲一定长的时间，连接器就会自动将它断开。这个时间通过参数 wait_timeout 控制，默认为28800秒=8小时 查询缓存在连接建立完成后，就可以开始执行查询语句了。这时来到第二步：查询缓存，这边所指的“缓存”，指的是完整的查询结果。当接收到一个查询请求后，会先到查询缓存看看，当查询命中缓存，会立刻返回结果，跳过了 解析、优化、和执行阶段。如果未命中，就会继续后面执行阶段，执行完成后，执行结果会被存入查询缓存中 查询缓存以 key-value 的形式被直接缓存在内存中，这个key其实是一个大小写敏感哈希值，这个哈希值包括了语句本身、查询的数据库、参数等一些其它会影响查询结果的信息，即使只有一个字节的不同都会导致缓存不命中。当查询中包含任何用户自定义函数、存储函数、用户变量、临时表、mysq库中的系统表，或者任何包含列级别权限的表，都不会被缓存。 查询缓存还会跟踪查询中涉及的每个表，如果这些表发生变化，那么和这个表相关的所有缓存数据都将失效，即使数据表变化时对缓存中的结果可能并没有影响。这也就是大多数情况下会建议不要使用查询缓存的原因，对于更新频繁的数据库来说，查询缓存命中率会非常低，除非是静态表，很少才有更新数据。mysql团队也意识到查询缓存的问题，自 mysql 5.6（2013 年）以来，查询缓存已被默认禁用，在 mysql8.0 版本中查询缓存模块直接被移除了 MySQL 8.0：退出查询缓存支持 语法解析器和预处理如果没有命中查询缓存，这时就会到解析器，开始真正的执行语句。解析器会先使用 mysql 语法规则验证和解析查询，例如，它会验证是否使用错误的关键字，或者关键字使用顺序问题，再或者引号、括号前后能否正确匹配，之后生成一颗对应的 “解析树”。如果语句不对，就会收到 “You have an error in your SQL syntax” 的错误提醒 预处理器则根据一些规则进行进一步的检查解析树是否合法，例如检查数据表、数据列是否都存在，解析名字和别名是否有歧义。最后会验证权限 查询优化器现在解析树被认为是合法的了，现在将由优化器将其转化为执行计划。一条语句可以有很多种执行的方式，虽然最后的查询结果都是相同的，优化器的作用就是找到这其中最好的方式 mysql 使用基于成本的优化器，它会 预测 一个查询使用某种执行计划时的成本，并选择其中成本最小的一个 更多关于优化器内容 执行器在优化器阶段完成后，查询对应的执行计划就已经生成好了，执行器则根据这个执行计划来完成整个查询。开始执行的时候，会先判断这个表是否有权限，如果没有就直接返回权限错误（上查询缓存阶段，如果命中缓存，在返回缓存结果的时候也会做权限校验。在优化器之前也会做precheck权限校验）。执行器简单的根据执行计划的指令逐步执行，在执行过程中，有大量的操作需要通过调用储存引擎 api 来完成，这些接口被称为 “handler API” 。存储引擎接口有着非常丰富的功能，但是底层接口却只有几十个。例如,有一个査询某个索引的第一行的接口，再有个査询某个索引条目的下一个条目的功能，有了这两个功能我们就可以完成全索引扫描的操作了。这种简单的接口模式，让 mysql 的存储引擎插件式架构成为可能 1SELECT * FROM T WHERE ID = 10； 比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的： 调用 InnoDB引擎取这个表的第一行的接口，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端 对于有索引的表，执行的逻辑也差不多，第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，这些接口都是引擎中已经定义好的 查询最后一步是将结果集返回给客户端，即使查询不需要返回结果集给客户端，mysql 仍然会返回一些查询的信息，如查询影响到的行数。如果查询可以被缓存，那么在这个阶段也会将结果存放到查询缓存中。你会在数据库的慢查询日志中看到一个 rows_examined 的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取结果集的时候累加的。在有些场景下，执行器调用一次，在引擎内部则扫描了多行，因此引擎扫描行数跟 rows_examined 并不是完全相同的 最后 mysql 将结果集返回给客户端是一个增量，逐步返回的过程。 在执行器得到第一条结果时，就可以向客户端逐步返回结果集了。这样的好处是： 服务端无须存储太多的结果，也就不会因为要返回太多结果而消耗太多内存 让客户端第一时间获得返回结果 结果集中的每一行都会以一个满足 mysql 客户端/服务器通信协议的封包发送，再通过TCP协议进行传输，在TCP传输的过程中,可能对。mysql 的封包进行缓存然后批量传输 一条更新查询语句是如何执行的更新语句前面的步骤和查询一致，解析器通过解析知道这是一条更新语句，然后执行器选择最优执行计划去执行 执行器会先open table，如果该表上有 MDL（X） （元数据排他锁），则等待。如果没有则在该表上加 MDL（S） （元数据共享锁） 进入到引擎层，首先会去 innodb_buffer_pool 里的 data dictionary (元数据信息，是InnoDB自己管理的表缓存) 得到表信息，通过元数据信息，去 lock info 里查出是否会有相关的锁信息，并把这条update语句需要的锁信息写入到 lock info 里 然后涉及的旧数据以快照的形式存储到缓冲池中的 undo page 里，并在 redo log 中记录 undo log（undo log持久化） 对数据页进行修改，并把数据页的物理修改记录到 redo log buffer里。由于一个事务会涉及到多个页面的修改，所以redo log buffer里会有多条页面的修改信息。并且由于 group_commit 的原因，本次事务所产生的redo log buffer可能会跟其它事务一同刷新到磁盘上 同时修改的信息，会按照 event 的格式，以不同的 event type 记录到 binlog_cache 中，在 事务 commit 后 dump 线程会从 binlog_cache 里把 event 发送给 slave 的 I/O 线程 之后把还需要在二级索引上做的修改，写入到 change buffer page，等到下次有读取该二级索引页时，再去与二级索引页做 merge commit操作，由于存储引擎层与 server 层之间采用的是内部 XA (保证两个事务的一致性，这里主要保证redo log和binlog的原子性)，所以提交分为prepare阶段与commit阶段 引擎将新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态 执行器将 binlog_cache 里的日志进行刷新到磁盘，并进行同步操作 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成 参考MYSQL实战 45讲 《高性能MySQL第3版》","link":"/2021/04/14/2021-04-14-mysql-sql-executing-process/"},{"title":"mysql日志文件","text":"bin log二进制日志 ( binary log) 是 Server 层的日志，记录了对 mysql 数据库执行更改的所有操作，但是不包括 SELECT和SHOW这类操作，因为这类操作对数据本身并没有修改。然而，若操作本身并没有导致数据库发生变化，那么该操作可能也会写入二进制日志。例如执行下列未更改的sql： 123mysql&gt; UPDATE t SET b = 2 WHERE a = 1;Query OK, 0 rows affected (0.00 sec)Rows matched: 0 Changed: 0 Warnings: 0 Changed：0 说明没有数据被修改。但通过 show master status 查看binlog日志文件和 SHOW BINLOG EVENTS IN 'mysql-bin.000006' 查看日志内容可以看到在 binlog 中的确进行了记录 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253mysql&gt; show master status\\G;*************************** 1. row *************************** File: mysql-bin.000006 Position: 482 Binlog_Do_DB: Binlog_Ignore_DB: Executed_Gtid_Set: 1 row in set (0.00 sec)mysql&gt; SHOW BINLOG EVENTS IN 'mysql-bin.000006'\\G*************************** 1. row *************************** Log_name: mysql-bin.000006 Pos: 4 Event_type: Format_desc Server_id: 123456End_log_pos: 123 Info: Server ver: 5.7.31-log, Binlog ver: 4*************************** 2. row *************************** Log_name: mysql-bin.000006 Pos: 123 Event_type: Previous_gtids Server_id: 123456End_log_pos: 154 Info: *************************** 3. row *************************** Log_name: mysql-bin.000006 Pos: 154 Event_type: Anonymous_Gtid Server_id: 123456End_log_pos: 219 Info: SET @@SESSION.GTID_NEXT= 'ANONYMOUS'*************************** 4. row *************************** Log_name: mysql-bin.000006 Pos: 219 Event_type: Query Server_id: 123456End_log_pos: 298 Info: BEGIN*************************** 5. row *************************** Log_name: mysql-bin.000006 Pos: 298 Event_type: Query Server_id: 123456End_log_pos: 402 Info: use `user`; UPDATE t SET b = 2 WHERE a = 1*************************** 6. row *************************** Log_name: mysql-bin.000006 Pos: 402 Event_type: Query Server_id: 123456End_log_pos: 482 Info: COMMIT6 rows in set (0.01 sec) binlog 日志文件默认并没有开启，需要手动指定参数来启动。通过 **log-bin=filename **可以启动日志，如果不指定filename，则默认日志文件名为主机名 123log_bin=/var/lib/mysql/mysql-binbinlog_format=mixed # 选择 ROW 模式server_id=123456 据官方手册中的测试表明，开启binlog会使数据库性能下降1%。下面为binlog相关配置参数： max_binlog_size：单个日志文件最大值（默认1g），如果超过会产生新的文件，后缀名 + 1 binlog_cache_size：未提交的事务日志会被记录到一个缓存中，等该事务提交时直接将缓存中的日志写入到日志文件，该缓存大小就由该配置控制，默认32k。这个缓存是基于会话（session）的，因此当开始一个事务时，会自动为其分配一个配置大小的缓存 sync_binlog：sync_binlog=N 表示写缓冲多少次就同步磁盘。如果设为1，即同步写磁盘的方式来写日志。但是，即使将sync_ binlog设为1，还是会有一种情况导致问题的发生。当一个事务发出 COMMIT 动作之前，因为同步写，因此会将日志立即写人磁盘。如果这时已经写人了日志，但是提交还没有发生，并且此时发生了宕机，那么在数据库下次启动时，由于 COMMIT 操作并没有发生，这个事务会被回滚掉。但是日志已经记录了该事务信息，不能被回滚。 这个问题可以通过将参数 innodb_ support_xa设为1来解决，虽然 innodb_ support_xa与XA事务有关，但它同时也确保了binlog和 INNODB存储引擎数据文件的同步 binlog-do-db：表示需要写入哪些库日志，默认全部 binlog-ignore-db：表示需要忽略哪些库日志，默认无 log-slave-update ： binlog_format：日志记录格式STATEMENT,ROW,MIXED 文件格式binlog记录的是 逻辑日志，格式分为 statement，row 以及 mixed 三种。在主从同步种一般不建议使用statement，因为有些语句不支持如uuid函数。一般使用 row 模式 statement：记录具体的执行sql，某些语句和函数如UUID, LOAD DATA INFILE等在复制过程可能导致数据不一致甚至出错 row：基于行的模式，记录的是行每个字段的变化，以及事件类型等，大小会比其他两种模式大很多 mixed：混合模式，根据语句来选用是statement还是row模式 作用binlog主要用来point-in-time恢复和主/从复制同步，从这里就可以衍生出许多应用场景 1.读写分离 有一个主库Master，所有的更新操作都在master上进行 同时会有多个Slave，每个Slave都连接到Master上，获取binlog在本地回放，实现数据复制。 在应用层面，需要对执行的sql进行判断。所有的更新操作都通过Master(Insert、Update、Delete等)，而查询操作(Select等)都在Slave上进行。由于存在多个slave，所以我们可以在slave之间做负载均衡。通常业务都会借助一些数据库中间件，如tddl、sharding-jdbc等来完成读写分离功能 2.数据恢复 如果数据误删，或需要恢复到某个时间点，则可以通过较旧时间点的版本+binlog来恢复，或通过全量binlog 3.数据最终一致性 在实际开发中，我们经常会遇到一些需求，在数据库操作成功后，需要进行一些其他操作，如：更新缓存或者更新搜索引擎中的索引等 如何保证数据库操作与这些行为的一致性，就成为一个难题。以数据库与redis缓存的一致性为例：操作数据库成功了，可能会更新redis失败；反之亦然。很难保证二者的完全一致 这时我们就可以利用binlog。如果数据库操作成功，必然会产生binlog。之后，我们通过一个组件（canal等），来模拟的mysql的slave，拉取并解析binlog中的信息。通过解析binlog的信息，去异步的更新缓存、索引或者发送MQ消息，保证数据库与其他组件中数据的最终一致 4.异地多活，数据同步 写入时机binlog只在 事务提交前 进行一次写入。什么时候从缓存刷新到磁盘跟参数sync_binlog相关 // todo redo log重做日志用来实现事务 // todo 文件格式作用重做日志用来保证事务的持久性。为了更好的性能，InnoDB会将数据缓存在内存中（Buffer Pool），对数据的修改也是先修改缓存中的，所以磁盘数据会落后于内存，这时如果进程或机器奔溃，会导致内存数据丢失。为了维护数据库本身的一致性和持久性，InnoDB维护了redo log，修改 page 之前需要先将修改的内容记录到redo log中，并保证 redo log早于对应的page落盘，也就是通常说的WAL（Write Ahead Log）。当故障发生导致内存数据丢失后，InnoDB会在重启时，通过重放redo，将page数据恢复到奔溃前的状态 写入时机bin log与redo log区别1. redo log是InnoDB引擎特有的；binlog 是 mysql 的 Server 层实现的，所有引擎 undo log// todo 参考MYSQL实战 45讲 《高性能MySQL第3版》","link":"/2021/04/18/2021-04-18-mysql-log-files/"},{"title":"InnoDB存储引擎中的锁","text":"我们都知道事务的4个特性，即 ACID。mysql 数据库使用加锁的方式来实现其中的 I (Isolation隔离性)。对于 InnoDB 存储引擎来说，它 支持行锁和表锁 ，而且行锁是由存储引擎通过给索引记录加锁来实现的，并且 InnoDB 默认是加行锁。好处就是锁定颗粒度小，发生锁冲突的概率低，并发度高 InnoDB存储引擎中的锁我们都知道事务的4个特性，即 ACID。mysql 数据库使用加锁的方式来实现其中的 I (Isolation隔离性)。对于 InnoDB 存储引擎来说，它 支持行锁和表锁 ，而且行锁是由存储引擎通过给索引记录加锁来实现的，并且 InnoDB 默认是加行锁。好处就是锁定颗粒度小，发生锁冲突的概率低，并发度高 锁的类型InnoDB 实现了两种行级锁： 共享锁(S) : 允许事务读一行数据 排他锁(X)：允许事务删除或更新一行数据 其中，X 锁与任何的锁都不兼容，而 S 锁仅和 S 锁兼容。即如果一个事务已经获得了行的 S锁，那么另外的事务可以立即获得这行的 S 锁，因为读取并没有改变行的数据，称这种情况为锁兼容。 但若有其他的事务想获得行的 X 锁，则其必须等待事务释放行上的 S 锁 InnoDB 存储引擎除了行锁以外，还有表锁，通常也称为意向锁，其设计目的主要是为了指示事务稍后对表中的行需要哪种类型的锁（共享或独占）。其支持两种意向锁： 意向共享锁（IS Lock），事务想要获得一张表中某几行的共享锁 意向排他锁（IX Lock），事务想要获得一张表中某几行的排他锁 如果需要对页上的记录 r 进行上 X 锁，那么分别需要对数据库、表、页上 IX，最后对记录 r 上 X 锁。若中间有任何一个部分导致等待，那么该操作需要等待粗粒度锁释放。 举例来说，在对记录 r 加 X 锁之前，已经有事务对表进行了 S 表锁，那么表上已存在 S 锁，之后事务需要对记录 r 在表上加上 IX，由于不兼容，所以该事务需要等待表锁操作的完成 InnoDB 存储引擎中锁的兼容性： 设计意向锁的目的何在? 假设事务1，用 X 锁来锁住了页上的几条记录，那么此时表、页上存在 IX 锁，即意向排他锁。此时事务2要进行LOCK TABLE … WRITE的表级别锁的请求或者事务2进行 DML 操作且未使用索引而升级为表锁时，可以直接根据意向锁是否存在而判断是否有锁冲突。如果没有这个意向锁，那么可能需要遍历整个表记录才能知道表是不是有记录被锁 DML表锁与MDL表锁mysql 里面表级别的锁有两种：一种是表锁，一种是元数据锁（metadata lock，MDL) 表锁 表锁的语法是 lock tables … read/write。可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放，commit 和 rollback 并不会释放锁（准确的说是InnoDB释放了内部表锁，server没释放表锁） 需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的写。举个例子，如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。写 t1 并不允许，也不能读写其他表 MDL元数据锁(metadata lock) MDL 不需要显式使用，在访问一个表的时候会被自动加上。在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁 MDL 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行 外键和锁自增长与锁锁的算法InnoDB 通过给索引项加锁来实现行锁，如果没有索引，则通过隐藏的聚簇索引来对记录加锁。如果操作不通过索引条件检索数据，InnoDB 则对表中的所有记录加锁，实际效果就和表锁一样 InnoDB 存储引擎有3种行锁的算法，分别是： Record Lock: 单个记录上的锁 Gap Lock: 间隙锁，锁定一个范围，但不包括本记录 Next-Key Lock: Gap Lock+Record Lock，锁定一个范围，并且锁定记录本身 例如一个索引有10，30这两个值，InnoDB 可以根据需要使用 Record Lock 将10，30两个索引锁住；也可以使用 Gap Lock 将(-∞,10)，(10,30)，(30, +∞)三个范围区间锁住；Next-Key Lock 类似于上述两种锁的结合，它可以锁住的区间有为(-∞,10]，(10,30]，(30, +∞)，可以看出它即锁定了一个范围，也会锁定记录本身 InnoDB 对于行的查询都是默认采用 Next-Key lock 算法。当条件索引是唯一索引时，InnoDB 存储引擎会进行优化，将其降级为 Record Lock，即锁住索引本身 ，而不是范围，从而提高并发性。若是通过辅助索引查询，不但会给辅助索引加锁，还会为聚集索引上锁。对于辅助索引加 Next-Key Lock，而聚集索引因为是唯一的，所以只会加 Record Lock 对于没有显示创建索引的表，则会对 rowId 的聚集索引来加锁。如果操作未使用索引查询，那么会对表中所有记录加锁，实际效果和表锁一样 参考Mysql锁：灵魂七拷问 《MySQL技术内幕InnoDB存储引擎第2版》","link":"/2021/04/26/2021-04-26-mysql-innodb-locking/"},{"title":"锁与事务隔离级别","text":"锁与事务隔离级别… 悲观锁和乐观锁 悲观锁 在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制（也只有数据库层提供的锁机制才能真正保证数据访问的排他性，否则，即使在本系统中实现了加锁机制，也无法保证外部系统不会修改数据） 在悲观锁的情况下，为了保证事务的隔离性，就需要 一致性锁定读。读取数据时给加锁，其它事务无法修改这些数据。修改删除数据时也要加锁，其它事务无法读取这些数据 乐观锁(一致性非锁定读) 相对悲观锁而言，乐观锁机制采取了更加宽松的加锁机制。悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性 而乐观锁机制在一定程度上解决了这个问题。乐观锁，大多是基于数据版本（ Version ）记录机制实现。在基于数据库表的版本解决方案中，一般是通过为数据库表增加一个 “version” 字段来实现。读取出数据时，将此版本号一同读出，之后更新时，对此版本号加一。此时，将提交数据的版本数据与数据库表对应记录的当前版本信息进行比对，如果提交的数据版本号大于数据库表当前版本号，则予以更新，否则认为是过期数据 Read Committed在RC级别中，数据的读取都是不加锁的（采用一致性非锁定读），但是数据的写入、修改和删除是需要加锁的（仅使用Record Lock） 为了防止并发过程中的修改冲突，事务 A 给 a = 4 的数据行加锁，并一直不commit（释放锁），那么事务 B 也就一直拿不到该行锁，wait直到超时 上面这种情况是在 a 是有索引情况下，如果是没有索引的 e 字段呢？ 1update t set a=3 where e = 1; 那么MySQL会给整张表的所有数据行的加行锁。MySQL并不知道哪些数据行是 e = 1 的，如果一个条件无法通过索引快速过滤，存储引擎层面就会将所有记录加锁后返回，再由MySQL Server层进行过滤 但在实际使用过程当中，MySQL做了一些改进，在MySQL Server过滤条件，发现不满足后，会调用 unlock_row 方法，把不满足条件的记录释放锁 (违背了二段锁协议的约束)。这样做，保证了最后只会持有满足条件记录上的锁，但是每条记录的加锁操作还是不能省略的。（参见《高性能MySQL》中文第三版p181） 这种情况同样适用于MySQL的默认隔离级别RR。所以对一个数据量很大的表做批量修改的时候，如果无法使用相应的索引，MySQL Server过滤数据的时候特别慢，就会出现虽然没有修改某些行的数据，但是它们还是被锁住了的现象 Repeatable ReadRC（不可重读）模式下的展现 事务A 事务B begin; begin; select id,class_name,teacher_id from class_teacher where teacher_id=1;id | class_name | teacher_id1 | 初三二班 ｜12 | 初三一班 ｜1 update class_teacher set class_name=’初三三班’ where id=1; commit; select id,class_name,teacher_id from class_teacher where teacher_id=1;id | class_name | teacher_id1 | 初三三班 | 12 | 初三一班 | 1 commit; 事务 B 修改 id = 1 的数据提交之后，事务 A 同样的查询，后一次和前一次的结果不一样，这就是不可重读（重新读取产生的结果不一样） 这就很可能带来一些问题，那么我们来看看在 RR 级别中的表现： 事务A 事务B 事务C begin; begin; begin; select id,class_name,teacher_id from class_teacher where teacher_id=1;id | class_name | teacher_id1 | 初三二班 | 12 | 初三一班 | 1 update class_teacher set class_name=’初三三班’ where id=1;commit; insert into class_teacher values (null,’初三三班’,1);commit; select id,class_name,teacher_id from class_teacher where teacher_id=1;id | class_name | teacher_id1 | 初三二班 | 12 | 初三一班 | 1 没有读到事务B修改的数据，和第一次sql读取的一样，是可重复读的没有读到事务C新添加的数据，没有出现幻读问题 commit; 我们注意到，当 teacher_id=1 时，事务 A 先做了一次读取，事务 B 中间修改了 id=1 的数据，并commit之后，事务A第二次读到的数据和第一次完全相同。所以说它是可重读的 在 RR 隔离级别下，对于读 InnoDB 存储引擎同样使用 一致性非锁定读 ，但加锁上却和RC不同，其使用 Next-Key Lock MVCC在InnoDB中的实现在InnoDB中，会在每行数据后添加两个额外的隐藏的值来实现 MVCC，这两个值一个记录这行数据创建事务id（DB_TRX_ID），另外一个记录这行回滚指针（DB_ROLL_PTR） DB_TRX_ID：表示最近一次对本记录行作修改（insert 或 update）的事务ID。至于delete操作，InnoDB认为是一个update操作，不过会更新一个另外的删除位，将行表示为deleted。并非真正删除 DB_ROLL_PTR：回滚指针，指向 undo log 记录。每次对某条记录进行改动时，该列会存一个指针，可以通过这个指针找到该记录修改前的信息 。当某条记录被多次修改时，该行记录会存在多个版本，通过DB_ROLL_PTR 链接形成一个类似版本链的概念。 每开启一个新事务，事务的版本号就会递增。 在可重读Repeatable reads事务隔离级别下： SELECT时，读取行版本号 &lt;= 当前事务版本号，并且删除版本号为空或&gt;当前事务版本号 INSERT时，保存当前事务版本号为行的版本号 DELETE时，保存当前事务版本号为行的版本号，并修改删除标识符 UPDATE时，插入一条新纪录，保存当前事务版本号为行创建版本号，同时保存当前事务版本号到原来删除的行版本号，并修改删除标识符 通过MVCC，虽然每行记录都需要额外的存储空间，更多的行检查工作以及一些额外的维护工作，但可以减少锁的使用，大多数读操作都不用加锁，读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行，也只锁住必要行 MVCC 实现依赖的是 undo log 与 read view undo log 根据行为的不同，undo log分为两种： insert undo log 和 update undo log insert undo log： insert 操作中产生的undo log，因为insert操作记录只对当前事务本身课件，对于其他事务此记录不可见，所以 insert undo log 可以在事务提交后直接删除而不需要进行purge操作。 purge的主要任务是将数据库中已经 mark del 的数据删除，另外也会批量回收undo pages 数据库 Insert 时的数据初始状态： update undo log：update 或 delete 操作中产生的 undo log。 因为会对已经存在的记录产生影响，为了提供 MVCC机制，因此update undo log 不能在事务提交时就进行删除，而是将事务提交时放到入 history list 上，等待 purge 线程进行最后的删除操作。 数据第一次被修改时： 当另一个事务第二次修改当前数据： ReadView在 Innodb 中每个SQL语句执行前都会得到一个read view。 主要保存了当前数据库系统中正处于活跃（没有commit）的事务的ID号，其实简单的说这个副本中保存的是系统中当前不应该被本事务看到的其他事务id列表 Read view 的几个重要属性 trx_ids: 当前系统活跃(未提交)事务版本号集合。不包括当前事务自己 和已提交的事务（正在内存中） low_limit_id: 创建当前read view 时“当前系统最大事务版本号+1”。即下一个将被分配的事务ID up_limit_id: 创建当前read view 时“系统正处于活跃事务最小版本号”，如果trx_ids为空，则up_limit_id 为 low_limit_id creator_trx_id: 创建当前read view的事务版本号； ReadView 匹配条件 （1）数据事务ID &lt; up_limit_id 则显示 如果数据事务ID小于read view中的最小活跃事务ID，则可以肯定该数据是在当前事务启之前就已经存在了的，所以可以显示 （2）数据事务ID &gt;= low_limit_id 则不显示 如果数据事务ID大于 read view 中的当前系统的最大事务ID，则说明该数据是在当前read view 创建之后才产生的，所以数据不予显示 （3） up_limit_id &lt;= 数据事务ID &lt; low_limit_id 如果数据的事务ID大于等于最小的活跃事务ID，同时又小于等于系统最大的事务ID，这种情况就说明这个数据有可能是在当前事务开始的时候还没有提交的。 所以这时候我们需要把数据的事务ID与当前read view 中的活跃事务集合trx_ids 匹配: 情况1: 如果事务ID不存在于trx_ids 集合（则说明read view产生的时候事务已经commit了），这种情况数据则可以显示。 情况2： 如果事务ID存在trx_ids则说明read view产生的时候数据还没有提交，但是如果数据的事务ID等于creator_trx_id ，那么说明这个数据就是当前事务自己生成的，自己生成的数据自己当然能看见，所以这种情况下此数据也是可以显示的 情况3： 如果事务ID既存在trx_ids而且又不等于creator_trx_id那就说明read view产生的时候数据还没有提交，又不是自己生成的，所以这种情况下此数据不能显示 （4）不满足read view条件时候，从undo log里面获取数据 当数据的事务ID不满足read view条件时候，从undo log里面获取数据的历史版本，然后数据历史版本事务号回头再来和read view 条件匹配 ，直到找到一条满足条件的历史数据，或者找不到则返回空结果； 附判断源码： 在RC和RR隔离级别下MVCC的差异在事务隔离级别 RC 和 RR (InnoDB存储引擎的默认事务隔离级别)下， InnoDB存储引擎使用MVCC（非锁定一致性读），但它们生成 read view 的时机却不同 在 RC 隔离级别下的每次读取数据前都生成一个ReadView (m_ids列表) 在 RR 隔离级别下只在事务开始后 第一次 读取数据时生成一个ReadView（m_ids列表） MVCC解决不可重复读问题虽然 RC 和 RR 都通过 MVCC 来读取快照数据，但由于 生成 ReadView 时机不同，从而实现可重复读 举个例子： 在 RC 下 ReadView 生成情况 假设时间来到 T4 ，那么此时数据行 id = 1 的版本链为： 由于 RC 级别下每次查询都会生成 ReadView ，并且事务101、102并未提交，此时 ReadView 中活跃的事务 trx_ids ：[101,102] 。所以查询语句会用当前版本链去 trx_ids 中对比，查找小于最大事务id且不在列表中的，由此可以知道返回的数据为 name = '菜花' 时间线来到 T6 ，数据的版本链为： 重新生成 ReadView，这时事务101已经提交，102并未提交，所以此时 ReadView 中活跃的事务 **trx_ids ：[102]**，因此查询到的数据为 name = 李四 时间线来到 T9 ，数据的版本链为： 当前数据的版本链如上，因为当前的事务 101 和事务 102 都已经提交，所以此时的活跃事务的 trx_ids 的列表为空 ，因此查询语句会直接查询当前数据库最新数据，即查询到的是 name = 赵六 总结： 使用RC隔离级别的事务在每次查询开始时都会生成一个独立的 ReadView。 在 RR 下 ReadView 生成情况在可重复读级别下，只会在事务开始后第一次读取数据时生成一个ReadView（m_ids列表） 在 T4 情况下的版本链为： 在当前执行 select 语句时生成一个 ReadView，此时 **trx_ids ：[101,102]**，所以根据当前 ReadView 查询到的数据为 name = 菜花 时间点 T6 情况下： 由于 T4 的时间点已经生成了 ReadView，所以在当前的事务中只会生成一次ReadView，所以此时依然沿用 **trx_ids ：[101,102]**，所以此时查询数据依然是 name = 菜花 时间点 T9 情况下： 此时情况跟 T6 完全一样，由于已经生成了 ReadView，此时依然沿用 **trx_ids ：[101,102]**，所以查询数据依然是 name = 菜花 总结： 在 READ COMMITTED 中每次查询都会生成一个实时的 ReadView，做到保证每次提交后的数据是处于当前的可见状态。而REPEATABLE READ 中，在当前事务第一次查询时生成当前的 ReadView，并且当前的 ReadView 会一直沿用到当前事务提交，以此来保证可重复读。简单的说在 READ COMMITTED 事务隔离级别下，对于快照数据，非一致性读总是读取被锁定行的最新一份快照数据。而在 REPEATABLE READ事务隔离级别下，对于快照数据，非一致性读总是读取事务开始时的行数据版本 MVCC + Next-Key Lock 解决幻读问题可能在很多地方看到 RR 级别是可重复读的，但无法解决幻读问题，而只有在Serializable级别才能解决幻读。但在上面重现可重复读的额例子中，在事务 C 中添加了一条 teacher_id = 1 的数据并且 commit，RR 级别中应该会有幻读现象，事务 A 在查询 teacher_id = 1 的数据时会读到事务 C 新加的数据。但是测试后发现，是不存在这种情况的，在事务 C 提交后，事务A还是不会读到这条数据。可见 MVCC 在MySQL的RR级别中，是解决了幻读的读问题 快照读与当前读 快照读：在RR级别中，通过 MVCC 机制，让数据变得可重复读，但我们读到的数据可能是历史数据，是不及时的数据，不是数据库当前的数据！对于这种读取历史数据的方式，我们叫它快照读 (snapshot read)，通过 MVCC + undo log 来实现 **当前读**：当前读也就是 一致性锁定读 ，读的是当前最新版本的数据。读的时候需要给数据加锁，其它事务无法修改这些数据。其它事务可以读取这些数据，但读取到的是快照 根据定义快照读和当前读在 mysql 中分别值： 快照读：普通 select select * from table ….; 当前读：特殊的读操作，插入/更新/删除操作，属于当前读，还有手动加锁读 select * from table where ? lock in share mode; select * from table where ? for update; insert; update ; delete; 只靠 MVCC 实现 RR 隔离级别，可以保证可重复读，还能防止部分幻读，但并不是完全防止。 比如事务 A 开始后，执行普通 select 语句，创建了快照；之后事务 B 执行 insert 语句；然后事务 A 再执行普通 select 语句，得到的还是之前B没有 insert 过的数据，因为这时候 A 读的数据是符合快照可见性条件的数据。这就防止了部分幻读，此时事务 A 是快照读 这点上面介绍过也复现过 但是，如果事务 A 执行的不是普通 select 语句，而是 select … for update / update 等语句。这时候，事务 A 是 当前读，每次语句执行的时候都是获取的最新数据。也就是说，在只有 MVCC 时，A 先执行 select … where nid between 1 and 10 … for update；然后事务B再执行 insert … nid = 5 …；然后 A 再执行 select … where nid between 1 and 10 … for update，就会发现，多了一条B insert进去的记录。这就产生幻读了，所以单独靠MVCC并不能完全防止幻读 在默认隔离级别 REPEATABLE READ 下，InnoDB 中行锁默认使用算法 Next-Key Lock，只有当查询的索引是唯一索引或主键时，InnoDB会对 Next-Key Lock 进行优化，将其降级为 Record Lock，即仅锁住索引本身，而不是范围 当查询的索引为辅助索引时，InnoDB则会使用Next-Key Lock进行加锁。InnoDB对于辅助索引有特殊的处理，不仅会锁住辅助索引值所在的范围，还会将其下一键值加上Gap Lock 123456CREATE TABLE e4 (a INT, b INT, PRIMARY KEY(a), KEY(b));INSERT INTO e4 SELECT 1,1;INSERT INTO e4 SELECT 3,1;INSERT INTO e4 SELECT 5,3;INSERT INTO e4 SELECT 7,6;INSERT INTO e4 SELECT 10,8; 然后执行下面的语句： 1SELECT * FROM e4 WHERE b=3 FOR UPDATE; 因为通过辅助索引b来进行查询，所以 InnoDB 会使用 Next-Key Lock 进行加锁，并且还会对主键索引a进行加锁。对于主键索引a，仅仅对值为5的索引加上 Record Lock。而对于索引b，需要加上 Next-Key Lock 索引，锁定的范围是(1,3]。除此之外，还会对其下一个键值加上Gap Lock，即还有一个范围为(3,6)的锁 再新开一个会话，执行下面的SQL语句，会发现都会被阻塞： 123SELECT * FROM e4 WHERE a = 5 FOR UPDATE; # 主键a被锁INSERT INTO e4 SELECT 4,2; # 插入行b的值为2，在锁定的(1,3]范围内INSERT INTO e4 SELECT 6,5; # 插入行b的值为5，在锁定的(3,6)范围内 若此时没有 Gap Lock 锁定(3,6) ，虽然会话A锁住了 b = 3 这条记录，但是会话B可以插入一条值为4的记录，这会导致会话A中再次执行查询时会返回不同的记录，即导致幻读问题 InnoDB 存储引擎采用 Next-Key Lock 来解决幻读问题。因为 Next-Key Lock 是锁住一个范围，所以就不会产生幻读问题。但是需要注意的是，InnoDB 只在 Repeatable Read 隔离级别下使用该机制 参考《MySQL技术内幕InnoDB存储引擎第2版》 Innodb中的事务隔离级别和锁的关系 MySQL事务与MVCC如何实现的隔离级别 Innodb MVCC实现原理","link":"/2021/05/10/2021-05-10-locking-and-isolation-level/"},{"title":"mybatis动态sql的问题","text":"最近组内有个小伙伴求助，说在 xml 中写 mybatis 动态sql，明明 if 条件是成立的，但实际执行的 sql 语句并没有拼接上，而是走了另外的判断分支 遇到的问题最近组内有个小伙伴求助，说在 xml 中写 mybatis 动态sql，明明 if 条件是成立的，但实际执行的 sql 语句并没有拼接上，而是走了另外的判断分支 问题复现xml 伪代码： 1234567891011&lt;select id=&quot;select&quot; resultType=&quot;com.ddmcc.mybatis.bindings.entity.Book&quot;&gt; SELECT * FROM book WHERE 1 = 1 &lt;if test=&quot;book.type = 1&quot;&gt; AND book_name = '书籍1' &lt;/if&gt; &lt;if test=&quot;book.type == 2&quot;&gt; AND book_name = '书籍2' &lt;/if&gt;&lt;/select&gt; 如果 type = 1 ，那么拼接条件 AND book_name = '书籍1' ，如果 type = 2 ，那么拼接条件 AND book_name = '书籍2' 查询伪代码： 1234567@Testvoid select() { Book book = new Book(); book.setType(2); System.out.println(bookMapper.select(book)); System.out.println(book);} 按照想的应该要拼接上 AND book_name = '书籍2' 的查询条件，因为满足 type = 2。然而并没有！ 并且在经过查询后，参数对象 book 的type字段值变成了1 ！ 发现问题原因如果认真一点看就能够发现，在第一个 if 标签中，写的是 = 号，第二个中写的是 == 。所以到这里大概也就能猜到， 是因为第一个 if 给type重新赋值变成了1，且赋值操作返回结果为 true ，导致进了第一个if判断，而第二个if因为type = 2，所以不满足 mybatis中动态sql的实现 在初始化阶段，mybatis会解析xml文件中的sql，并解析成对象（SqlSource），比如动态sql会被解析成 DynamicSqlSource 然后在 XMLScriptBuilder 中会根据动态标签解析成相应的节点对象，比如 if 标签 对应 IfSqlNode 节点对象 所有节点对象在 org.apache.ibatis.scripting.xmltags 包目录下 如本例子的sql 被解析成5个node 调用各自 SqlNode 对象的 apply方法去执行相应判断，拼接sql，如：StaticTextSqlNode 静态sql则可以直接拼接 123456// StaticTextSqlNode ====&gt; apple直接把sql拼接到context@Overridepublic boolean apply(DynamicContext context) { context.appendSql(text); return true;} 如果是 IfSqlNode 则会用 Ognl 执行表达式来获取结果，根据结果判断拼接sql evaluateBoolean 方法结果为真，拼接字符串 123456789// IfSqlNode#33 ====&gt; 执行表达式@Overridepublic boolean apply(DynamicContext context) { if (evaluator.evaluateBoolean(test, context.getBindings())) { contents.apply(context); return true; } return false;} 12345678910111213141516171819// OgnlCache#41 ====&gt; 通过Ognl传入参数来执行表达式public static Object getValue(String expression, Object root) { try { Map&lt;Object, OgnlClassResolver&gt; context = Ognl.createDefaultContext(root, new OgnlClassResolver()); return Ognl.getValue(parseExpression(expression), context, root); } catch (OgnlException e) { throw new BuilderException(&quot;Error evaluating expression '&quot; + expression + &quot;'. Cause: &quot; + e, e); }}// OgnlCache#50 ====&gt; 解析表达式，缓存private static Object parseExpression(String expression) throws OgnlException { Object node = expressionCache.get(expression); if (node == null) { node = Ognl.parseExpression(expression); expressionCache.put(expression, node); } return node;} 总结到这里基本也就清楚了，mybatis 其实是用 Ognl 来对表达式进行判断，本例第一个 ifSqlNode 对变量进行赋值操作并返回了结果为1，判断1为真，执行 sql 拼接","link":"/2021/05/22/2021-05-22-mybatis-dynamic-sql/"},{"title":"Java虚拟机类加载过程与类加载机制","text":"虚拟机把16进制描述类的 .class 文件加载到内存，并对数据进行校验、解析和初始化等操作，最终变为可以被虚拟机使用的 Java 类型，这就是虚拟机的类加载机制 或者说 通过类加载机制，虚拟机把类的字节码转换成运行时数据结构，并保存在方法区，在内存中会生成一个代表这个类数据结构的 java.lang.Class 对象，后续访问这个类的数据结构就可以通过这个 Class 对象来访问 类加载过程虚拟机把16进制描述类的 .class 文件加载到内存，并对数据进行校验、解析和初始化等操作，最终变为可以被虚拟机使用的 Java 类型，这就是虚拟机的类加载机制 或者说 通过类加载机制，虚拟机把类的字节码转换成运行时数据结构，并保存在方法区，在内存中会生成一个代表这个类数据结构的 java.lang.Class 对象，后续访问这个类的数据结构就可以通过这个 Class 对象来访问 文件内容是按照 类文件结构 规定的存储结构存储的 如下图为编译后的 class 文件： 类加载过程包括：加载（Loading）、连接（Linking）、初始化（Initialization）3个阶段。其中连接过程又可以分为验证（Verification）、准备（Preparation）、解析（Resolution）三个阶段 加载阶段（loading）加载（创建）阶段主要是通过类的全限定名来查找这个类的文件，将静态的类文件存储结构转化为运行时数据结构，并在内存中生成一个代表这个类的 java.lang.Class 对象，作为这个类的数据访问入口 将静态的类文件存储结构转化为运行时数据结构：意思就是在 .java 被编译成 .class 文件后，class文件是按照严格的存储结构进行存储的，比如一个字符串常量 类加载器如果某个类不是数组类，类文件则通过 类加载器 进行加载。类加载器主要有两种：Java虚拟机提供的引导类加载器和用户自定义的类加载器 Java虚拟机提供的： 启动类（或根类）加载器（Bootstrap ClassLoader） 这个加载器不是一个Java类，而是由虚拟机底层的 c/c++ 实现，负责将存放在 JAVA_HOME 下lib目录中的类库，比如 rt.jar。因此，启动类加载器不属于 Java 类库，无法被Java程序直接引用 扩展类加载器（ExtClassLoader） ​ 由 sun.misc.Launcher$ExtClassLoader 实现，负责加载 JAVA_HOME 下 lib.ext 目录下的，或者被 java.ext.dirs 系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器 应用类加载器（AppClassLoader） 由 sun.misc.Launcher$AppClassLoader 实现的。由于这个类加载器是 ClassLoader 中的 getSystemClassLoader 方法的返回值，所以也叫系统类加载器。它负责加载用户类路径上所指定的类库，可以被直接使用。如果未自定义类加载器，默认为该类加载器 加载器对应加载路径： 用户自定义类加载器： 每个用户定义的类加载程序都是抽象类 ClassLoader 子类的实例。用户定义的类加载程序可用于创建源自用户定义源的类。例如，类可以通过网络下载、实时生成或从加密文件中提取 对于加载器的初始化：除启动类加载器外，扩展类加载器和应用类加载器都是通过类sun.misc.Launcher进行初始化，而Launcher类则由根类加载器进行加载 双亲委派加载机制当一个类加载器接收到类加载请求时，会先请求其父类加载器进行递归，如果父类能够找到该类，则由父加载器加载；当父类加载器无法找到该类时（根据类的全限定名称），子类加载器才会尝试去加载 加载类的源码实现在 ClassLoader#loadClass ，核心源码如下： 123456789101112131415161718192021222324252627282930313233protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException{ // 进行类加载操作时首先要加锁，避免并发加载 synchronized (getClassLoadingLock(name)) { // 首先，检查类是否已经加载 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) { long t0 = System.nanoTime(); try { // 如果父加载器不为空，递归调用 if (parent != null) { c = parent.loadClass(name, false); } else { // 如果当前类没有被加载且父加载器为null，则请求根类加载器进行加载操作 // 这里面调用本地方法加载 c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { } if (c == null) { // 如果父类加载器加载失败，则由当前类加载器进行加载，自定义加载器实现该方法进行加载 long t1 = System.nanoTime(); c = findClass(name); // 其它操作... } } if (resolve) { resolveClass(c); } return c; }} 这种委派机制是以 组合的方式 实现的，如我们自定义一个 MyClassLoader 继承 ClassLoader ，并且不指定父加载器： 可以看到不同加载器通过组合的方式实现。经过观察上图也产生两个疑惑： 我并没有给自定义加载器 MyClassLoader 指定父加载器，为什么会有父加载器，并且是 AppClassLoader？ AppClassLoader 父类加载器就是ExtClassLoader吗？ 针对 问题2，在 Launcher 类初始化时，先初始化 ExtClassLoader ，然后在初始化 AppClassLoader 时把 ExtClassLoader 作为父加载器传入。下面为初始化源码： 12345678910111213141516171819public Launcher() { Launcher.ExtClassLoader var1; try { // 初始化创建ExtClassLoader var1 = Launcher.ExtClassLoader.getExtClassLoader(); } catch (IOException var10) { throw new InternalError(&quot;Could not create extension class loader&quot;, var10); } try { // 初始化AppClassLoader将ExtClassLoader作为父加载器传入 this.loader = Launcher.AppClassLoader.getAppClassLoader(var1); } catch (IOException var9) { throw new InternalError(&quot;Could not create application class loader&quot;, var9); } // 设置AppClassLoader为线程上下文loader Thread.currentThread().setContextClassLoader(this.loader); // 其它安全初始化} 而 问题1 则要看 ClassLoader 的无参构造方法，在构造方法中会调用 getSystemClassLoader() 方法取获取系统的ClassLoader，内部实现其实就是取获取 Launcher 类的 loader 属性，也就是上面问题2代码片段中的 this.loader = ...，所以自定义loader未指定父加载器，默认就为 AppClassLoader 通过以上的分析，我们也就可以知道加载器之间的关系为： 连接阶段（linking）JVM规范规定： 类或接口在链接之前完全加载 类或接口在初始化之前会对其进行完全验证和准备 验证阶段（Verification）验证的目的主要为了确保文件中的表示满足 静态语法或结构的约束 ，及安全性校验。主要包括文件格式验证、元数据验证、字节码验证、符号引用验证 文件格式验证：验证字节流是否符合Class文件格式的规范；比如，是否以魔术0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。只有验证通过才会进入方法区进行存储 元数据验证：对字节码描述的信息进行语义分析，以保证其描述的信息符合Java语言规范的要求；比如，是否有父类（除Object类）、父类是否为final修饰、是否实现抽象方法或接口、重载是否正确等 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。比如，保证数据类型与指令正常配合工作、指令不会跳转到方法体外的字节码上，方法体中的类型转换是有效的等 符号引用验证：在虚拟机将符号引用转化为直接引用的时候进行验证，可以看做是对类自身以外的信息（常量池中的各种符号引用）进行匹配性的校验。常见的异常比如：java.lang.NoSuchMethdError、java.lang.NoSuchFiledError等 准备阶段（Preparation）准备阶段主要为类或接口的静态字段分配内存，并将此类字段初始化为其默认值，变量所使用的内存都将在 方法区 进行分配。对于该阶段有以下几点需要注意： 这时候进行内存分配的仅包括类变量（ Class Variables ，即静态变量，被 static 关键字修饰的变量，只与类相关，因此被称为类变量），而不包括实例变量。实例变量会在对象实例化时随着对象一块分配在 Java 堆中。 从概念上讲，类变量所使用的内存都应当在 方法区 中进行分配。不过有一点需要注意的是：JDK 7 之前，HotSpot 使用永久代来实现方法区的时候，实现是完全符合这种逻辑概念的。 而在 JDK 7 及之后，HotSpot 已经把原本放在永久代的字符串常量池、静态变量等移动到堆中，这个时候类变量则会随着 Class 对象一起存放在 Java 堆中 这里所设置的初始值”通常情况”下是数据类型默认的零值（如 0、0L、null、false 等），比如我们定义了public static int value=111 ，那么 value 变量在准备阶段的初始值就是 0 而不是 111（初始化阶段才会赋值）。特殊情况：比如给 value 变量加上了 final 关键字public static final int value=111 ，那么准备阶段 value 的值就被赋值为 111。 静态字段的显示初始化会在初始化阶段（Initialization）进行，而不是在准备阶段。并且准备阶段可以在加载后的任何时候进行，但必须在初始化之前完成 比如类中有字段： 12public static int a = 1;public static final String b = &quot;123&quot;; 那么在此阶段，对于字段 a 会被初始化为 0，在初始化阶段才会被赋值为 1。对于字段 b 有一点特殊，因为是常量，会在此阶段被初始化为指定的值，即：b = 123 解析阶段（Resolution）解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符 7 类符号引用进行 符号引用就是一组符号来描述目标，可以是任何字面量。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。在程序实际运行时，只有符号引用是不够的，举个例子：在程序执行方法时，系统需要明确知道这个方法所在的位置。Java 虚拟机为每个类都准备了一张方法表来存放类中所有的方法。当需要调用一个类的方法的时候，只要知道这个方法在方法表中的偏移量就可以直接调用该方法了。通过解析操作符号引用就可以直接转变为目标方法在类中方法表的位置，从而使得方法可以被调用。 综上，解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量 了解更多关于符号引用替换为直接引用的过程 初始化（Initialization）初始化阶段就是执行初始化方法 &lt;clinit&gt; () 方法的过程，是类加载的最后一步，这一步 JVM 才开始真正执行类中定义的 Java 程序代码(字节码) &lt;clinit&gt; ()方法是编译之后自动生成的 对于 &lt;clinit&gt; () 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 &lt;clinit&gt; () 方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起多个进程阻塞，并且这种阻塞很难被发现 对于初始化阶段，虚拟机严格规范了有且只有 5 种情况下，必须对类进行初始化(只有主动去使用类才会初始化类)： 当遇到 new 、 getstatic、putstatic 或 invokestatic 这 4 条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。 当 jvm 执行 new 指令时会初始化类。即当程序创建一个类的实例对象。 当 jvm 执行 getstatic 指令时会初始化类。即程序访问类的静态变量(不是静态常量，常量会被加载到运行时常量池)。 当 jvm 执行 putstatic 指令时会初始化类。即程序给类的静态变量赋值。 当 jvm 执行 invokestatic 指令时会初始化类。即程序调用类的静态方法。 使用 java.lang.reflect 包的方法对类进行反射调用时如 Class.forname(“…”), newInstance() 等等。如果类没初始化，需要触发其初始化 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类 MethodHandle 和 VarHandle 可以看作是轻量级的反射调用机制，而要想使用这 2 个调用， 就必须先使用 findStaticVarHandle 来初始化要调用的类 当一个接口中定义了 JDK8 新加入的默认方法（被 default 关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化 卸载卸载类即该类的 Class 对象被 GC 卸载类需要满足 3 个要求: 该类的所有的实例对象都已被 GC，也就是说堆不存在该类的实例对象。 该类没有在其他任何地方被引用 该类的类加载器的实例已被 GC 所以，在 JVM 生命周期中，由 jvm 自带的类加载器加载的类是不会被卸载的。但是由我们自定义的类加载器加载的类是可能被卸载的 只要想通一点就好了，jdk 自带的 BootstrapClassLoader, ExtClassLoader, AppClassLoader 负责加载 jdk 提供的类，所以它们(类加载器的实例)肯定不会被回收。而我们自定义的类加载器的实例是可以被回收的，所以使用我们自定义加载器加载的类是可以被卸载掉的","link":"/2021/05/29/2021-05-29-jvm-class-file-loading-process/"},{"title":"利用微信公众号+腾讯云cos制作图床","text":"工作流程图床 向 公众号 发送图片，微信服务器收到后推送给配置的接口 接口收到消息、校验、解析根据消息类型路由到具体的处理类 下载图片后上传到cos服务器 组装图片地址信息返回给发送者 自动部署 向 github、gitlab 等推送代码，触发 webhooks 通知阿里云镜像服务 镜像服务接口到请求、拉取代码、根据 Dockerfile 构建镜像，完后根据触发器配置链接通知 jenkins jenkins 接收到请求、拉取镜像、部署 准备工作图床 开通腾讯云cos服务、创建桶 gitlab、github创建代码仓库，配置 webhooks 开通阿里云镜像服务，创建、配置代码仓库，构建规则、触发器 申请公众号 部署准备部署的话，因为我为了方便，所以打成镜像用 docker 运行了。也可以打成 jar 包直接运行 准备一台服务器 安装docker、docker-compose 安装jenkins docker-compose.yml 12345678910111213141516171819version: '3.0'services: jenkins: container_name: jenkins image: 'jenkins/jenkins:lts' restart: always user: root environment: - TZ=Asia/Shanghai ports: - '8081:8080' - '50001:50000' volumes: - /usr/bin/docker:/usr/bin/docker - /var/run/docker.sock:/var/run/docker.sock - /usr/local/bin/docker-compose:/usr/local/bin/docker-compose - ./jenkins:/var/jenkins_home - /usr/local/logs:/usr/local/logs - /usr/lib64/libltdl.so.7:/usr/lib/x86_64-linux-gnu/libltdl.so.7 安装nginx docker-compose.yml 12345678910111213141516version: '3.1'services: nginx: image: nginx:latest container_name: nginx restart: always volumes: - ./html:/usr/share/nginx/html - ./nginx.conf:/etc/nginx/nginx.conf - ./2_api.ddmcc.cn.key:/etc/nginx/2_api.ddmcc.cn.key # 这个是证书的没有可以不用 - ./1_api.ddmcc.cn_bundle.crt:/etc/nginx/1_api.ddmcc.cn_bundle.crt # 这个是证书的没有可以不用 - ./logs:/var/log/nginx - ./wwwroot:/usr/share/nginx/wwwroot ports: - 80:80 - 443:443 nginx.cnf 123456789101112131415161718192021222324252627282930worker_processes 1;events { worker_connections 1024;}http { include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; log_format main '$remote_addr - $remote_user [$time_local] &quot;$request&quot; ' '$status $body_bytes_sent &quot;$http_referer&quot; ' '&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;'; access_log /var/log/nginx/access.log main; server { listen 80; charset utf-8; server_name api.ddmcc.cn; location /index { alias /usr/share/nginx/html; index index.html; } location ^~/api/ { proxy_pass http://127.0.0.1:10001/wxServer/api/; } }} 编写实现pom.xml 中引入依赖123456789101112&lt;!-- 微信开发工具包 https://github.com/Wechat-Group/WxJava --&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.binarywang&lt;/groupId&gt; &lt;artifactId&gt;weixin-java-mp&lt;/artifactId&gt; &lt;version&gt;4.0.1.B&lt;/version&gt;&lt;/dependency&gt; &lt;!-- cos java sdk https://mvnrepository.com/artifact/com.qcloud/cos_api --&gt;&lt;dependency&gt; &lt;groupId&gt;com.qcloud&lt;/groupId&gt; &lt;artifactId&gt;cos_api&lt;/artifactId&gt; &lt;version&gt;5.6.34&lt;/version&gt;&lt;/dependency&gt; 配置微信开发工具设置与开发 –&gt; 基本配置 –&gt; 服务器配置 地址 application.yml 中加入 12345678wx: mp: useRedis: false configs: - appId: # 公众号的appid secret: # 公众号的appsecret token: # 接口配置里的Token值 aesKey: # 接口配置里的EncodingAESKey值 服务器地址(URL) 根据自己 nginx 配置和接口地址来配置如：http://api.ddmcc.cn/api/wx/message Controller类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687@Slf4j@RestController@RequestMapping(&quot;/api/wx&quot;)public class WeChatController { @Autowired private WxMpService wxService; @Autowired private WxMpMessageRouter messageRouter; @Autowired private SignUtil signUtil; @GetMapping(value = &quot;/message&quot;) public String message(@RequestParam(value = &quot;signature&quot;) String signature, @RequestParam(value = &quot;timestamp&quot;) String timestamp, @RequestParam(value = &quot;nonce&quot;) String nonce, @RequestParam(value = &quot;echostr&quot;) String echostr) { log.info(&quot;微信校验接口：signature={},timestamp={},nonce={},echostr={}&quot;, signature, timestamp, nonce, echostr); try { if (signUtil.checkSignature(signature, timestamp, nonce)) { log.info(&quot;接口校验成功！！&quot;); return echostr; } return null; } catch (Exception e) { log.error(&quot;微信校验失败&quot;, e); } return null; } @PostMapping(value = &quot;/message&quot;, produces = &quot;application/xml; charset=UTF-8&quot;) public String message(@RequestBody String requestBody, @RequestParam(&quot;signature&quot;) String signature, @RequestParam(&quot;timestamp&quot;) String timestamp, @RequestParam(&quot;nonce&quot;) String nonce, @RequestParam(&quot;openid&quot;) String openid, @RequestParam(name = &quot;encrypt_type&quot;, required = false) String encType, @RequestParam(name = &quot;msg_signature&quot;, required = false) String msgSignature) { log.info(&quot;\\n接收微信请求：[openid=[{}], [signature=[{}], encType=[{}], msgSignature=[{}],&quot; + &quot; timestamp=[{}], nonce=[{}], requestBody=[\\n{}\\n] &quot;, openid, signature, encType, msgSignature, timestamp, nonce, requestBody); if (!wxService.checkSignature(timestamp, nonce, signature)) { throw new IllegalArgumentException(&quot;非法请求，可能属于伪造的请求！&quot;); } String out = null; if (encType == null) { // 明文传输的消息 WxMpXmlMessage inMessage = WxMpXmlMessage.fromXml(requestBody); WxMpXmlOutMessage outMessage = this.route(inMessage); if (outMessage == null) { return &quot;&quot;; } out = outMessage.toXml(); } else if (&quot;aes&quot;.equalsIgnoreCase(encType)) { // aes加密的消息 WxMpXmlMessage inMessage = WxMpXmlMessage.fromEncryptedXml(requestBody, wxService.getWxMpConfigStorage(), timestamp, nonce, msgSignature); log.debug(&quot;\\n消息解密后内容为：\\n{} &quot;, inMessage.toString()); WxMpXmlOutMessage outMessage = this.route(inMessage); if (outMessage == null) { return &quot;&quot;; } out = outMessage.toEncryptedXml(wxService.getWxMpConfigStorage()); } log.debug(&quot;\\n组装回复信息：{}&quot;, out); return out; } private WxMpXmlOutMessage route(WxMpXmlMessage message) { try { return this.messageRouter.route(message); } catch (Exception e) { log.error(&quot;路由消息时出现异常！&quot;, e); } return null; } 图片消息处理类 ImageHandle12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576@Componentpublic class ImageHandler extends AbstractHandler { @Autowired private CosUploader uploader; @Autowired private CosProperties cosProperties; /** * 处理微信推送消息. * * @param wxMessage 微信推送消息 * @param context 上下文，如果handler或interceptor之间有信息要传递，可以用这个 * @param wxMpService 服务类 * @param sessionManager session管理器 * @return xml格式的消息，如果在异步规则里处理的话，可以返回null */ @SneakyThrows @Override public WxMpXmlOutMessage handle(WxMpXmlMessage wxMessage, Map&lt;String, Object&gt; context, WxMpService wxMpService, WxSessionManager sessionManager) { // 1）下载图片 URLConnection connection = urlConnection(wxMessage.getPicUrl()); int length = connection.getContentLength(); try (InputStream inputStream = connection.getInputStream()) { String uuid = UUID.randomUUID().toString(); // 2）上传到cos PutObjectResult uploadResult = uploader.upload(inputStream, uuid + &quot;.png&quot;, length); if (uploadResult != null) { // 3）返回信息拼接 String url = &quot;https://&quot; + cosProperties.getBucketName() + &quot;.file.myqcloud.com/&quot; + uuid + &quot;.png&quot;; String content = getContent(wxMessage.getPicUrl(), url); return WxMpXmlOutMessage.TEXT() .fromUser(wxMessage.getToUser()) .toUser(wxMessage.getFromUser()) .content(content) .build(); } } return WxMpXmlOutMessage.TEXT() .fromUser(wxMessage.getToUser()) .toUser(wxMessage.getFromUser()) .content(&quot;图片上传失败！！&quot;) .build(); } @SneakyThrows private URLConnection urlConnection(String path) { URL url = new URL(path); URLConnection connection = url.openConnection(); connection.setRequestProperty(&quot;Accept-Encoding&quot;, &quot;identity&quot;); connection.setRequestProperty(&quot;User-Agent&quot;, &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/37.0.2062.120 Safari/537.36&quot;); return connection; } private String getContent(String picUrl, String url) { return String.format(&quot;腾讯系网站链接：&quot; + &quot;\\n%s&quot; + &quot;\\n\\n![markdown](%s)&quot; + &quot;\\n\\n\\n&quot; + &quot;其它网站链接：&quot; + &quot;\\n%s&quot; + &quot;\\n\\n![markdown](%s)&quot;, picUrl, picUrl, url, url); }} 上传工具类 CosUploader1234567891011121314151617181920212223242526272829@Componentpublic class CosUploader implements Uploader { @Autowired private CosProperties cosProperties; @SneakyThrows @Override public PutObjectResult upload(InputStream inputStream, String key, int length) { COSCredentials cred = new BasicCOSCredentials(cosProperties.getSecretId(), cosProperties.getSecretKey()); ClientConfig clientConfig = new ClientConfig(new Region(cosProperties.getRegion())); // 3 生成 cos 客户端。 ObjectMetadata objectMetadata = new ObjectMetadata(); objectMetadata.setContentLength(length); objectMetadata.setContentType(&quot;image/png&quot;); COSClient cosClient = new COSClient(cred, clientConfig); PutObjectRequest putObjectRequest = new PutObjectRequest(cosProperties.getBucketName(), key, inputStream, objectMetadata); try { return cosClient.putObject(putObjectRequest); } catch (CosClientException e) { e.printStackTrace(); } finally { cosClient.shutdown(); } return null; }} cos上传服务配置配置cos服务密钥、存储桶、区域信息 地址 application.yml 中加入 123456wx: cos: secret-id: # secretId secret-key: # secretKey bucket-name: # 存储桶名称 region: # 区域 配置属性类 123456789101112@Data@ConfigurationProperties(prefix = &quot;wx.cos&quot;)public class CosProperties { private String secretId; private String secretKey; private String region; private String bucketName;} 持续自动部署配置Dockerfile12345678FROM openjdk:8-jreFROM maven:3.5.3RUN mkdir /appADD . /app/WORKDIR /appRUN mvn clean packageENTRYPOINT [&quot;java&quot;, &quot;-jar&quot;, &quot;/app/bills-application/target/bills-application-1.0.0.jar&quot;, &quot;--spring.profiles.active=prod&quot;]EXPOSE 10001 自动化持续部署github+jenkins自动化持续部署 github+阿里云容器镜像服务+jenkins自动化持续部署 最后看看效果","link":"/2021/07/10/2021-07-10-use-wx-and-oss-create-image-hosting-website/"},{"title":"饿汉单例真的是空间换时间吗？","text":"饿汉、懒汉单例模式先复习回顾一下这两种单例的写法： 饿汉模式1234567891011public class SingletonPatternTest1 { private static SingletonPatternTest1 instance = new SingletonPatternTest1(); private SingletonPatternTest1() { } public static SingletonPatternTest1 getInstance() { return instance; }} 懒汉模式1234567891011121314151617public class SingletonPatternTest1 { private static volatile SingletonPatternTest1 instance; private SingletonPatternTest1() { } public static SingletonPatternTest1 getInstance() { if (instance == null) { synchronized (SingletonPatternTest1.class) { if (instance == null) { instance = new SingletonPatternTest1(); } } } return instance; }} 通常我们对这两种写法的认识是：懒汉式在类加载阶段不会初始化单例，在调用 getInstance() 时，对象才会被创建。而饿汉式单例在类加载阶段就已经初始化了，典型的空间换时间… 饿汉单例在类加载阶段真的会被初始化吗？ 先说答案：不会 我们先复习一下类加载过程：包括加载、验证、准备、解析、初始化、使用 和 卸载 阶段 其中在 准备阶段 会给类变量（即静态变量）初始化 零值 ，如上面示例中的 instance 为引用类型，所以会被初始化为 null。类变量 显示初始化 工作是在 初始化阶段 clinit 方法中， 顺序完成父类子类静态成员变量显示初始化和父类子类静态代码块语句 特殊情况：对于被 static final 修饰的基本数据类型常量，会在 “准备” 阶段显示初始化 所以 不管饿汉、懒汉单例在准备阶段都不会被显示初始化，实例化工作都是在初始化阶段完成的，那么这两种方式又在什么时候会被显示初始化呢？（什么时候执行初始化阶段） 对于 初始化阶段 ，虚拟机严格规范了有且只有 5 种情况下，必须对类进行初始化(只有主动去使用类才会初始化类)： 当 jvm 执行 new 指令时会初始化类。即当程序创建一个类的实例对象 当 jvm 执行 getstatic 指令时会初始化类。即程序访问类的静态变量(不是静态常量，常量会被加载到运行时常量池) 当 jvm 执行 putstatic 指令时会初始化类。即程序给类的静态变量赋值 当 jvm 执行 invokestatic 指令时会初始化类。即程序调用类的静态方法 使用 java.lang.reflect 包的方法对类进行反射调用时如 Class.forname(&quot;...&quot;), newInstance() 等等。如果类没初始化，需要触发其初始化 初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。 当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。 当一个接口中定义了 JDK8 新加入的默认方法（被 default 关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化 结论也就是说 只有在第一次被调用的时候，单例对象才会被显示初始化（new SingletonPatternTest1()），在这种情况下饿汉和懒汉是没什么区别的（懒汉还需要判断、加锁）。所以不存在空间换时间的说法，因为饿汉的单例对象也没创建 特殊情况在单例模式中虽然大多数都是调用 getInstance 方法，但是也不能确定有其他的方式（或者调用其他的静态方法）导致类执行显示初始化。这时候懒汉模式对于饿汉就有懒加载的优势 其它最后还是推荐静态内部类单例的写法，在未调用 getInstance 时，内部类不会被加载内存，实例对象自然也不会被实例化！并且它是 线程安全的 对于 clinit 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 () 方法是带锁线程安全 123456789101112public class SingletonPattenTest { private SingletonPattenTest(){} private static class SingletonHolder { private static SingletonPattenTest instance = new SingletonPattenTest(); } public static SingletonPattenTest getInstance(){ return SingletonHolder.instance; }}","link":"/2021/07/13/2021-07-13-difference-between-the-lazy-and-eager-singleton/"},{"title":"关于类解析阶段符号引用转为直接引用的过程","text":"在 类加载过程 中介绍了 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量 那么什么是 符号引用? 什么又是 **直接引用**？二者之间又有什么关联？字段、方法在内存中又是如何存储和访问的？下面就通过 class 文件来具体分析 符号引用有下面两个类User 和 Address： 123456789101112131415161718192021222324252627282930/** * @author jiangrz */public class User { private boolean sex; private String name; private Integer age; private Address address; public void printName() { address.printAddress(); }}/** * @author jiangrz */public class Address { private String province; public void printAddress() { }} 其中 User 类编译出来的 class 文件的文本形式如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374Classfile /Users/jiangrz/workspace/other/demo/target/classes/com/example/demo/User.class Last modified 2021年6月14日; size 553 bytes SHA-256 checksum d9a9b9e80018881155b444972b9bcc32d83988780b8fbdf5e717ca49c1a5b944 Compiled from &quot;User.java&quot;public class com.example.demo.User minor version: 0 major version: 52 flags: (0x0021) ACC_PUBLIC, ACC_SUPER this_class: #4 // com/example/demo/User super_class: #5 // java/lang/Object interfaces: 0, fields: 4, methods: 2, attributes: 1Constant pool: #1 = Methodref #5.#24 // java/lang/Object.&quot;&lt;init&gt;&quot;:()V #2 = Fieldref #4.#25 // com/example/demo/User.address:Lcom/example/demo/Address; #3 = Methodref #26.#27 // com/example/demo/Address.printAddress:()V #4 = Class #28 // com/example/demo/User #5 = Class #29 // java/lang/Object #6 = Utf8 sex #7 = Utf8 Z #8 = Utf8 name #9 = Utf8 Ljava/lang/String; #10 = Utf8 age #11 = Utf8 Ljava/lang/Integer; #12 = Utf8 address #13 = Utf8 Lcom/example/demo/Address; #14 = Utf8 &lt;init&gt; #15 = Utf8 ()V #16 = Utf8 Code #17 = Utf8 LineNumberTable #18 = Utf8 LocalVariableTable #19 = Utf8 this #20 = Utf8 Lcom/example/demo/User; #21 = Utf8 printName #22 = Utf8 SourceFile #23 = Utf8 User.java #24 = NameAndType #14:#15 // &quot;&lt;init&gt;&quot;:()V #25 = NameAndType #12:#13 // address:Lcom/example/demo/Address; #26 = Class #30 // com/example/demo/Address #27 = NameAndType #31:#15 // printAddress:()V #28 = Utf8 com/example/demo/User #29 = Utf8 java/lang/Object #30 = Utf8 com/example/demo/Address #31 = Utf8 printAddress{ public com.example.demo.User(); descriptor: ()V flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 6: 0 LocalVariableTable: Start Length Slot Name Signature 0 5 0 this Lcom/example/demo/User; public void printName(); descriptor: ()V flags: (0x0001) ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: getfield #2 // Field address:Lcom/example/demo/Address; 4: invokevirtual #3 // Method com/example/demo/Address.printAddress:()V 7: return LineNumberTable: line 19: 0 line 20: 7 LocalVariableTable: Start Length Slot Name Signature 0 8 0 this Lcom/example/demo/User;} 在 class 文件中有一段 Constant pool 信息，里面存储的该Class文件里的大部分常量的内容，如：类和接口的全限定名、字段的名称和描述符以及方法的名称和描述符 看到 printName() 方法有一条字节码指令： 11: getfield #2 // Field address:Lcom/example/demo/Address; 其中 #2 代表在常量池中的下标，在常量池中找到下标为2的信息： 1#2 = Fieldref #4.#25 // com/example/demo/User.address:Lcom/example/demo/Address; Fieldref 是字段的结构表示，具体查看 虚拟机规范4.4.2 ，后面的 #4.#25 分别表示 class_index 和 name_and_type_index ，这两个都是常量池下标（即所在类结构在常量池的下标和字段名与字段类型在常量池的下标），引用着另外两个常量池项。顺着这个方法把所有引用的常量都找出来，可以得到： 123456 #2 = Fieldref #4.#25 // com/example/demo/User.address:Lcom/example/demo/Address; #4 = Class #28 // com/example/demo/User#28 = Utf8 com/example/demo/User#25 = NameAndType #12:#13 // address:Lcom/example/demo/Address;#12 = Utf8 address#13 = Utf8 Lcom/example/demo/Address; 上面的常量池项对应文件结构表示为： Class：CONSTANT_Class_info Utf8：CONSTANT_Utf8_info NameAndType：CONSTANT_NameAndType_info 标记为 Utf8 的常量池项在 class 文件中实际为 CONSTANT_Utf8_info，是以 UTF-8 编码的字符串文本 ，比如 #12 = Utf8 实际值为 address 这就是class 文件里的 符号引用 实际存储的值：带有类型（tag） / 结构（符号间引用层次）的字符串。将上面的引用关系画成树： 12345 #2：Fieldref #4.#25 / \\ #4：Class #28 #25：NameAndType #12:#13 / / \\#28：Utf8 com/example/demo/User #12：Utf8 address #13：Utf8 Lcom/example/demo/Address; 上面是字段的符号引用表示，我们再来看一个方法的符号引用表示，还是在 printName() 方法： 14: invokevirtual #3 // Method com/example/demo/Address.printAddress:()V 和上面相同，这个虚拟机指令是调用 Address 类的 printAddress 方法。找到常量池中下标为 #3 的项： 1#3 = Methodref #26.#27 // com/example/demo/Address.printAddress:()V Methodref 和 Fieldref 的结构相同，由 1 个字节的 tag / 2 个字节 class_index / 2 个字节的 name_and_type_index 组成： u1 为1个字节，u2为两个字节 The class File Format 1234567891011CONSTANT_Fieldref_info { u1 tag; u2 class_index; u2 name_and_type_index;}CONSTANT_Methodref_info { u1 tag; u2 class_index; u2 name_and_type_index;} 所以可以知道 #26 为类符号结构的下标，**#27** 则是方法名和返回值的结构下标。根据上面的方法得到引用关系： 123456 #3 = Methodref #26.#27 // com/example/demo/Address.printAddress:()V#26 = Class #30 // com/example/demo/Address#30 = Utf8 com/example/demo/Address#27 = NameAndType #31:#15 // printAddress:()V#31 = Utf8 printAddress#15 = Utf8 ()V // 这是方法返回值的符号引用 由此可以看出，Class文件中的invokevirtual指令的操作数经过几层引用之后，最后都是由字符串来表示的 直接引用上面都是说的 “符号引用”，下面在看看直接引用： 大致是在类加载的时候会把 Class 文件的各个部分分别解析（parse）为 JVM 的内部数据结构。例如说类的元数据记录在 Class 结构体里，每个方法的元数据记录在各自的 methodblock 结构体里等等 在刚加载好一个类的时候，Class 文件里的常量池和每个方法的字节码（Code属性）会被基本原样的拷贝到内存里先放着，也就是说仍然处于使用 “符号引用” 的状态，直到真的要被使用到的时候才会被解析（resolve）为直接引用 假定我们要第一次执行到 printName() 方法里调用 printAddress() 方法的那条invokevirtual指令了，此时 JVM 会发现该指令尚未被解析（resolve），所以会先去解析一下。通过其操作数所记录的常量池下标找到常量池项 #3，发现该常量池项也尚未被解析（resolve），于是进一步去解析一下。通过 Methodref 所记录的 class_index 找到类名，进一步找到被调用方法的类的 Class 结构体，然后通过name_and_type_index 找到方法名和方法返回类型，到找到的 Class 结构体上记录的方法列表里找到匹配的那个 methodblock，最终把找到的 methodblock 的指针写回到常量池项 #3 里 也就是说，原本常量池项 #3 在类加载后的运行时常量池里的内容跟Class文件里的一致，只是解析后它的内容变了，由原来的字符串表示的 “符号引用” 变为一个能直接找到 Java 方法元数据的 methodblock 了。这里的 methodblock 就是一个 “直接引用”，**#3** 常量项保存着 methodblock 直接指针 解析好常量池项 #3 之后回到 invokevirtual 指令的解析，在解析后虚拟机指令从 invokevirtual 改写为 invokevirtual_quick表示该指令已经解析完毕。原本存储操作数的 2 字节空间现在分别存了 2个1 字节信息，第一个是 虚方法表的下标（vtable index），第二个是 方法的参数个数（args_size）。这两项信息都由前面解析常量池项 #3 得到的 methodblock 读取而来 1invokevirtual_quick vtable_index=5, args_size=1 在 Address 类的虚方法表就会有： 123456[0]: java.lang.Object.hashCode:()I[1]: java.lang.Object.equals:(Ljava/lang/Object;)Z[2]: java.lang.Object.clone:()Ljava/lang/Object;[3]: java.lang.Object.toString:()Ljava/lang/String;[4]: java.lang.Object.finalize:()V[5]: com.example.demo.Address.printAddress:()V User 类的虚方法表则： 123456[0]: java.lang.Object.hashCode:()I[1]: java.lang.Object.equals:(Ljava/lang/Object;)Z[2]: java.lang.Object.clone:()Ljava/lang/Object;[3]: java.lang.Object.toString:()Ljava/lang/String;[4]: java.lang.Object.finalize:()V[5]: com.example.demo.User.printName:()V 在执行 invokevirtual_quick 调用 printAddress() 时，通过对象引用查找到虚方法表后，从中取出第 #5 项的methodblock，就可以找到实际应该调用的目标然后调用过去了 直接引用是运行时所能直接使用的形式，即可以表现为直接指针（如上面#3常量项解析为 methodblock），也可能是其它形式（如invokevirtual_quick 指令里的vtable的下标），关键点不在于是否为直接指针或其它偏移量，在于jvm能不能直接使用 jvm中多态的实现假如有一个类 Staff 继承了类 User ，并重写了 printName()方法 ，那么类 Staff 的虚方法表就会有： 12345678[0]: java.lang.Object.hashCode:()I[1]: java.lang.Object.equals:(Ljava/lang/Object;)Z[2]: java.lang.Object.clone:()Ljava/lang/Object;[3]: java.lang.Object.toString:()Ljava/lang/String;[4]: java.lang.Object.finalize:()V[5]: com.example.demo.User.otherMethod:()V // 继承而来的方法[5]: com.example.demo.Staff.printName:()V // 重写了父类的方法[6]: com.example.demo.Staff.printStaff:()V // 子类自己的方法 虚方法表中方法存放顺序是先父类-再子类的顺序，所有的类都继承自 Object 类，所以表中 最先存放的是Object类的方法，接下来是该类的父类 User 的方法，最后是该类本身的方法。当调用的时候通过传入的实际指向this来确定方法的接收者（receiver），动态绑定（分派）具体对象的类型（因为是多态，所以指向的是子类对象的类型），继而找到方法区里子类的方法表，根据偏移量找到子类方法表对应的方法 参考JVM里的符号引用如何存储？","link":"/2021/06/10/2021-06-10-about-symbolic-reference-to-address-reference/"},{"title":"InnoDB对MVCC的实现","text":"这篇主要是对 锁与事务隔离级别 中MVCC内容的一个总结 一致性非锁定读和锁定读一致性非锁定读对于 一致性非锁定读（Consistent Nonlocking Reads） 的实现，通常做法是加一个版本号或者时间戳字段，在更新数据的同时版本号 + 1或者更新时间戳。查询时，将当前可见的版本号与对应记录的版本号进行比对，如果记录的版本小于可见版本，则表示该记录可见 在 InnoDB 存储引擎中，多版本控制 (multi versioning) 就是对非锁定读的实现。如果读取的行正在执行 DELETE 或 UPDATE 操作，这时读取操作不会去等待行上锁的释放。相反地，InnoDB 存储引擎会去读取行的一个快照数据，对于这种读取历史数据的方式，我们叫它快照读 (snapshot read) 在 Repeatable Read 和 Read Committed 两个隔离级别下，如果是执行普通的 select 语句（不包括 select ... lock in share mode , select ... for update）则会使用 一致性非锁定读（MVCC）。并且在 Repeatable Read 下 MVCC 实现了可重复读和防止部分幻读 锁定读如果执行的是下列语句，就是 锁定读（Locking Reads） select … lock in share mode select … for update insert、update、delete 操作 在锁定读下，读取的是数据的最新版本，这种读也被称为 当前读（current read）。锁定读会对读取到的记录加锁： select ... lock in share mode：对记录加 S 锁，其它事务也可以加S锁，如果加 x 锁则会被阻塞 select ... for update、insert、update、delete ：对记录加 X 锁，且其它事务不能加任何锁 在一致性非锁定读下，即使读取的记录已被其它事务加上 X 锁，这时记录也是可以被读取的，即读取的快照数据。上面说了在 Repeatable Read 下 MVCC 防止了部分幻读，这边的 “部分” 是指在 一致性非锁定读 情况下，只能读取到第一次查询之前所插入的数据（根据Read View判断数据可见性，Read View在第一次查询时生成），但如果是当前读 ，每次读取的都是最新数据，这时如果两次查询中间有其它事务插入数据，就会产生幻读。所以 InnoDB 在实现Repeatable Read 时，如果执行的是当前读，则会对读取的记录使用 Next-key Lock ，来防止其它事务在间隙间插入数据 InnoDB对MVCC的实现MVCC 的实现依赖于：隐藏字段、Read View、undo log。在内部实现中，InnoDB 通过数据行的 DB_TRX_ID 和 Read View 来判断数据的可见性，如不可见，则通过数据行的 DB_ROLL_PTR 找到 undo log 中的历史版本。每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 Read View 之前已经提交的修改和该事务本身做的修改 隐藏字段在内部，InnoDB 存储引擎为每行数据添加了三个 隐藏字段： DB_TRX_ID（6字节）：表示最后一次插入或更新该行的事务id。此外，delete 操作在内部被视为更新，只不过会在记录头 Record header 中的 deleted_flag 字段将其标记为已删除 DB_ROLL_PTR（7字节） 回滚指针，指向该行的 undo log 。如果该行未被更新，则为空 DB_ROW_ID（6字节）：如果没有设置主键且该表没有唯一非空索引时，InnoDB 会使用该id来生成聚簇索引 ReadViewRead View 主要是用来做可见性判断，里面保存了 “当前对本事务不可见的其他活跃事务” 主要有以下字段： m_low_limit_id：目前出现过的最大的事务ID+1，即下一个将被分配的事务ID。大于这个ID的数据版本均不可见 m_up_limit_id：活跃事务列表 m_ids 中最小的事务ID，如果 m_ids 为空，则 m_up_limit_id 为 m_low_limit_id。小于这个ID的数据版本均可见 m_ids：Read View 创建时其他未提交的活跃事务ID列表。创建 Read View 时，将当前未提交事务ID记录下来，后续即使它们修改了记录行的值，对于当前事务也是不可见的。m_ids 不包括当前事务自己和已提交的事务（正在内存中） m_creator_trx_id：创建该 Read View 的事务ID undo-logundo log 主要有两个作用： 当事务回滚时用于将数据恢复到修改前的样子 另一个作用是 MVCC ，当读取记录时，若该记录被其他事务占用或当前版本对该事务不可见，则可以通过 undo log 读取之前的版本数据，以此实现非锁定读 在 InnoDB 存储引擎中 undo log 分为两种： insert undo log 和 update undo log： insert undo log ：指在 insert 操作中产生的 undo log。因为 insert 操作的记录只对事务本身可见，对其他事务不可见，故该 undo log 可以在事务提交后直接删除。不需要进行 purge 操作 insert 时的数据初始状态： update undo log ：update 或 delete 操作中产生的 undo log。该 undo log可能需要提供 MVCC 机制，因此不能在事务提交时就进行删除。提交时放入 undo log 链表，等待 purge线程 进行最后的删除 数据第一次被修改时： 数据第二次被修改时： 不同事务或者相同事务的对同一记录行的修改，会使该记录行的 undo log 成为一条链表，链首就是最新的记录，链尾就是最早的旧记录 数据可见性算法在 InnoDB 存储引擎中，创建一个新事务后，执行每个 select 语句前，都会创建一个快照（Read View），快照中保存了当前数据库系统中正处于活跃（没有commit）的事务的ID号。其实简单的说保存的是系统中当前不应该被本事务看到的其他事务ID列表（即m_ids）。当用户在这个事务中要读取某个记录行的时候，InnoDB 会将该记录行的 DB_TRX_ID 与 Read View 中的一些变量及当前事务ID进行比较，判断是否满足可见性条件 具体的比较算法如下：图源 如果记录 DB_TRX_ID &lt; m_up_limit_id，那么表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照之前就提交了，所以该记录行的值对当前事务是可见的 如果 DB_TRX_ID &gt;= m_low_limit_id，那么表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照之后才修改该行，所以该记录行的值对当前事务不可见。跳到步骤5 m_ids 为空，则表明在当前事务创建快照之前，修改该行的事务就已经提交了，所以该记录行的值对当前事务是可见的 如果 m_up_limit_id &lt;= DB_TRX_ID &lt; m_up_limit_id，表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照的时候可能处于“活动状态”或者“已提交状态”；所以就要对活跃事务列表 m_ids 进行查找（源码中是用的二分查找，因为是有序的） 如果在活跃事务列表 m_ids 中能找到 DB_TRX_ID，表明：①在当前事务创建快照前，该记录行的值被事务ID为 DB_TRX_ID 的事务修改了，但没有提交；或者 ②在当前事务创建快照后，该记录行的值被事务ID为 DB_TRX_ID 的事务修改了。这些情况下，这个记录行的值对当前事务都是不可见的。跳到步骤5 在活跃事务列表中找不到，则表明“id为trx_id的事务”在修改“该记录行的值”后，在“当前事务”创建快照前就已经提交了，所以记录行对当前事务可见 在该记录行的 DB_ROLL_PTR 指针所指向的 undo log 取出快照记录，用快照记录的 DB_TRX_ID 跳到步骤1重新开始判断，直到找到满足的快照版本或返回空 RC和RR隔离级别下MVCC的差异在事务隔离级别 RC 和 RR （InnoDB存储引擎的默认事务隔离级别）下， InnoDB 存储引擎使用 MVCC（非锁定一致性读），但它们生成 Read View 的时机却不同 在 RC 隔离级别下的 每次select 查询前都生成一个Read View (m_ids列表) 在 RR 隔离级别下只在事务开始后 第一次select 数据前生成一个Read View（m_ids列表） MVCC解决不可重复读问题虽然 RC 和 RR 都通过 MVCC 来读取快照数据，但由于 生成 Read View 时机不同，从而在 RR 级别下实现可重复读 举个例子： 在RC下ReadView生成情况 假设时间线来到 T4 ，那么此时数据行 id = 1 的版本链为： 由于 RC 级别下每次查询都会生成 Read View ，并且事务101、102 并未提交，此时 103 事务生成的 Read View 中活跃的事务 m_ids 为：[101,102] ，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id 为：103 此时最新记录的 DB_TRX_ID 为101，m_up_limit_id &lt;= 101 &lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见 根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 还是101，不可见 继续找上一条 DB_TRX_ID为1，满足 1 &lt; m_up_limit_id，可见，所以事务103查询到数据为 name = 菜花 时间线来到 T6 ，数据的版本链为： 因为在 RC 级别下，重新生成 Read View，这时事务101已经提交，102并未提交，所以此时 Read View 中活跃的事务 m_ids：[102] ，m_low_limit_id为：104，m_up_limit_id为：102，m_creator_trx_id为：103 此时最新记录的 DB_TRX_ID 为102，m_up_limit_id &lt;= 102 &lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见 根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 为101，满足 102 &lt; m_up_limit_id，记录可见，所以在 T6 时间点查询到数据为 name = 李四，与时间 T4 查询到的结果不一致，不可重复读！ 时间线来到 T9 ，数据的版本链为： 重新生成 Read View， 这时事务 101 和 102 都已经提交，所以 m_ids 为空，则 m_up_limit_id = m_low_limit_id = 104，最新版本事务ID为102，满足 102 &lt; m_low_limit_id，可见，查询结果为 name = 赵六 总结： 在RC隔离级别下，事务在每次查询开始时都会生成并设置新的 Read View，所以导致不可重复读 在RR下ReadView生成情况在可重复读级别下，只会在事务开始后第一次读取数据时生成一个Read View（m_ids列表） 在 T4 情况下的版本链为： 在当前执行 select 语句时生成一个 Read View，此时 m_ids：[101,102] ，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id 为：103 此时和 RC 级别下一样： 最新记录的 DB_TRX_ID 为101，m_up_limit_id &lt;= 101 &lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见 根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 还是101，不可见 继续找上一条 DB_TRX_ID为1，满足 1 &lt; m_up_limit_id，可见，所以事务103查询到数据为 name = 菜花 时间点 T6 情况下： 在 RR 级别下只会生成一次Read View，所以此时依然沿用 m_ids ：[101,102] ，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id 为：103 最新记录的 DB_TRX_ID 为102，m_up_limit_id &lt;= 102 &lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见 根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 为101，不可见 继续根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 还是101，不可见 继续找上一条 DB_TRX_ID为1，满足 1 &lt; m_up_limit_id，可见，所以事务103查询到数据为 name = 菜花 时间点 T9 情况下： 此时情况跟 T6 完全一样，由于已经生成了 Read View，此时依然沿用 m_ids ：[101,102] ，所以查询结果依然是 name = 菜花 MVCC➕Next-key-Lock防止幻读InnoDB存储引擎在 RR 级别下通过 MVCC和 Next-key Lock 来解决幻读问题： 执行普通 select，此时会以 MVCC 快照读的方式读取数据 在快照读的情况下，RR 隔离级别只会在事务开启后的第一次查询生成 Read View ，并使用至事务提交。所以在生成 Read View 之后其它事务所做的更新、插入记录版本对当前事务并不可见，实现了可重复读和防止快照读下的 “幻读” 执行select…for update/lock in share mode、insert、update、delete等当前读 在当前读下，读取的都是最新的数据，如果其它事务有插入新的记录，并且刚好在当前事务查询范围内，就会产生幻读！InnoDB 使用 Next-key Lock 来防止这种情况。当执行当前读时，会锁定读取到的记录的同时，锁定它们的间隙，防止其它事务在查询范围内插入数据。只要我不让你插入，就不会发生幻读 参考 《MySQL技术内幕InnoDB存储引擎第2版》 Innodb中的事务隔离级别和锁的关系 MySQL事务与MVCC如何实现的隔离级别","link":"/2021/07/26/2021-07-26-innodb-implementation-of-mvcc/"},{"title":"消息队列基础知识","text":"一、消息队列应用场景1. 异步处理 如：秒杀功能 通常秒杀功能包括：风险控制、库存锁定、生成订单、短信通知、数据统计等，实际上只要用户请求通过风险控制，并完成库存锁定，就可以返回秒杀结果了，对于后续的生成订单、短信通知等步骤，并不一定要在秒杀请求中完成。可以把请求数据放入请求队列，由队列异步地进行后续的操作 2. 流量控制 - 达到雪峰填谷的作用如何避免过多的请求压垮我们的系统？ 一个健壮的程序应该可以在海量的请求下，在自身能力范围内尽可能地处理请求，拒绝处理不了的请求，而正常的运行。 有两种方式： 使用消息队列隔离网关和后端服务，以达到流量控制和保护后端服务的目的 加入消息队列后，整个秒杀流程变为： 网关在收到请求后，将请求放入请求消息队列； 后端服务从请求消息队列中获取 APP 请求，完成后续秒杀处理过程，然后返回结果 当大量请求到达网关时，不会直接冲击后端服务，而先堆积在消息队列中，后端服务根据自己最大处理能力，从队列中进行消费请求。对于超时的请求可以直接丢弃，返回调用者失败即可。运维人员还可以随时增加后端服务的实例数量，进行水平扩容，而不需要对其他服务进行更改。 优点： 能根据下游的处理能力自动调节流量，达到“削峰填谷”的作用 缺点： 增加了系统调用链环节，导致总体的响应时延变长。上下游系统都要将同步调用改为异步消息，增加了系统的复杂度。 令牌桶的方式 令牌桶的原理是： 单位时间内只发放固定数量的令牌到令牌桶中，规定服务在处理请求之前必须先从令牌桶中拿出一个令牌，如果令牌桶中没有令牌，则拒绝请求。这样就保证单位时间内，能处理的请求不超过发放令牌的数量，起到了流量控制的作用。 令牌桶可以简单地用一个有固定容量的消息队列加一个“令牌发生器”来实现：令牌发生器按照预估的处理能力，匀速生产令牌并放入令牌队列（如果队列满了则丢弃令牌），网关在收到请求时去令牌队列消费一个令牌，获取到令牌则继续调用后端秒杀服务，如果获取不到令牌则直接返回秒杀失败 3. 服务解耦在没有使用消息队列时，上游系统需要应对下游系统化的变化，任何一个下游系统变更都需要尚有系统重新上线一次。所以引入消息队列来解决类似系统耦合过于紧密的问题。引入消息队列后，上游系统在变化时发送一条消息到某主题，所有下游系统都订阅该主题，这样每个下游系统都可以获取一份实时完整的数据。 无论增加、减少或是下游系统需求变化，上游服务都无需做任何更改，实现服务间的解耦 消息队列带来的问题和局限性 引入消息队列带来延迟问题 增加系统的复杂度 可能产生数据不一致问题 二、如何选择消息队列RabbitMQ 优点 轻量级的消息队列，容易部署和使用 拥有灵活的路由配置，提供 Exchange 模块，根据配置的路有规则将生产者消息分发到不同的队列中，支持自己实现路有规则，扩展容易 缺点 对消息堆积支持不好，在它设计里面，消息队列是一个管道，当大量消息积压的时候，会导致 RabbitMQ 性能急剧下降 性能相较于其它消息队列是最差的，大概每秒可以处理几万到十几万消息，如果对性能要求非常高，那就不要选择 RabbitMQ RabbitMQ 使用 Erlang 开发，如果你想基于它做一些扩展和二次开发什么的，建议慎重考虑 RocketMQ 优点 RocketMQ 中文社区活跃，大多数问题可以找到中文的答案。另外使用 Java 语言开发，贡献者大多都是中文人，源码比较容易读懂，容易对其进行扩展或二次开发 对响应做了很多优化，大多数情况下可以做到毫秒级响应 性能要比 RabbitMQ 高一个量级，每秒大概能处理几十万条消息 缺点 作为国产消息队列，在国际上还没那么流行，与周边生态系统集成和兼容程度要略逊一筹 Kafka 优点 与周边生态系统的兼容性是做好的，尤其在大数据和流计算领域，几乎所有相关的开源系统都会优先支持 Kafka 设计上大量的使用了批量和异步的思想，使之有着超高的性能。但与 RocketMQ 并没有量级上的差异，大约每秒可以处理几十万条消息 缺点 同步收发消息延迟比较高，因为收到一条消息时，Kafka 并不会立即发出去，而是要攒一会，一批一起发送。如果每秒消息数量没那么多，延迟反而会比较高 Pulsar如何选择？选择中间件的考量维度：可靠性，性能，功能，可运维性，可拓展性，是否开源 及 社区活跃度 如果说，消息队列并不是你将要构建系统的主角之一，你对消息队列功能和性能都没有很高的要求，只需要一个开箱即用易于维护的产品，建议你使用 RabbitMQ。 如果你的系统使用消息队列主要场景是处理在线业务，比如在交易系统中用消息队列传递订单，那 RocketMQ 的低延迟和金融级的稳定性是你需要的。 如果你需要处理海量的消息，像收集日志、监控信息或是前端的埋点这类数据，或是你的应用场景大量使用了大数据、流计算相关的开源产品，那 Kafka 是最适合你的消息队列 三、消息模型主题和队列有什么区别？队列就是按照“队列”的数据结构来设计的。生产者（Producer）发消息就是入队操作，消费者（Consumer）收消息就是出队也就是删除操作。这里面隐含着的一个要求是，在消息入队出队过程中，需要保证这些消息严格有序，按照什么顺序写进队列，必须按照同样的顺序从队列中读出来 如果有多个生产者往同一个队列里面发送消息，这个队列中可以消费到的消息，就是这些生产者生产的所有消息的合集。消息的顺序就是这些生产者发送消息的自然顺序。如果有多个消费者接收同一个队列的消息，这些消费者之间实际上是竞争的关系，每个消费者只能收到队列中的一部分消息，也就是说任何一条消息只能被其中的一个消费者收到 如果需要将一份消息数据分发给多个消费者，要求每个消费者都能收到全量的消息，例如，对于一份订单数据，风控系统、分析系统、支付系统等都需要接收消息。这个时候，单个队列就满足不了需求，一个可行的解决方式是，为每个消费者创建一个单独的队列，让生产者发送多份。显然这是个比较蠢的做法 同样的一份消息数据被复制到多个队列中会浪费资源 更重要的是，生产者必须知道有多少个消费者。为每个消费者单独发送一份消息，这实际上违背了消息队列“解耦”这个设计初衷 主题为了解决上述问题，演化出了另外一种消息模型：发布 - 订阅模型（Publish-Subscribe Pattern）。消息的发送方称为发布者（Publisher），消息的接收方称为订阅者（Subscriber），服务端存放消息的容器称为主题（Topic）。发布者将消息发送到主题中，订阅者在接收消息之前需要先订阅主题。“订阅”在这里既是一个动作，同时还可以认为是主题在消费时的一个逻辑副本，每份订阅中，订阅者都可以接收到主题的所有消息 两者的区别生产者就是发布者，消费者就是订阅者，队列就是主题，并没有本质的区别。它们最大的区别其实就是，一份消息数据能不能被消费多次的问题。 实际上，在这种发布 - 订阅模型中，如果只有一个订阅者，那它和队列模型就基本是一样的了。也就是说，发布 - 订阅模型在功能层面上是可以兼容队列模型的 RabbitMQ消息模型RabbitMQ 是坚持使用队列模型的产品之一。在 RabbitMQ 中，有一个 Exchange 模块，位于生产者和队列之间，生产者并不关心将消息发送给哪个队列，而是将消息发送给 Exchange，由 Exchange 上配置的策略来决定将消息投递到哪些队列中 同一份消息如果需要被多个消费者来消费，需要配置 Exchange 将消息发送到多个队列，每个队列中都存放一份完整的消息数据，变相地实现 发布 - 订阅模型 RocketMQ消息模型RocketMQ 使用的消息模型是标准的 发布 - 订阅模型 请求 - 确认机制 生产端 生产者先将消息发送给服务端，也就是 Broker，服务端在收到消息并将消息写入主题或者队列中后，会给生产者发送确认的响应。如果生产者没有收到服务端的确认或者收到失败的响应，则会重新发送消息 消费端 消费者在收到消息并完成自己的消费业务逻辑（比如，将数据保存到数据库中）后，也会给服务端发送消费成功的确认，服务端只有收到消费确认后，才认为一条消息被成功消费，否则它会给消费者重新发送这条消息，直到收到对应的消费成功确认 模型这个确认机制很好地保证了消息传递过程中的可靠性，但是，为了确保消息的有序性，在某一条消息被成功消费之前，下一条消息是不能被消费的，否则就会出现消息空洞，违背了有序性这个原则 也就是说，每个主题在任意时刻，至多只能有一个消费者实例在进行消费，那就没法通过水平扩展消费者的数量来提升消费端总体的消费性能。为了解决这个问题，RocketMQ 在主题下面增加了队列的概念：每个主题包含多个队列，通过多个队列来实现多实例并行生产和消费。需要注意的是，RocketMQ 只在队列上保证消息的有序性，主题层面是无法保证消息的严格顺序的 RocketMQ 中，订阅者的概念是通过消费组（Consumer Group）来体现的。每个消费组都消费主题中一份完整的消息，不同消费组之间消费进度彼此不受影响，也就是说，一条消息被 Consumer Group1 消费过，也会再给 Consumer Group2 消费。消费组中包含多个消费者，同一个组内的消费者是竞争消费的关系，每个消费者负责消费组内的一部分消息。如果一条消息被消费者 Consumer1 消费了，那同组的其他消费者就不会再收到这条消息 在 Topic 的消费过程中，由于消息需要被不同的组进行多次消费，所以消费完的消息并不会立即被删除，这就需要 RocketMQ 为每个消费组在每个队列上维护一个消费位置（Consumer Offset），这个位置之前的消息都被消费过，之后的消息都没有被消费过，每成功消费一条消息，消费位置就加一 Kafka消息模型Kafka 的消息模型和 RocketMQ 是完全一样的，我刚刚讲的所有 RocketMQ 中对应的概念，和生产消费过程中的确认机制，都完全适用于 Kafka。唯一的区别是，在 Kafka 中，队列这个概念的名称不一样，Kafka 中对应的名称是“分区（Partition）”，含义和功能是没有任何区别的 四、如何确保消息不会丢失检测消息是否丢失 分布式链路追踪可以使用此类系统来追踪每一条消息 利用消息队列的有序性来验证在 生产者 端给每个发出的消息加上一个连续递增的序号，在 消费者 端来检查这个序号的连续性 大多数消息队列都支持 拦截器机制，可以 在生产者的拦截器中注入消息序号 ， 在消费者的拦截器中检测序号的连续性 ，这样的好处是检测代码不会侵入到业务代码中，系统稳定后也方便关闭或删除 分布式系统中需要注意的问题 像 Kafka 和 RocketMQ 这样的消息队列，它不能保证在 Topic 上的严格顺序的，只能保证分区/队列上的消息是有序的，所以我们在发消息的时候必须要指定分区/队列，并且，在每个分区/队列单独检测消息序号的连续性 如果系统中生产者是多实例，由于并不好协调多个 Producer 之间的发送顺序，所以需要每个 Producer 分别生成各自的序号，在 Consumer 端按照每个 Producer 分别来检测序号的连续性 Consumer 实例的数量最好和分区/队列数量一致，做到 Consumer 和分区/队列一一对应，这样会比较方便地在 Consumer 内检测消息序号的连续性 2.如何确保消息可靠传递消息传递主要分为三个阶段：生产阶段，存储阶段，消费阶段 生产阶段： 消息从 Producer 创建出来，经过网络传输到 Broker 端 存储阶段： 消息在 Broker 存储，如果是集群，消息会在这个阶段被复制到其他的副本上 消费阶段： 在这个阶段，**Consumer 从 Broker 上拉取消息，经过网络传输到 Consumer 上** 生产阶段在生产阶段，消息队列通过 请求确认机制 ，来保证消息的可靠传递：当调用发消息方法时，消息队列客户端会将消息发送到 Broker ，Broker 收到消息后，会给客户端返回一个确认的响应，表明消息已经收到了。客户端收到响应后，完成一次正常的消息发送 只要 Producer 收到了 Broker 的确认响应，就可以保证消息在生产阶段不会丢失。有些消息队列在长时间没收到发送确认响应后，会自动重试，如果重试再失败，就会以返回值或者异常的方式告知用户 在编写发送消息代码时，正确处理返回值或者捕获异常，就可以保证这个阶段的消息不会丢失 存储阶段在存储阶段正常情况下，只要 Broker 在正常运行，就不会出现丢失消息的问题，但是如果 Broker 出现了故障，比如进程死掉了或者服务器宕机了，还是可能会丢失消息的 如果对消息的可靠性要求非常高，一般可以通过配置 Broker 参数来避免因为宕机丢消息 单个节点Broker对于单个节点的 Broker ，需要配置刷盘策略，将消息写入磁盘后再给 Producer 返回确认响应，这样即使宕机，由于消息已经写入磁盘，就不会丢失消息，恢复后还可以继续消费 在 RocketMQ 中，将刷盘方式 flushDiskType 配置成 SYNC_FLUSH 同步刷盘 多节点集群Broker如果是由多个节点组成的集群，可以将 Broker 集群配置成：至少将消息发送到2个以上的节点，再给客户端恢复确认响应。这样即使某个 Broker 宕机时，其它的 Broker 可以替代宕机的，也不会发生消息丢失 消费阶段消费阶段采用和生产阶段类似的确认机制来保证消息的可靠传递，客户端从 Broker 拉取消息后，执行业务逻辑，成功后才给 Broker 发送消费确认响应 ，如果没有收到响应下次拉消息的时候还会返回同一条消息，确保消息不会在网络传输过程中丢失，也不会因为客户端在执行消费逻辑中出错导致丢失 在编写消费代码时，不要在收到消息后就立即发送消费确认，而是应该在执行完所有消费业务逻辑之后，再发送消费确认 3.总结 在生产阶段，你需要捕获消息发送的错误，并重发消息 在存储阶段，你可以通过配置刷盘和复制相关的参数，让消息写入到多个副本的磁盘上，来确保消息不会因为某个 Broker 宕机或者磁盘损坏而丢失 在消费阶段，你需要在处理完全部消费业务逻辑之后，再发送消费确认 五、如何处理重复消息消息重复的情况必然存在 在 MQTT 协议中，给出了三种消息传递的标准： At most once：至多一次。 消息至多会被送达一次，换个说法也就是允许消息丢失，但不允许消息重复 At lease once：至少一次。 消息至少被送达一次，也就是不允许丢消息，但有可能重复","link":"/2022/09/13/2022-09-13-message-queue/"},{"title":"Spring Cloud Gateway工作原理","text":"工作流程示意图下图概括介绍了 Spring Cloud Gateway 的工作流程： 客户端向 Spring Cloud Gateway 发出请求。如果网关 HandlerMapping 确定请求与路由匹配，则将其发送到网关 WebHandler 。Handler 通过特定的过滤器链来将请求发送到下游服务, 过滤器中间用虚线的原因是过滤器可以在发送代理请求之前和之后运行特定逻辑。顺序是所有“前置”过滤器逻辑都已执行，然后发出代理请求，后将运行“后置”过滤器逻辑 处理请求流程要摸清请求流程，首先要找到请求入口，网关请求入口在 DispatcherHandler ，当收到请求后，会执行 handle 方法 根据请求获取handle当请求到达 DispatcherHandler 时，向 handlerMappings 获取处理请求的 handler ，handlerMappings 就是所有路由的映射，包括 Controller 接口、一些系统自带的接口及下游服务，如果没找到则直接返回 404 遍历handlerMappings获取handle源码： 1234567891011121314151617public Mono&lt;Void&gt; handle(ServerWebExchange exchange) { // 没有 handlerMappings ，直接返回 404 if (this.handlerMappings == null) { return createNotFoundError(); } // OPTIONS 请求 if (CorsUtils.isPreFlightRequest(exchange.getRequest())) { return handlePreFlight(exchange); } // handlerMappings 中获取匹配的handler，没有找到则返回404 return Flux.fromIterable(this.handlerMappings) .concatMap(mapping -&gt; mapping.getHandler(exchange)) .next() .switchIfEmpty(createNotFoundError()) .flatMap(handler -&gt; invokeHandler(exchange, handler)) .flatMap(result -&gt; handleResult(exchange, result));} 可以看到 handlerMappings 其实是一个列表，其中有保存 Controller 接口的 RequestMappingHandlerMapping ，还有保存服务路由的 RoutePredicateHandlerMapping 我们也可以定义自己 handlerMapping，通过继承 AbstractHandlerMapping ，重写获取handle的方法 getHandlerInternal，这里可以根据请求路径等匹配，然后返回handle处理器就是用于处理请求的类 根据请求预设规则匹配路由RoutePredicateHandlerMapping#getHandlerInternal 方法返回的是具体的处理器 handle，这里handle其实是同一个 webHandler，并不会根据路径接口或者路由的服务变化， 方法里的逻辑是匹配路由，然后把路由放到请求上下文中 首先判断是不是访问管理端口的请求，是的话则不处理。接着根据请求查找路由，查找逻辑是判断当前请求是否符合预设的匹配规则。 默认情况下，通过 DiscoveryClient 创建的每个网关路由都会有一个默认的匹配规则：/serviceId/** ，这个 serviceId 是 DiscoveryClient 中服务的ID 也就是根据访问服务名来匹配对应的路由。找到路由后返回网关路由特定处理器 webHandler ，对应上面图中的 Gateway Web Handler DiscoveryClient：是用于从注册中心获取服务路由 webHandler： 即 org.springframework.cloud.gateway.handler.FilteringWebHandler 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051protected Mono&lt;?&gt; getHandlerInternal(ServerWebExchange exchange) { // don't handle requests on management port if set and different than server port // 不处理管理端口请求 if (this.managementPortType == DIFFERENT &amp;&amp; this.managementPort != null &amp;&amp; exchange.getRequest().getURI().getPort() == this.managementPort) { return Mono.empty(); } exchange.getAttributes().put(GATEWAY_HANDLER_MAPPER_ATTR, getSimpleName()); // 根据访问路径查找路由 return lookupRoute(exchange) // .log(&quot;route-predicate-handler-mapping&quot;, Level.FINER) //name this .flatMap((Function&lt;Route, Mono&lt;?&gt;&gt;) r -&gt; { exchange.getAttributes().remove(GATEWAY_PREDICATE_ROUTE_ATTR); if (logger.isDebugEnabled()) { logger.debug(&quot;Mapping [&quot; + getExchangeDesc(exchange) + &quot;] to &quot; + r); } // 路由存入请求上下文，在具体访问时取出 exchange.getAttributes().put(GATEWAY_ROUTE_ATTR, r); // 返回网关路由特定处理器 org.springframework.cloud.gateway.handler.FilteringWebHandler return Mono.just(webHandler); }).switchIfEmpty(Mono.empty().then(Mono.fromRunnable(() -&gt; { exchange.getAttributes().remove(GATEWAY_PREDICATE_ROUTE_ATTR); if (logger.isTraceEnabled()) { logger.trace(&quot;No RouteDefinition found for [&quot; + getExchangeDesc(exchange) + &quot;]&quot;); } })));}// lookupRouteprotected Mono&lt;Route&gt; lookupRoute(ServerWebExchange exchange) { return this.routeLocator.getRoutes() .concatMap(route -&gt; Mono.just(route).filterWhen(r -&gt; { exchange.getAttributes().put(GATEWAY_PREDICATE_ROUTE_ATTR, r.getId()); // 根据请求判断所有断言 return r.getPredicate().apply(exchange); }) .doOnError(e -&gt; logger.error( &quot;Error applying predicate for route: &quot; + route.getId(), e)) .onErrorResume(e -&gt; Mono.empty())) .next() .map(route -&gt; { if (logger.isDebugEnabled()) { logger.debug(&quot;Route matched: &quot; + route.getId()); } validateRoute(route, exchange); return route; });} webHandler处理路由网关的处理步骤和springMVC一样，由 HandlerAdapter 适配器处理，适配器也是一个列表，通过重写 supports 方法来判断是否支持该处理器。对于路由请求处理器webHandler来说，对应的适配器为 org.springframework.web.reactive.result.SimpleHandlerAdapter。 1234567891011private Mono&lt;HandlerResult&gt; invokeHandler(ServerWebExchange exchange, Object handler) { if (this.handlerAdapters != null) { for (HandlerAdapter handlerAdapter : this.handlerAdapters) { // 判断能否处理该处理器，只有SimpleHandlerAdapter返回true if (handlerAdapter.supports(handler)) { return handlerAdapter.handle(exchange, handler); } } } return Mono.error(new IllegalStateException(&quot;No HandlerAdapter: &quot; + handler)); 经过适配器最终会调用 FilteringWebHandler#handle 方法 12345678910111213141516public Mono&lt;Void&gt; handle(ServerWebExchange exchange) { // 从上下文中取路由，在getHandlerInternal中放入的 Route route = exchange.getRequiredAttribute(GATEWAY_ROUTE_ATTR); // 获取网关过滤器， List&lt;GatewayFilter&gt; gatewayFilters = route.getFilters(); // 全局 + 网关过滤器，并按照 @Ordered注解值排序 List&lt;GatewayFilter&gt; combined = new ArrayList&lt;&gt;(this.globalFilters); combined.addAll(gatewayFilters); AnnotationAwareOrderComparator.sort(combined); if (logger.isDebugEnabled()) { logger.debug(&quot;Sorted gatewayFilterFactories: &quot; + combined); } return new DefaultGatewayFilterChain(combined).filter(exchange);} Handler 会使用一系列过滤器来处理，最终将请求发送到下游服务，其中过滤器又分全局过滤器和网关过滤器，全局过滤器所有路由都会执行，一般我们的鉴权、限流逻辑就是通过自定义全局过滤器来实现的，通过实现 GlobalFilter 接口来定义，也可以通过配置项 spring.cloud.gateway.default-filters 来配置，只针对注册中心来的路由还可以通过配置 spring.cloud.gateway.discovery.locator.filters。网关处理器只对这个网关有效，也可以针对每个网关单独配置 spring.cloud.gateway.routes[0].filters 在默认情况下，网关会为通过 DiscoveryClient 创建的路由定义一个断言和过滤器： 默认断言是使用 /serviceId/** 定义的path断言，其中serviceId是 DiscoveryClient 中服务的ID 默认的过滤器使用正则表达式 /serviceId/(?&lt;remaining&gt;.*) 和替换的 /${remaining} 进行重写，用于在请求之前从路径中截取掉serviceId 最终请求会在 NettyRoutingFilter 中发出","link":"/2023/03/11/2023-03-12-how-the-gateway-works/"},{"title":"设计模式在工作中的应用","text":"策略模式在公司内部的供应链物流系统中，会与第三方物流承运商的系统进行对接。为了减少对对方系统的依赖，交互设计成“通知-反查”的形式，即当物流订单 “状态” 发生变更时，通过统一接口通知到我们系统，这边再根据不同的通知去做相应的处理。比如：订单确认、拒绝、完成，车辆启运，位置更新等等的通知 在没有使用策略模式下，我们可能会写成这样： 123456if (noticeType == 订单确认) { // 处理订单确认逻辑} else if (noticeType == 位置更新) { // 更新物流位置逻辑} ....等等其它逻辑分支 每次新增通知都要这边新增一个分支，到最后可能会非常长，后续的扩展和维护也会变得复杂且容易出错。策略模式是解决 if-else 代码块过长的方法之一，且后续维护与变更会变得很容易 策略定义 通知接口包含获取通知类型和处理通知两个方法： 12345678910111213141516171819202122232425262728/** * 物流通知 * * @author jiangrz * @date 2021-12-29 11:11 */public interface INotice&lt;V&gt; { /** * 通知回调 * * @param param param * @param merchantNo 商户编号 * @return V * @author jiangrz * @date 2021/12/29 11:12 上午 */ V notice(CallbackParam param, String merchantNo); /** * 获取通知类型 * * @return List&lt;NoticeTypeEnum&gt; * @author jiangrz * @date 2021/12/29 11:16 上午 */ List&lt;NoticeTypeEnum&gt; getNoticeType(); 各个具体的处理类都需要实现上面两个方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 在途更新、车辆到达通知 * * @author jiangrz * @date 2022-01-07 11:39 */@Slf4j@Componet@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class LocationNotice implements INotice&lt;Boolean&gt; { @Override public Boolean notice(CallbackParam param, String merchantNo) { // 更新车辆位置 } @Override public List&lt;NoticeTypeEnum&gt; getNoticeType() { return Lists.newArrayList(NoticeTypeEnum.LOCATION_UPDATE); }}/** * 车辆交付 * * @author jiangrz * @date 2022-01-13 16:08 */@Slf4j@Componet@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class OrderCompletedNotice extends INotice&lt;Boolean&gt; { @Override public Boolean notice(CallbackParam param, String merchantNo) { // 处理车辆交付 return Boolean.TRUE; } @Override public List&lt;NoticeTypeEnum&gt; getNoticeType() { return Lists.newArrayList(NoticeTypeEnum.ORDER_CAR_COMPLETE); }} 策略的管理与获取在接收到外部系统请求后，会根据不同的通知类型找到对应的处理类，然后由处理类来执行。这边结合 Spring 容器来管理，在构造 NoticeFactory 时让容器把所有通知处理类注入进来，然后循环转为map方便获取 1234567891011121314151617181920212223@Compoentpublic class NoticeFactory { /** * key：通知类型 value：通知处理器 * 多个key可能对应同一个处理 如：{101: 询价处理器, 102: 询价处理器, 200: 其它...} * 具体有几个取决于 {@link INotice#getNoticeType()} 返回了什么通知类型 */ public final Map&lt;Integer, INotice&lt;?&gt;&gt; NOTICE_MAP; public NoticeFactory(List&lt;INotice&lt;?&gt;&gt; list) { // 初始化处理器map NOTICE_MAP = Maps.newHashMap(); list.forEach(handle -&gt; { List&lt;NoticeTypeEnum&gt; noticeType = handle.getNoticeType(); noticeType.forEach(e -&gt; NOTICE_MAP.put(e.getCode(), handle)); }); } public INotice getNotice(Integer noticeType) { return NOTICE_MAP.get(noticeType); }} 策略使用上面已经将所有策略类以key：通知类型，value：处理器对象放在了map中，这边就简单的根据类型来获取即可 12CallbackParam param = &quot;外部请求信息&quot;;Boolean result = NoticeFactory.getNotice(param.getNoticeType()).notice(param, merchantNo); 责任链模式这个功能是从旧的用户系统同步用户到新的SSO系统中，方便已有的系统切换到SSO，以及后续用户信息有变更时能自动同步。同步通过mq的方式，当人员信息有变更时会从旧的用户系统发一条消息出来，消息包含基本信息、公司组织、岗位部门等 面对这样的业务，我们通常会采用面向过程的设计方法将流程拆分成N个步骤，每个步骤执行独立的逻辑： 1234567public void doSync() { // 处理基本信息 // 公司组织 // 岗位部门 } 如上代码是不符合开闭原则的，修改其中一个步骤仍然可能影响其他步骤(同一个类修改，不符合开闭原则)。当逻辑复杂后，后续的扩展和维护会容易出错，其他人接手也需要更多的时间。在这种场景下，有一种通过责任链模式，可以将这些子步骤封装成独立的handler，然后通过pipeline将其串联起来 责任链的定义常见的责任链模式会设计如下： 这边使用了自动生成责任链模式代码的工具 foldright/auto-pipeline ，在需要生成pipeline的接口上加上 @AutoPipeline 12345678910111213141516171819/** * 同步人员处理 * * @author jiangrz * @date 2022-09-28 14:26 */@AutoPipelinepublic interface Sync { /** * 同步用户 * * @param param param * @return boolean * @author jiangrz * @date 2022/9/28 14:29 */ boolean sync(SyncUserDTO param);} 会自己生成handle接口，名称为pipeline接口名+Handler。即：SyncHandler 123public interface SyncHandler { boolean sync(SyncUserDTO param, SyncHandlerContext syncHandlerContext);} 然后自定义handler去实现SyncHandler即可，这里分成了处理基本信息、用户组织、岗位部门、用户默认角色等几个处理类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/** * 用户信息处理逻辑 * * @author jiangrz * @date 2022-09-28 14:54 */@Slf4jpublic class UserInfoHandler implements SyncHandler { @Override public boolean sync(SyncUserDTO param, SyncHandlerContext syncHandlerContext) { // 同步用户信息 // 下一步 return syncHandlerContext.sync(param); }}/** * 用户组织同步 * * @author jiangrz * @date 2022-09-28 14:54 */@Slf4jpublic class UserOrgHandler implements SyncHandler { @Override public boolean sync(SyncUserDTO param, SyncHandlerContext syncHandlerContext) { // 同步用户组织信息 // 下一步 return syncHandlerContext.sync(param); }}/** * 用户部门/岗位同步 * * @author jiangrz * @date 2022-09-28 14:54 */@Slf4jpublic class UserDeptPostHandler implements SyncHandler { @Override public boolean sync(SyncUserDTO param, SyncHandlerContext syncHandlerContext) { // 下一步 return syncHandlerContext.sync(param); }}/** * 赋予用户默认角色 * * @author jiangrz * @date 2022-09-28 14:54 */@Slf4jpublic class DefaultRoleHandler implements SyncHandler { @Override public boolean sync(SyncUserDTO param, SyncHandlerContext syncHandlerContext) { // 用户默认角色逻辑 // 下一步 return syncHandlerContext.sync(param); }} 责任链使用1234567891011121314151617 // 用户信息UserInfoHandler userInfoHandler = new UserInfoHandler(params);// 用户组织UserOrgHandler userOrgHandler = new UserOrgHandler(params);// 用户默认角色DefaultRoleHandler defaultRoleHandler = new DefaultRoleHandler(params);// 用户部门岗位UserDeptPostHandler userDeptPostHandler = new UserDeptPostHandler(params);SyncPipeline syncPipeline = new SyncPipeline() .addLast(userInfoHandler) .addLast(userOrgHandler) .addLast(defaultRoleHandler) .addLast(userDeptPostHandler);// 开始处理SyncUserDTO syncUser = &quot;&quot;;syncPipeline.sync(syncUser);","link":"/2022/10/12/2022-10-12-use-pattern/"},{"title":"Spring Cloud Gateway路由的管理","text":"路由的构建步骤gateway支持多种配置路由的方式，如通过 properties 文件配置、通过接口添加、从注册中心获取等等。不管是在yml文件配置的 routes 节点，还是通过 DiscoveryClient 加载的注册中心的服务，亦或是通过 /gateway/routes/{id} 接口增加的，在网关中都会生成对应一个个 RouteDefinition ，再通过RouteDefinition 生成对应的 route 。 RouteDefinition 加载生成RouteDefinition 通过 RouteDefinitionLocator 接口来定位获取，这里面使用了组合的设计模式（Composite Pattern） 如上图，就是它们的UML类图，RouteDefinitionLocator 是抽象接口，提供了 getRouteDefinitions() 方法用于外部获取 RouteDefinition 列表， CompositeRouteDefinitionLocator 作为组合对象里面持有一个 List&lt;RouteDefinitionLocator&gt; ，当调用getRouteDefinitions() 方法时，会循环调用列表中具体的定位器的 getRouteDefinitions() 方法获取，然后合并成一个列表返回 默认有上图几个定位器，他们的作用分别是： PropertiesRouteDefinitionLocator： 加载配置文件中 spring.cloud.gateway.routes 配置项的路由 DiscoveryClientRouteDefinitionLocator： 加载注册中心的服务作为路由 InMemoryRouteDefinitionRepository： 通过web接口提交的路由或者其它在程序内通过接口（RouteDefinitionWriter#save）增加的 上面这几个 RouteDefinitionLocator 由框架创建并注册到 Spring 容器中，可以看到 CompositeRouteDefinitionLocator 被注解 @Primary 标记，代表后续要自动注入 RouteDefinitionLocator 时会被优先注入 当然我们也可以自定义 RouteDefinitionLocator，只需要实现 RouteDefinitionLocator，然后将它注册到 Spring 容器即可，在创建 CompositeRouteDefinitionLocator 时会被自动注入到参数列表中。如下例子：定义一个名为 MyRouteDefinitionLocator的定位器，并用 @Component 注解声明 实现自定义 RouteDefinitionLocator： 12345678910111213/** * @author ddmcc */@Componentpublic class MyRouteDefinitionLocator implements RouteDefinitionLocator { @Override public Flux&lt;RouteDefinition&gt; getRouteDefinitions() { // 自定义新增RouteDefinition return null; }} 加载注册中心中的服务DiscoveryClientRouteDefinitionLocator 作为加载注册中心服务的路由定义定位器，其通过访问 DiscoveryClient 接口来获取注册中心的服务信息，再将其生成 RouteDefinition。所以第三方注册中心要想接入gateway，则创建一个bean来实现 DiscoveryClient 接口即可 在这里 DiscoveryClient 也是采用了组合模式来实现，其结构如下图： gateway在配置类 CompositeDiscoveryClientAutoConfiguration 中，手动创建了组合器 CompositeDiscoveryClient ，将所有 DiscoveryClient 的bean注入并赋值到 discoveryClients，在调用获取服务ID或者根据服务ID获取服务列表时会循环请求 discoveryClients 。 这边要注意的是，getServices方法返回的是所有client的服务ID，getInstances返回的是第一个client的结果（根据这个id获取），至于访问client的顺序可以用注解@Ordered，所以serviceId最好不要与其他的重复 123456789101112@Configuration(proxyBeanMethods = false)@AutoConfigureBefore(SimpleDiscoveryClientAutoConfiguration.class)public class CompositeDiscoveryClientAutoConfiguration { @Bean @Primary public CompositeDiscoveryClient compositeDiscoveryClient( List&lt;DiscoveryClient&gt; discoveryClients) { return new CompositeDiscoveryClient(discoveryClients); }} 下面以nacos为例，是如何接入gateway并将服务作为路由的： NacosDiscoveryClient 实现了 DiscoveryClient 接口，并实现了 getServices 方法获取所有服务ID、getInstances(String serviceId) 方法获取服务对象信息，ServiceInstance 对象主要有服务id、IP、端口、是否https、额外信息等字段，利用这些就可以生成相应的路由了 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class NacosDiscoveryClient implements DiscoveryClient { private static final Logger log = LoggerFactory.getLogger(NacosDiscoveryClient.class); /** * Nacos Discovery Client Description. */ public static final String DESCRIPTION = &quot;Spring Cloud Nacos Discovery Client&quot;; private NacosServiceDiscovery serviceDiscovery; public NacosDiscoveryClient(NacosServiceDiscovery nacosServiceDiscovery) { this.serviceDiscovery = nacosServiceDiscovery; } @Override public String description() { return DESCRIPTION; } // 根据服务ID获取服务对象列表 @Override public List&lt;ServiceInstance&gt; getInstances(String serviceId) { try { return serviceDiscovery.getInstances(serviceId); } catch (Exception e) { throw new RuntimeException( &quot;Can not get hosts from nacos server. serviceId: &quot; + serviceId, e); } } // 获取所有服务ID @Override public List&lt;String&gt; getServices() { try { return serviceDiscovery.getServices(); } catch (Exception e) { log.error(&quot;get service name from nacos server fail,&quot;, e); return Collections.emptyList(); } }} 可以看到 getInstances 返回的是一个列表，而不是单个对象，这是因为返回的列表是这个服务的所有节点，用于负载均衡访问 生成 Route现在路由定义信息已经有了，下面就可以通过定义信息来生成真正的 Route 。Route 通过 RouteLocator 生成，它的实现方式和前面一样，使用了组合的设计模式，RouteDefinitionRouteLocator 只是作为其获取路由的方式之一 在创建 routeDefinitionRouteLocator 时会将 RouteDefinitionLocator 注入进来，因为在创建 CompositeRouteDefinitionLocator 时被声明为@Primary，所以这里注入的对象就是组合类 生成 route 的时候通过 CompositeRouteDefinitionLocator 获取所有的 RouteDefinition ，然后转为route 123456789101112131415161718@Overridepublic Flux&lt;Route&gt; getRoutes() { Flux&lt;Route&gt; routes = this.routeDefinitionLocator.getRouteDefinitions() .map(this::convertToRoute); // 省略一部分逻辑 return routes.map(route -&gt; { if (logger.isDebugEnabled()) { logger.debug(&quot;RouteDefinition matched: &quot; + route.getId()); } return route; });}private Route convertToRoute(RouteDefinition routeDefinition) { AsyncPredicate&lt;ServerWebExchange&gt; predicate = combinePredicates(routeDefinition); List&lt;GatewayFilter&gt; gatewayFilters = getFilters(routeDefinition); return Route.async(routeDefinition).asyncPredicate(predicate).replaceFilters(gatewayFilters).build();} 得益于组合模式，我们也可以很方便的自定义 RouteLocator 去注册路由：定义MyRouteLocator，并实现RouteLocator的getRoutes方法 12345678@Componentpublic class MyRouteLocator implements RouteLocator { @Override public Flux&lt;Route&gt; getRoutes() { return null; }} 可以看到 MyRouteLocator 已被注入到 CompositeRouteLocator 路由动态管理","link":"/2023/03/14/2023-03-14-gateway-dynamic-routing/"},{"title":"设计模式之组合模式","text":"组合模式（Composite Pattern），又叫部分整体模式，是用于把一组相似的对象当作一个单一的对象。 组合模式依据树形结构来组合对象，用来表示部分以及整体层次。 这种类型的设计模式属于结构型模式，它创建了对象组的树形结构。 这种模式创建了一个包含自己对象组的类 结构 Leaf: 叶子节点Composite：组合对象 Leaf 叶子节点和 Composite 组合对象实现共同接口，不同的是子节点在方法中编写具体的处理逻辑，组合对象的作用是循环所有子节点的处理方法。如上图，Composite 维护一个子对象列表 List ，当使用者向Composite对象（类型）发送请求时，该请求被转发到所有子Component对象（Leaf和Composite） 为什么说该请求被转发到所有子Leaf和Composite？因为组合对象维护的是List，是顶层Component，所以可以是一个组合对象Composite 组合对象Composite中，通常还会有提供对子列表操作的方法，用于在运行时动态添加或者结合Spring的自动注入子组件，这样的好处就是外部的子组件也可以被添加进来并被执行 源码中的例子在 Spring Cloud Gateway 中，除了可以通过配置文件的方式还可以从外部的注册中心动态拉取服务列表，比如网关项目整合了nacos，网关就能将路由到我们注册nacos中的服务，那这是怎么做到的呢？ 首先 Spring Cloud Gateway 提供了统一获取服务列表的接口 DiscoveryClient ，各个服务注册框架都会去适配它，比如nacos中的 NacosDiscoveryClient 。那gateway是怎么知道又怎么调用到它的呢？ DiscoveryClient接口： 12345678910111213public interface DiscoveryClient extends Ordered { int DEFAULT_ORDER = 0; String description(); List&lt;ServiceInstance&gt; getInstances(String serviceId); List&lt;String&gt; getServices(); default int getOrder() { return 0; }} CompositeDiscoveryClientgateway定义了一个组合对象 CompositeDiscoveryClient 创建时注入了所有的 DiscoveryClient ，并且 CompositeDiscoveryClient 声明为 @Primary，这意味着后面获取 DiscoveryClient 时，如果有多个会被优先获取 123456789101112@Configuration(proxyBeanMethods = false)@AutoConfigureBefore(SimpleDiscoveryClientAutoConfiguration.class)public class CompositeDiscoveryClientAutoConfiguration { @Bean @Primary public CompositeDiscoveryClient compositeDiscoveryClient( List&lt;DiscoveryClient&gt; discoveryClients) { return new CompositeDiscoveryClient(discoveryClients); }} 在组合对象的实现逻辑中，实际是去循环调用 discoveryClients 列表，discoveryClients 也就是创建 CompositeDiscoveryClient 时传入的 1234567891011121314151617181920212223242526272829303132public List&lt;ServiceInstance&gt; getInstances(String serviceId) { if (this.discoveryClients != null) { Iterator var2 = this.discoveryClients.iterator(); while(var2.hasNext()) { DiscoveryClient discoveryClient = (DiscoveryClient)var2.next(); List&lt;ServiceInstance&gt; instances = discoveryClient.getInstances(serviceId); if (instances != null &amp;&amp; !instances.isEmpty()) { return instances; } } } return Collections.emptyList();}public List&lt;String&gt; getServices() { LinkedHashSet&lt;String&gt; services = new LinkedHashSet(); if (this.discoveryClients != null) { Iterator var2 = this.discoveryClients.iterator(); while(var2.hasNext()) { DiscoveryClient discoveryClient = (DiscoveryClient)var2.next(); List&lt;String&gt; serviceForClient = discoveryClient.getServices(); if (serviceForClient != null) { services.addAll(serviceForClient); } } } return new ArrayList(services);} 通过上面可以看出，gateway使用组合模式来实现获取服务节点列表，并且结合了spring容器注入，所以 NacosDiscoveryClient 想要被调用到，还需要注册到spring容器中 12345678public class NacosDiscoveryClientConfiguration { @Bean public DiscoveryClient nacosDiscoveryClient( NacosServiceDiscovery nacosServiceDiscovery) { return new NacosDiscoveryClient(nacosServiceDiscovery); }}","link":"/2023/04/01/2023-04-01-composite-pattern/"},{"title":"openFeign整合ribbon实现负载均衡","text":"通过 上篇文章 了解了 OpenFeign 生成动态代理的整个过程，并且知道了它是基于 JDK 动态代理来实现的。这篇文章主要来看看 OpenFeign 是如何基于 ribbon 实现负载均衡的，还会了解到 ribbon的一些主要对象及运行原理，最后还有一个自定义 ribbon 负载均衡规则的代码案例 回顾feign 客户端接口的调用是基于 JDK 动态代理来实现的，在所有的方法调用的时候最终都会走 InvocationHandler 接口，默认实现类是 ReflectiveFeign.FeignInvocationHandler，下面就跟着代码执行流程，看看如何实现rpc调用及整合ribbon负载均衡","link":"/2023/05/29/2023-05-29-ribbon-components-and-integration-of-gateway/"},{"title":"websocket服务集群搭建与消息收发实现","text":"本文介绍如何将websocket服务注册到nacos中，并实现负载均衡调用，以及集群模式下消息（单聊，群发）的收发处理 注册websocket服务到nacos定义websocket服务这边使用了 netty-websocket 来开发搭建websocket服务，其中 ${ws.path} 和 ${ws.port} 是配置项，{userId} 是前端传入的路径参数，相当于 @PathVariable 123456@Slf4j@Component@ServerEndpoint(path = &quot;${ws.path}/{userId}&quot;, port = &quot;${ws.port}&quot;)public class WebsocketEndpoint {} WebsocketProperties 1234ws: name: &quot;websocket-demo&quot; path: &quot;/ws&quot; port: &quot;19998&quot; 将服务注册到nacos中注册到 nacos 主要是利用 NamingService 提供的接口 namingService.registerInstance。首先注入项目的 nacos 配置，创建 NamingService ，然后调用 registerInstance 注册，包含3个参数name,ip,port 。name 是服务名，待会我们配置负载均衡或者前端访问会用到，ip和port 是上面定义端点的时候用的 1234567891011121314151617181920@Slf4j@Component@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class WebsocketServiceRegister implements CommandLineRunner { private final WebsocketProperties websocketProperties; private final NacosDiscoveryProperties nacosDiscoveryProperties; @Override public void run(String... args) throws Exception { Properties properties = new Properties(); // 根据项目的nacos配置创建NamingService properties.setProperty(&quot;serverAddr&quot;, nacosDiscoveryProperties.getServerAddr()); properties.setProperty(&quot;namespace&quot;, nacosDiscoveryProperties.getNamespace()); NamingService namingService = NamingFactory.createNamingService(properties); // 注册websocket服务节点 namingService.registerInstance(websocketProperties.getName(), nacosDiscoveryProperties.getIp(), websocketProperties.getPort()); }} 配置网关负载均衡现在服务已经起来了，也注册到 nacos 上了，下一步就是要配置网关，让网关能够正确路由到 websocket 端点，并且在多节点下也能够实现负载均衡。直接上配置： 1234567891011spring: cloud: gateway: enabled: true routes: - id: wsRouter uri: lb:ws://websocket-demo predicates: - Path=/websocket-demo/** filters: - StripPrefix=1 上面配置的意思就是当访问路径匹配上 /websocket-demo/** 时，去访问 lb:ws://websocket-demo ，lb 代表负载均衡访问的意思，访问 lb:ws://websocket-demo 时，会拿着 websocket-demo 找服务节点，也就是上面注册nacos时的 name ，向nacos获取到节点后，访问路径会被重写为ws://ip:port/** 这时websocket服务基本就搭建完成了，前端访问 http://gatewayIp:gatewayPort/websocket-demo/ws/* 就能连接上websocket服务 集群模式下消息收发处理集群模式下最大的问题是 session 共享问题，因为 session 不能持久化，就会出现比如：用户1连接请求打在了节点A，这时对应的连接session就保存在了节点A里，然后来了一个给用户1发送消息的请求，此时并不能保证会请求到节点A，如果请求不在A也就无法找到session向A发送消息 这边使用 redis + 消息队列 来解决这个问题： 群聊模式因为 session 分散存储在各个节点，群聊相当于 全量 的发送，这时可以利用消息队列的 广播模式 ，将消息广播到每个节点，节点内在去获取 session 发送 下面以rocketMQ为例： 群聊消息发送 123private final RocketMQTemplate template;template.syncSend(&quot;GROUP:TAG&quot;, JSONObject.toJSONString(message)); 群聊消息消费 主要是 messageModel = MessageModel.BROADCASTING ，消息模型声明为广播模式 12345678910111213141516171819202122@Slf4j@Component@RocketMQMessageListener(consumerGroup = &quot;group&quot;, topic = &quot;GROUP&quot;, messageModel = MessageModel.BROADCASTING)@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class GroupChatMessageConsumer implements RocketMQListener&lt;ReceiveMessage&gt; { @Override public void onMessage(ReceiveMessage dto) { log.info(&quot;开始消费群聊消息：{}&quot;, dto); Long to = dto.getTo(); // 根据群ID获取群人员或者全部发送 SessionCache.USER_SESSION_MAP.entrySet().forEach(item -&gt; { SessionWrapper sessionWrapper = item.getValue(); if (sessionWrapper != null) { // 发送 // sessionWrapper.sendMessage(JSONObject.toJSONString(sendMessage)) } }); }} 单聊模式单聊模式在集群节点少的情况下用上面的广播消息也可以，但在节点较多的情况下会产生大量冗余消息，造成性能问题，比如说：用户A的session存储在节点1，这时广播消息，那么节点2,3,4,5…也会收到，再去处理的话就显得多余了。这边的解决方案是在 redis 中存储的用户与节点的关系，然后将消息直接发送给对应的节点 收到连接请求时 将用户与节点关系存到redis中，比如： key -&gt; userId, value -&gt; nodeName 123456789101112131415161718// 节点private final String discoveryIp;@Overridepublic void handle(SessionWrapper session) { Long userId = session.getUserId(); log.info(&quot;新用户连接 userId：{}&quot;, userId); // 用户与服务器ip关系存入redis stringRedisTemplate.opsForValue().set(super.buildUserNodeKey(userId), discoveryIp); // 用户session存入map SessionWrapper sessionWrapper = SessionCache.USER_SESSION_MAP.computeIfPresent(userId, (k, v) -&gt; { log.info(&quot;用户userId：{} 在其它地方上线了，旧sessionId关闭：{}&quot;, k, v.getId()); v.close(); return session; }); SessionCache.USER_SESSION_MAP.put(userId, sessionWrapper);} 收到websocket消息时 根据消息接收人拿到对应session所在的节点，发送消息到对应节点 12String node = stringRedisTemplate.opsForValue().get(&quot;对应接收人ID&quot;);template.syncSend(&quot;SINGLE:&quot; + node, JSONObject.toJSONString(message)); 消费消息时 demo用的是 rocketMQ ， 请注意不要将节点定义成相同的消费组，否则可能会丢消息，也就是如果利用tag来过滤消息，记得将消费组定义成不同的 消费者定义： 123456789101112131415161718192021222324@Slf4j@Component@RocketMQMessageListener(topic = &quot;SINGLE&quot;, consumerGroup = &quot;simple-${node_name}&quot;, selectorType = SelectorType.TAG, selectorExpression = &quot;${node_name}&quot;)@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class SimpleChatMessageConsumer implements RocketMQListener&lt;ReceiveMessage&gt; { private final SessionHandler closeSessionHandler; @Override public void onMessage(ReceiveMessage dto) { log.info(&quot;开始消费聊天消息：{}&quot;, dto); SessionWrapper sessionWrapper = SessionCache.USER_SESSION_MAP.get(dto.getTo()); if (sessionWrapper == null) { log.info(&quot;未找到消息接收人：{} ，可能已下线&quot;, dto.getTo()); } else if (sessionWrapper.isClose()) { log.info(&quot;消息接收人链接已关闭：{}，可能已断开链接&quot;, dto.getTo()); closeSessionHandler.handle(sessionWrapper); } else { // 发送 // sessionWrapper.sendMessage(JSONObject.toJSONString(sendMessage)); } }} 当然如果用其它消息队列可能会更直接点，比如说 rabbitMQ 可以直接用节点ip绑定队列 参考 netty-websocket demo代码","link":"/2023/05/29/2023-05-29-websocket-clusters/"},{"title":"OpenFeign的实现原理","text":"前言在过去使用feign时，会将feign接口单独放在一个包，并在接口类上声明 @FeignClient(name = 服务名)， 在接口方法上声明 @xxxMapping(接口路径) ，实现者和使用者都会去引入这个包。对于 使用者 来说只需要注入接口类，然后调用对应的方法就能调用到在另一个服务的实现者实现的逻辑 根据过去的经验我猜测，在调用侧上的原理和mybatis相似，扫描所有被 @FeignClient 注解声明的类，然后为其生成代理对象，当执行某个方法时，会被拦截转而请求方法上的链接，不同的是mybatis是找到这个方法对应的MappedStatement对象，然后执行sql。实现侧的话则和普通controller类似，将这些方法封装成一个个handle并注册到handleMapping中，等待被DispatcherServlet调用 调用侧扫描@FeignClient注解类为了验证上面的猜想，我们从 @EnableFeignClients 入手，看看启动时都做了什么 1234567@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@Documented@Import(FeignClientsRegistrar.class)public @interface EnableFeignClients { } 可以看到注解 EnableFeignClients 通过 @Import 注解导入一个配置类 FeignClientsRegistrar，它实现了 ImportBeanDefinitionRegistrar 接口，这么说在启动的时候，FeignClientsRegistrar 类中的 registerBeanDefinitions 方法会被调用，来往Spring容器中注册 BeanDefinition 1234567@Overridepublic void registerBeanDefinitions(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { // 解析 @EnableFeignClients 注解中的配置，并注册到容器 registerDefaultConfiguration(metadata, registry); // 扫描被 @FeignClient 注解声明的接口，并注册到容器 registerFeignClients(metadata, registry);} registerFeignClients 方法是重点，这里扫描到所有 @FeignClient 注解声明的接口后，并做了一系列的处理 1234567891011121314151617181920212223242526272829303132333435363738394041public void registerFeignClients(AnnotationMetadata metadata, BeanDefinitionRegistry registry) { LinkedHashSet&lt;BeanDefinition&gt; candidateComponents = new LinkedHashSet&lt;&gt;(); // 获取EnableFeignClients注解配置属性 Map&lt;String, Object&gt; attrs = metadata.getAnnotationAttributes(EnableFeignClients.class.getName()); // 获取配置的clients，如果有配置，则不再进行类路径扫描 final Class&lt;?&gt;[] clients = attrs == null ? null : (Class&lt;?&gt;[]) attrs.get(&quot;clients&quot;); if (clients == null || clients.length == 0) { ClassPathScanningCandidateComponentProvider scanner = getScanner(); scanner.setResourceLoader(this.resourceLoader); // 指定要扫描的注解 scanner.addIncludeFilter(new AnnotationTypeFilter(FeignClient.class)); // 获取EnableFeignClients中配置的包路径，如果没有就默认为注解所在类的包路径 Set&lt;String&gt; basePackages = getBasePackages(metadata); for (String basePackage : basePackages) { // 扫描并构建ScannedGenericBeanDefinition candidateComponents.addAll(scanner.findCandidateComponents(basePackage)); } } else { for (Class&lt;?&gt; clazz : clients) { candidateComponents.add(new AnnotatedGenericBeanDefinition(clazz)); } } for (BeanDefinition candidateComponent : candidateComponents) { if (candidateComponent instanceof AnnotatedBeanDefinition beanDefinition) { // verify annotated class is an interface AnnotationMetadata annotationMetadata = beanDefinition.getMetadata(); Assert.isTrue(annotationMetadata.isInterface(), &quot;@FeignClient can only be specified on an interface&quot;); Map&lt;String, Object&gt; attributes = annotationMetadata .getAnnotationAttributes(FeignClient.class.getCanonicalName()); String name = getClientName(attributes); String className = annotationMetadata.getClassName(); // 注册client的配置到容器 -&gt; FeignClientSpecification registerClientConfiguration(registry, name, className, attributes.get(&quot;configuration&quot;)); // 创建client的BeanDefinition，注册到容器 registerFeignClient(registry, annotationMetadata, attributes); } }} 先获取 @EnableFeignClients 的配置，看是否有配置指定的client，有的话就只生成配置的BeanDefinition，没有的话再用ClassPathScanningCandidateComponentProvider来扫描被 @FeignClient 注解标记的类，扫描的路径的话看有没有配置，如果没有配置则默认为 @EnableFeignClients 注解所在类的所在包开始向下扫描。扫描到的类都会生成一个BeanDefinition，可以把BeanDefinition看成对每个标有 @FeignClient 注解的类信息的封装。拿到所有BeanDefinition之后，遍历调用 registerClientConfiguration 和 registerFeignClient registerClientConfiguration 12345678910private void registerClientConfiguration(BeanDefinitionRegistry registry, Object name, Object className, Object configuration) { BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(FeignClientSpecification.class); builder.addConstructorArgValue(name); builder.addConstructorArgValue(className); builder.addConstructorArgValue(configuration); registry.registerBeanDefinition(name + &quot;.&quot; + FeignClientSpecification.class.getSimpleName(), builder.getBeanDefinition());} 这里的意思就是拿出再 @FeignClient 指定的配置类，也就是 configuration 属性，然后构建一个bean class为FeignClientSpecification。这个类的最主要作用就是将每个client的配置类（configuration 属性）封装成一个 FeignClientSpecification 的 BeanDefinition，注册到spring容器中 registerFeignClient 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556private void registerFeignClient(BeanDefinitionRegistry registry, AnnotationMetadata annotationMetadata, Map&lt;String, Object&gt; attributes) { String className = annotationMetadata.getClassName(); if (String.valueOf(false).equals( environment.getProperty(&quot;spring.cloud.openfeign.lazy-attributes-resolution&quot;, String.valueOf(false)))) { eagerlyRegisterFeignClientBeanDefinition(className, attributes, registry); } else { lazilyRegisterFeignClientBeanDefinition(className, attributes, registry); }}private void eagerlyRegisterFeignClientBeanDefinition(String className, Map&lt;String, Object&gt; attributes, BeanDefinitionRegistry registry) { validate(attributes); // 构建class为FeignClientFactoryBean的BeanDefinition BeanDefinitionBuilder definition = BeanDefinitionBuilder.genericBeanDefinition(FeignClientFactoryBean.class); definition.addPropertyValue(&quot;url&quot;, getUrl(null, attributes)); definition.addPropertyValue(&quot;path&quot;, getPath(null, attributes)); String name = getName(attributes); definition.addPropertyValue(&quot;name&quot;, name); String contextId = getContextId(null, attributes); definition.addPropertyValue(&quot;contextId&quot;, contextId); definition.addPropertyValue(&quot;type&quot;, className); definition.addPropertyValue(&quot;dismiss404&quot;, Boolean.parseBoolean(String.valueOf(attributes.get(&quot;dismiss404&quot;)))); Object fallback = attributes.get(&quot;fallback&quot;); if (fallback != null) { definition.addPropertyValue(&quot;fallback&quot;, (fallback instanceof Class ? fallback : ClassUtils.resolveClassName(fallback.toString(), null))); } Object fallbackFactory = attributes.get(&quot;fallbackFactory&quot;); if (fallbackFactory != null) { definition.addPropertyValue(&quot;fallbackFactory&quot;, fallbackFactory instanceof Class ? fallbackFactory : ClassUtils.resolveClassName(fallbackFactory.toString(), null)); } definition.addPropertyValue(&quot;fallbackFactory&quot;, attributes.get(&quot;fallbackFactory&quot;)); definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_BY_TYPE); definition.addPropertyValue(&quot;refreshableClient&quot;, isClientRefreshEnabled()); String[] qualifiers = getQualifiers(attributes); if (ObjectUtils.isEmpty(qualifiers)) { qualifiers = new String[] { contextId + &quot;FeignClient&quot; }; } // This is done so that there's a way to retrieve qualifiers while generating AOT // code definition.addPropertyValue(&quot;qualifiers&quot;, qualifiers); AbstractBeanDefinition beanDefinition = definition.getBeanDefinition(); beanDefinition.setAttribute(FactoryBean.OBJECT_TYPE_ATTRIBUTE, className); // has a default, won't be null boolean primary = (Boolean) attributes.get(&quot;primary&quot;); beanDefinition.setPrimary(primary); BeanDefinitionHolder holder = new BeanDefinitionHolder(beanDefinition, className, qualifiers); BeanDefinitionReaderUtils.registerBeanDefinition(holder, registry); registerRefreshableBeanDefinition(registry, contextId, Request.Options.class, OptionsFactoryBean.class); registerRefreshableBeanDefinition(registry, contextId, RefreshableUrl.class, RefreshableUrlFactoryBean.class);} 默认情况下会调用 eagerlyRegisterFeignClientBeanDefinition ，所以我们来看这个方法做了哪些事。先构建了一个class为 FeignClientFactoryBean 的BeanDefinition，这个class实现了FactoryBean接口，spring在生成bean的时候判断BeanDefinition中bean的class如果是FactoryBean的实现的话，会调用这个实现类的getObject来获取对象。 到这里生成动态代理对象的准备工作就基本做完了，再来总结一下前面做了哪些：根据 @EnableFeignClients 注解的配置扫描指定（不指定就默认路径下的）包下所有加了 @FeignClient 注解的类，然后每个类都会生成一个BeanDefinition，随后 遍历每个BeanDefinition ，然后取出每个 @FeignClient 注解的属性，构造class为 FeignClientFactoryBean 的新的BeanDefinition，随后注册到spring容器中，同时有配置类（注解上configuration属性）的也会将配置类构件出一个class为 FeignClientSpecification 的BeanDefinition注册到spring容器中 生成动态代理对象上面每个 @FeignClient 都生成了一个 class为 FeignClientFactoryBean 的BeanDefinition，后面就会根据这个BeanDefinition来生成动态代理对象。因为FeignClientFactoryBean实现了FactoryBean接口，所以会调用 getObject() 来获取对象： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@Overridepublic Object getObject() { return getTarget();}&lt;T&gt; T getTarget() { // FeignClientFactory 里面保存了每个client的配置，也就是FeignClientSpecification（具体可以看FeignAutoConfiguration） FeignClientFactory feignClientFactory = beanFactory != null ? beanFactory.getBean(FeignClientFactory.class) : applicationContext.getBean(FeignClientFactory.class); // 获取builder，Feign.Builder对象在FeignClientsConfiguration中配置 Feign.Builder builder = feign(feignClientFactory); // 判断是否配置了url属性，url链接直连访问 if (!StringUtils.hasText(url) &amp;&amp; !isUrlAvailableInConfig(contextId)) { if (LOG.isInfoEnabled()) { LOG.info(&quot;For '&quot; + name + &quot;' URL not provided. Will try picking an instance via load-balancing.&quot;); } if (!name.startsWith(&quot;http&quot;)) { url = &quot;http://&quot; + name; } else { url = name; } url += cleanPath(); // 负载均衡访问 return (T) loadBalance(builder, feignClientFactory, new HardCodedTarget&lt;&gt;(type, name, url)); } if (StringUtils.hasText(url) &amp;&amp; !url.startsWith(&quot;http&quot;)) { url = &quot;http://&quot; + url; } String url = this.url + cleanPath(); Client client = getOptional(feignClientFactory, Client.class); if (client != null) { if (client instanceof FeignBlockingLoadBalancerClient) { // not load balancing because we have a url, // but Spring Cloud LoadBalancer is on the classpath, so unwrap client = ((FeignBlockingLoadBalancerClient) client).getDelegate(); } if (client instanceof RetryableFeignBlockingLoadBalancerClient) { // not load balancing because we have a url, // but Spring Cloud LoadBalancer is on the classpath, so unwrap client = ((RetryableFeignBlockingLoadBalancerClient) client).getDelegate(); } builder.client(client); } applyBuildCustomizers(feignClientFactory, builder); Targeter targeter = get(feignClientFactory, Targeter.class); return targeter.target(this, builder, feignClientFactory, resolveTarget(feignClientFactory, contextId, url));} 先获取FeignClientFactory，这里保存了每个client的配置，然后获取到一个Feign.Builder，然后调用feign方法。feign方法主要事设置一些默认的组件，如果需要更改上面的这些组件，可以通过 @FeignClient 的 configuration 属性配置配置类，在配置类里面替换 12345678910111213141516protected Feign.Builder feign(FeignClientFactory context) { FeignLoggerFactory loggerFactory = get(context, FeignLoggerFactory.class); Logger logger = loggerFactory.create(type); Feign.Builder builder = get(context, Feign.Builder.class) // required values .logger(logger) .encoder(get(context, Encoder.class)) .decoder(get(context, Decoder.class)) .contract(get(context, Contract.class)); // 从配置文件中读取feign的配置 configureFeign(context, builder); return builder;} 下面就是这段逻辑： 123456789101112131415if (!StringUtils.hasText(url) &amp;&amp; !isUrlAvailableInConfig(contextId)) { if (LOG.isInfoEnabled()) { LOG.info(&quot;For '&quot; + name + &quot;' URL not provided. Will try picking an instance via load-balancing.&quot;); } if (!name.startsWith(&quot;http&quot;)) { url = &quot;http://&quot; + name; } else { url = name; } url += cleanPath(); // 负载均衡访问 return (T) loadBalance(builder, feignClientFactory, new HardCodedTarget&lt;&gt;(type, name, url));} 先判断有没有指定url，也就是在 @FeignClient 注解中指定的url属性，如果配置了这个属性，就不通过注册中心，直接访问链接，isUrlAvailableInConfig() 也是判断在配置文件中是否配置了url属性 。一般情况下这个是不配置的，因为得从注册中心获取服务的ip和端口列表，然后进行负载均衡访问。所以从这也也可以看出，没有注册中心，feign也是能够跑的，只要配置url属性就行 下面就是拼接url，name就是我们在 @FeignClient 配置的value，一般是服务名，这段代码拼出来的结果就类似 http://ServiceA，之后就会走loadBalance方法，传入一个HardCodedTarget参数，封装了feign客户端接口的类型、服务的名称、还有刚构建的url 123456789101112protected &lt;T&gt; T loadBalance(Feign.Builder builder, FeignClientFactory context, HardCodedTarget&lt;T&gt; target) { Client client = getOptional(context, Client.class); if (client != null) { builder.client(client); applyBuildCustomizers(context, builder); Targeter targeter = get(context, Targeter.class); return targeter.target(this, builder, context, target); } throw new IllegalStateException( &quot;No Feign Client for loadBalancing defined. Did you forget to include spring-cloud-starter-loadbalancer?&quot;);} 首先从feign客户端对应的ioc容器中获取一个Client，这个client对象由配置类注册，默认情况下由 DefaultFeignLoadBalancerConfiguration 提供，使用了 spring-cloud-starter-loadbalancer 组件，可能就由 FeignRibbonClientAutoConfiguration 去提供。在旧版本openfeign下使用了开启了ribbon(spring.cloud.loadbalancer.ribbon.enabled)的话由 FeignRibbonClientAutoConfiguration 提供 获取到Client后，接下来获取到Targeter，Targeter是通过 FeignAutoConfiguration 来配置的，默认是DefaultTargeter，如果整合hystrix或sentinel就是FeignCircuitBreakerTargeter（看有没有CircuitBreakerFactory）。然后调用DefaultTargeter的target方法： 12345@Overridepublic &lt;T&gt; T target(FeignClientFactoryBean factory, Feign.Builder feign, FeignClientFactory context, Target.HardCodedTarget&lt;T&gt; target) { return feign.target(target);} 然后调用Feign.Builder的tartget方法： 1234567891011121314151617public &lt;T&gt; T target(Target&lt;T&gt; target) { return build().newInstance(target);}public Feign build() { super.enrich(); MethodHandler.Factory&lt;Object&gt; synchronousMethodHandlerFactory = new SynchronousMethodHandler.Factory(client, retryer, requestInterceptors, responseInterceptor, logger, logLevel, dismiss404, closeAfterDecode, propagationPolicy, options, decoder, errorDecoder); ParseHandlersByName&lt;Object&gt; handlersByName = new ParseHandlersByName&lt;&gt;(contract, encoder, queryMapEncoder, synchronousMethodHandlerFactory); return new ReflectiveFeign&lt;&gt;(handlersByName, invocationHandlerFactory, () -&gt; null);} 构建了一个ReflectiveFeign，然后调用ReflectiveFeign的newInstance方法，传入target，也就是前面传入的HardCodedTarget 123456789101112131415161718@SuppressWarnings(&quot;unchecked&quot;)public &lt;T&gt; T newInstance(Target&lt;T&gt; target, C requestContext) {TargetSpecificationVerifier.verify(target);Map&lt;Method, MethodHandler&gt; methodToHandler = targetToHandlersByName.apply(target, requestContext);InvocationHandler handler = factory.create(target, methodToHandler);T proxy = (T) Proxy.newProxyInstance(target.type().getClassLoader(), new Class&lt;?&gt;[] {target.type()}, handler);for (MethodHandler methodHandler : methodToHandler.values()) { if (methodHandler instanceof DefaultMethodHandler) { ((DefaultMethodHandler) methodHandler).bindTo(proxy); }}return proxy;} 这个方法解释一下做了什么，首先 targetToHandlersByName.apply 通过target拿到client接口类型，去遍历接口内所有的方法，然后通过 Contract 解析所有方法注解封装成 MethodMetadata ，然后根据 MethodMetadata 等生成 MethodHandler ，返回的map的key就是方法，值为该方法的处理器，处理器里有该方法解析好的 RequestTemplate 等 Contract 主要是用来解析方法上的注解的 默认是 SpringMvcContract，所以能支持MVC的注解 后面就通过 InvocationHandlerFactory ，获取到一个InvocationHandler，之后通过jdk的动态代理，生成一个代理对象，InvocationHandler默认是 ReflectiveFeign.FeignInvocationHandler，在 FeignInvocationHandler#invoke 方法中会根据方法获取对应MethodHandler，具体的请求逻辑就在其invoke方法中，方法处理器有同步/异步请求等 123456789101112131415161718192021222324252627282930313233@Overridepublic InvocationHandler create(Target target, Map&lt;Method, MethodHandler&gt; dispatch) { return new ReflectiveFeign.FeignInvocationHandler(target, dispatch);}FeignInvocationHandler(Target target, Map&lt;Method, MethodHandler&gt; dispatch) { this.target = checkNotNull(target, &quot;target&quot;); this.dispatch = checkNotNull(dispatch, &quot;dispatch for %s&quot;, target);}FeignInvocationHandler#invoke@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable { if (&quot;equals&quot;.equals(method.getName())) { try { Object otherHandler = args.length &gt; 0 &amp;&amp; args[0] != null ? Proxy.getInvocationHandler(args[0]) : null; return equals(otherHandler); } catch (IllegalArgumentException e) { return false; } } else if (&quot;hashCode&quot;.equals(method.getName())) { return hashCode(); } else if (&quot;toString&quot;.equals(method.getName())) { return toString(); } return dispatch.get(method).invoke(args);} 总结 实现侧开头说实现侧和 普通controller类似，将这些方法封装成一个个handle并注册到handleMapping中，等待被DispatcherServlet调用 ，经过上面对 @FeignClient 注解的分析，发现并没有关于接口这块处理。那是怎么被识别为 Controller 接口的呢？ 先来回顾一下怎么定义接口： 首先定义feign接口类 12345678910@FeignClient(value = AppConfig.APP_NAME)public interface ICompanyClient { String API_PREFIX = &quot;/client/company&quot;; @GetMapping(API_PREFIX + &quot;/find-by-id/{id}&quot;) R&lt;CompanyApiVo&gt; findById(@PathVariable String id);} 一般feign接口会单独放在一个jar包中，接口实现和调用方分别引入该jar包 实现feign接口 现在接口定义已经被单独放在了一个jar中了，在具体实现的包中引入它，然后编写具体实现： 12345678910111213@Slf4j@RestController@RequiredArgsConstructor(onConstructor = @__(@Autowired))public class CompanyClientImpl implements ICompanyClient { private final ICompanyService companyService; @Override public R&lt;CompanyApiVo&gt; findById(String id) { final Company company = companyService.findById(id); return R.data(CompanyConverter.INSTANCE.toCompanyApiVo(company)); }} 嗯。。。实际上这些方法能被识别并封装成一个个handle注册到handleMapping中，是因为实现类上加上了 @RestController 注解。在解析方法的时候又通过 findMergedAnnotation 获取方法的注解，所以标记在父类方法上的 @xxxMapping 也能获取到","link":"/2023/04/13/2023-04-13-how-feign-works/"},{"title":"feign基于ribbon实现负载均衡","text":"前言上篇分析了 OpenFeign 使用 FeignClient 注解来生成 动态代理对象的流程","link":"/2023/07/06/2023-07-06-feign-integrates-ribbon/"}],"tags":[{"name":"Servlet 多线程","slug":"Servlet-多线程","link":"/tags/Servlet-%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"maven","slug":"maven","link":"/tags/maven/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"JUnit","slug":"JUnit","link":"/tags/JUnit/"},{"name":"开发问题","slug":"开发问题","link":"/tags/%E5%BC%80%E5%8F%91%E9%97%AE%E9%A2%98/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"Hibernate","slug":"Hibernate","link":"/tags/Hibernate/"},{"name":"设计模式 策略模式","slug":"设计模式-策略模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/"},{"name":"设计模式 观察者模式","slug":"设计模式-观察者模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"ubuntu linux","slug":"ubuntu-linux","link":"/tags/ubuntu-linux/"},{"name":"jenkins","slug":"jenkins","link":"/tags/jenkins/"},{"name":"TCP","slug":"TCP","link":"/tags/TCP/"},{"name":"设计模式 工厂方法模式","slug":"设计模式-工厂方法模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%B7%A5%E5%8E%82%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/"},{"name":"ConcurrentHashMap","slug":"ConcurrentHashMap","link":"/tags/ConcurrentHashMap/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"ThreadLocal","slug":"ThreadLocal","link":"/tags/ThreadLocal/"},{"name":"volatile synchronized","slug":"volatile-synchronized","link":"/tags/volatile-synchronized/"},{"name":"并发编程","slug":"并发编程","link":"/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"线程池","slug":"线程池","link":"/tags/%E7%BA%BF%E7%A8%8B%E6%B1%A0/"},{"name":"设计模式 单例模式","slug":"设计模式-单例模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"name":"公众号开发","slug":"公众号开发","link":"/tags/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%BC%80%E5%8F%91/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"设计模式 责任链模式","slug":"设计模式-责任链模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%B4%A3%E4%BB%BB%E9%93%BE%E6%A8%A1%E5%BC%8F/"},{"name":"设计模式 装饰者模式","slug":"设计模式-装饰者模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F/"},{"name":"UML","slug":"UML","link":"/tags/UML/"},{"name":"设计模式 命令模式","slug":"设计模式-命令模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%91%BD%E4%BB%A4%E6%A8%A1%E5%BC%8F/"},{"name":"jvm","slug":"jvm","link":"/tags/jvm/"},{"name":"设计模式 外观模式","slug":"设计模式-外观模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%A4%96%E8%A7%82%E6%A8%A1%E5%BC%8F/"},{"name":"设计模式 模板方法模式","slug":"设计模式-模板方法模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F/"},{"name":"设计模式 迭代器模式","slug":"设计模式-迭代器模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%BF%AD%E4%BB%A3%E5%99%A8%E6%A8%A1%E5%BC%8F/"},{"name":"设计模式 状态模式","slug":"设计模式-状态模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F/"},{"name":"隔离级别 事务","slug":"隔离级别-事务","link":"/tags/%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB-%E4%BA%8B%E5%8A%A1/"},{"name":"设计模式 适配器模式","slug":"设计模式-适配器模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F/"},{"name":"mybatis","slug":"mybatis","link":"/tags/mybatis/"},{"name":"react","slug":"react","link":"/tags/react/"},{"name":"synchronized cas volatile","slug":"synchronized-cas-volatile","link":"/tags/synchronized-cas-volatile/"},{"name":"Java基础","slug":"Java基础","link":"/tags/Java%E5%9F%BA%E7%A1%80/"},{"name":"常用计算机词汇表","slug":"常用计算机词汇表","link":"/tags/%E5%B8%B8%E7%94%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AF%8D%E6%B1%87%E8%A1%A8/"},{"name":"图床","slug":"图床","link":"/tags/%E5%9B%BE%E5%BA%8A/"},{"name":"jvm 设计模式","slug":"jvm-设计模式","link":"/tags/jvm-%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"RabbitMQ","slug":"RabbitMQ","link":"/tags/RabbitMQ/"},{"name":"Spring Cloud Gateway","slug":"Spring-Cloud-Gateway","link":"/tags/Spring-Cloud-Gateway/"},{"name":"设计模式 策略模式 责任链模式","slug":"设计模式-策略模式-责任链模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F-%E8%B4%A3%E4%BB%BB%E9%93%BE%E6%A8%A1%E5%BC%8F/"},{"name":"ribbon OpenFeign","slug":"ribbon-OpenFeign","link":"/tags/ribbon-OpenFeign/"},{"name":"websocket","slug":"websocket","link":"/tags/websocket/"},{"name":"OpenFeign","slug":"OpenFeign","link":"/tags/OpenFeign/"},{"name":"ribbon","slug":"ribbon","link":"/tags/ribbon/"}],"categories":[{"name":"Servlet","slug":"Servlet","link":"/categories/Servlet/"},{"name":"maven","slug":"maven","link":"/categories/maven/"},{"name":"other","slug":"other","link":"/categories/other/"},{"name":"JUnit","slug":"JUnit","link":"/categories/JUnit/"},{"name":"开发问题","slug":"开发问题","link":"/categories/%E5%BC%80%E5%8F%91%E9%97%AE%E9%A2%98/"},{"name":"mysql","slug":"mysql","link":"/categories/mysql/"},{"name":"设计模式","slug":"设计模式","link":"/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"git","slug":"git","link":"/categories/git/"},{"name":"ubuntu","slug":"ubuntu","link":"/categories/ubuntu/"},{"name":"自动化部署","slug":"自动化部署","link":"/categories/%E8%87%AA%E5%8A%A8%E5%8C%96%E9%83%A8%E7%BD%B2/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"网络协议","slug":"网络协议","link":"/categories/%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE/"},{"name":"并发编程","slug":"并发编程","link":"/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"公众号开发","slug":"公众号开发","link":"/categories/%E5%85%AC%E4%BC%97%E5%8F%B7%E5%BC%80%E5%8F%91/"},{"name":"nginx","slug":"nginx","link":"/categories/nginx/"},{"name":"UML","slug":"UML","link":"/categories/UML/"},{"name":"jvm","slug":"jvm","link":"/categories/jvm/"},{"name":"事务","slug":"事务","link":"/categories/%E4%BA%8B%E5%8A%A1/"},{"name":"mybatis","slug":"mybatis","link":"/categories/mybatis/"},{"name":"前端","slug":"前端","link":"/categories/%E5%89%8D%E7%AB%AF/"},{"name":"Java基础","slug":"Java基础","link":"/categories/Java%E5%9F%BA%E7%A1%80/"},{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"常用计算机词汇表","slug":"常用计算机词汇表","link":"/categories/%E5%B8%B8%E7%94%A8%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AF%8D%E6%B1%87%E8%A1%A8/"},{"name":"其它","slug":"其它","link":"/categories/%E5%85%B6%E5%AE%83/"},{"name":"消息队列","slug":"消息队列","link":"/categories/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Spring Cloud","slug":"Spring-Cloud","link":"/categories/Spring-Cloud/"},{"name":"websocket","slug":"websocket","link":"/categories/websocket/"}],"pages":[]}